[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Processing with R",
    "section": "",
    "text": "Preface\nThe first version of this tutorial was created for a course given at the Swiss Ornithological Institute in 2023. Researchers and students in ecology were the original audience but the whole material can be used by anybody who wants to learn more about how to perform GIS analyses and design maps with the R statistical programming language. The tutorial was heavily updated in 2025.\nMy goal was to combine a very general and basic introduction to GIS with some sort of cookbook showing how to perform common GIS analyses and create maps with R. The introductory sections should provide enough information for readers with no or little GIS experience in order to understand the rest of the material. You’re welcome to skip them, especially if you understand GIS data models and how GIS data is stored in R.\nThe cookbook part of the tutorial is a collection of analyses and mapping techniques that I regularly use in my job. Most of them are standard GIS procedures, but you’ll also find more advanced topics. There are two ways to run the code snippets in this tutorial. You can either run everything, starting with the first section and continuing with the others. Or, if you prefer the cookbook approach, I also included a collapsible code snippet at the start of each section that will allow you to get all the required data and load packages for this section only.\nYou will notice that the amount of content (examples, explanations, exercises, etc.) can be variable depending on the section. I’m sorry for this, this is purely due to a lack of time on my side… I promise to do my best to add more content soon (hopefully).\nNone of this could have been written without the incredible work done by the R-spatial community. I’d especially like to thank all the software developers who created and are maintaining the R software, the R packages and the open-source libraries used in these packages!\n\n\nGetting ready\nYou’ll need the following packages to run the examples in this book. You can use the following code to install them. All the required dependencies will be automatically installed. The most important ones are sf1, terra2, tmap3 and mapview4.\n\n\n\n\n\n\nImportant\n\n\n\nPlease also update to the latest version of R, otherwise you may get packages that are not fully up-to-date.\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\ninstall.packages(\"tmap\")\ninstall.packages(\"mapview\")\ninstall.packages(\"classInt\")\ninstall.packages(\"httr2\")\ninstall.packages(\"leaflet.extras2\")\ninstall.packages(\"leafpop\")\ninstall.packages(\"leafsync\")\ninstall.packages(\"lwgeom\")\ninstall.packages(\"qgisprocess\")\ninstall.packages(\"spatstat.random\")\ninstall.packages(\"spdep\")\ninstall.packages(\"tmaptools\")\nThe required data is available on the GitHub repository (https://github.com/jguelat/spatial-r). You’ll also find all the code needed to re-create the whole book.\n\n\nChangelog\nOnly the big updates are listed here.\n\n\n\n2025-05-08\nThe course was converted to a Quarto book and is available as a GitHub website: https://jguelat.github.io/spatial-r.\n\n\n2025-02-25\nUpdate of the tmap subsections (static and dynamic maps), compatibility with version 4.x, more explanations and map examples.\n\n\n2025-02-07\nLarge update of the “Tips and tricks for vectors” and “Dynamic maps” sections, second workshop in Sempach.\n\n\n2023-01-20\nFirst version of the book for the workshop in Sempach.\n\n\n\n\n\n\n\n1. Pebesma E. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal. 2018. 10(1):439-446.\n\n\n2. Hijmans RJ. Terra: Spatial Data Analysis. Published online 2023. https://rspatial.org\n\n\n3. Tennekes M. Tmap: Thematic Maps. Published online 2023. https://github.com/r-tmap/tmap/\n\n\n4. Appelhans T, Detsch F, Reudenbach C, Woellauer S. Mapview: Interactive Viewing of Spatial Data in R. Published online 2023. https://github.com/r-spatial/mapview/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 R and GIS\nYou’ve probably already used a more traditional GIS software such as QGIS or ArcGIS Pro, you’ve maybe even heard that many GIS specialists use the Python programming language. Then why would you start using R, a statistic software, to perform your GIS analyses and make maps? I will only outline a few advantages and disadvantages in this introduction.\nA lot of ecological data you’re going to analyse has a spatial component and it’s convenient to perform everything in the same software. You will not need to transfer files and everything will be in the right format for subsequent statistical analyses. Moreover, most of you already know R and it’s definitely easier to extend a bit you knowledge to include GIS analyses than to learn how to use a new software or how to code in Python. Fortunately, there’s a really active community of R-users doing GIS, so you will not be alone and you’ll easily find a lot of documentation online. There’s also an incredibly large number of packages on CRAN for spatial data processing or analysis. You’ll find an overview on the CRAN Task View: Analysis of Spatial Data.\nDoing your GIS analyses with code is also a nice opportunity to make your research more reproducible. The whole data processing is documented and you and others can easily check and re-run everything. The same applies for maps, you can re-create them in a few seconds if the data changed. This is much harder if you only use a “point and click” GIS software.\nHowever there are some GIS tasks where R doesn’t shine. I would for example never digitize GIS data or georeference images (such as old maps) using R. As we will later, the cartographic capabilities of some R packages are really impressive. But it will still be easier to use a traditional GIS software if you need more specialized techniques such as complex labeling, advanced symbology or complex map layouts. The same applies if you need to use 3D vector data (with the exception of point clouds).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#gis-data-models",
    "href": "introduction.html#gis-data-models",
    "title": "1  Introduction",
    "section": "1.2 GIS data models",
    "text": "1.2 GIS data models\nWhen we work with geographic data we need to decide how to model real world objects or concepts (buildings, forests, elevation, etc.) before we can use them in a computer with a GIS (Geographic Information System) software. GIS people mainly use 2 main data models: vector and raster. Other models exist, such as TINs, point clouds or meshes, but we won’t cover them here.\nVector data is normally used for high precision data sets and can be represented as points, lines or polygons. Properties of the represented features are stored as attributes. The vector types you will use depends of course on your own data and on the required analyses. For example: points could be appropriate for bird nests and sightings, lines for moving animals and linear structures (paths, rivers), and polygons for territories and land cover categories. Of course a river can also be modeled as a polygon if you’re interested in its width (or you can also store its width as an attribute of the line).\n\n\n\n\n\n\nNote\n\n\n\nHigh precision doesn’t necessarily mean high accuracy! For example the coordinates of some points could be stored in meters with 5 decimals even though the measurement error was 2 meters.\nMost vector data formats include some possibility to store information about measurement errors but this is actually very rarely used.\n\n\nThe best known format for storing vector data is the shapefile, an old and inefficient format developed by ESRI. Even though the shapefile format is still widely used, it has a lot of limitations and problems (listed on the following website: http://switchfromshapefile.org). Nowadays GIS specialists advise to replace it with better alternatives such as the GeoPackage format. Every modern GIS software can read and write GeoPackages and the format is also completely open-source. It is also published as a standard by the Open Geospatial Consortium (OGC) which makes it a future-proof alternative.\n\n\n\n\n\n\nImportant\n\n\n\nI strongly advise against using GeoPackages (or any other file database format) on cloud-storage platforms such as Dropbox, OneDrive or Google Drive, especially if you need to edit them! Most of the time everything will work fine, but the risk of corruption and/or data loss due to the synchronization mechanism is not negligible!\n\n\nRaster data is basically an image divided into cells (pixels) of constant size, and each cell has an associated value. Satellite imagery, topographic maps and digital elevation models (DEM) are typical examples where the raster data model is appropriate. A raster data set can have several layers called bands, for example most aerial images have at least three bands: red, green and blue. In the raster data model, specific geographic features are aggregated to a given resolution to create a consistent data set, associated with a loss of precision. The resolution used to aggregate can have a large influence of some analyses and must be thought of carefully.\nThere exists thousands of different raster data formats. As of today I recommend using the GeoTiff format. It is widely used in the GIS world and every GIS software can read and write raster data in this format. Note that it is also possible to use the GeoPackage format to save raster data sets, however I would advise against using it since some GIS software won’t be able to read these rasters.\n\n\n\n\n\n\nTip\n\n\n\nVector data: use the GeoPackage format\nRaster data: use the GeoTiff format",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "vector_data.html",
    "href": "vector_data.html",
    "title": "2  Vector data",
    "section": "",
    "text": "2.1 Vector data model\nThe main vector types are points, lines and polygons (or a combination thereof) and the point is the base of all these types. For example a simple line consists of 2 connected points, similarly an ordered sequence of connected points will represent a more complex line (often called a polyline). A simple polygon will be modeled as an external ring, which is a special type of polyline where the first and last points are identical. In the case of lines and polygons we often speak of vertices to describe these points. Things can be a bit more complex, for example a polygon could have a hole which is modeled as an internal ring.\nThe Simple Feature standard (full documentation) was developed to be sure that we all speak the same language when describing vector elements. The specification describes 18 geometry types, but don’t worry only 7 of them will be useful for us. The following figure shows these 7 types (source: Lovelace et al., 20191):\nA feature represents a geographic entity modeled by one of these types. For example a building would be a single feature of type POLYGON, while the whole Hawaii archipelago would be a single feature of type MULTIPOLYGON (but you could of course also model each island separately as type POLYGON). A single feature using the MULTI* types can have multiple elements but this is not mandatory. Most of the time we will use the default 2D version of these types. However it is possible to include additional numeric values such as the height of each point (Z values) or some kind of measurement error (M values). Note that many GIS software will ignore Z and M values for the vast majority of spatial analyses.\nIn most GIS softwares (including R), simple features are internally encoded using the well-known binary (WKB) or well-known text (WKT) standards. As the name mentions, WKB is a binary format and hence not easily readable by normal humans. The WKT format is encoding exactly the same information as WKB, but in a more human friendly way. Here are some examples of WKT-encoded features (check the Wikipedia page if you need more):\nThe geometry is of course essential in order to have a spatial information but the vector data model also allows storing non-spatial attributes (often called attribute table) for each feature. As we will see, these tables are stored as data frames in R and each column will store some property of the related feature (identification number, name, etc.). Each row relates to a single spatial feature (which can consist of several geometries if its type is MULTI*). The following figure shows some examples (source: Tennekes & Nowosad, 20212):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector data</span>"
    ]
  },
  {
    "objectID": "vector_data.html#vector-data-model",
    "href": "vector_data.html#vector-data-model",
    "title": "2  Vector data",
    "section": "",
    "text": "Important\n\n\n\nThe feature type is usually defined for the whole vector data set, and not per feature (actually sf lets you do that but this will brings you all sorts of troubles). For example, if you know that your data set will contain POLYGON and MULTIPOLYGON features, then you will have to use the MULTIPOLYGON type for all of them.\n\n\n\n\na point: POINT (10 5)\na linestring made of 3 points: LINESTRING (1 1, 2 4, 5 10)\na polygon (without a hole): POLYGON ((10 5, 10 9, 5 8, 4 2, 10 5))\na multilinestring: MULTILINESTRING ((1 1, 2 4, 5 10), (2 2, 5 2))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector data</span>"
    ]
  },
  {
    "objectID": "vector_data.html#a-first-look-at-vector-data-in-r",
    "href": "vector_data.html#a-first-look-at-vector-data-in-r",
    "title": "2  Vector data",
    "section": "2.2 A first look at vector data in R",
    "text": "2.2 A first look at vector data in R\nLet’s have a look at how R stores a vector data set. The main classes and methods needed to work with spatial vector data are defined in the sf package. We will also load the tmap package to have access to some spatial data sets.\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.4.3\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.4.3\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\nWhen you first load the sf package, it will provide you with version information about some important open-source GIS libraries it uses. In a few rare cases, some functions will only be available if you use recent version of these libraries. If you use sf on Windows or Mac and install it from CRAN, they will be included inside the sf package and there’s no easy way to update them. These libraries are used in almost all open-source GIS software and even in some commercial ones. GDAL takes care of reading and writing your GIS files and can read 99.9% of all the existing GIS formats (the vector part of the GDAL library is often called OGR); GEOS is a Euclidean planar geometry engine and is used for all the common GIS analyses (intersection, union, buffer, etc.); PROJ is responsible for all the coordinate reference systems operations. The s2 library is a spherical geometry engine which is active by default for computations when using unprojected data.\n\n\n\n\n\n\nA bit of history\n\n\n\nThe availability of the sf package was a massive change in the “R/GIS” ecosystem (often called R-Spatial). In the old days we used a combination of several packages to process GIS vector data in R. The spatial classes (and some functions) were defined in the sp package, the import/export of data was managed by the rgdal package, and the geometric operations were available in the rgeos package. You’ll find a lot of code using these packages on the internet. Please refrain from using them since they are not maintained anymore. The packages rgdal and rgeos were removed from CRAN, sp is still available. Moreover the sf package is definitely more powerful and much faster.\n\n\nWe will now inspect the World vector data set inside the tmap package and have a look at its structure.\n\nclass(World)\n\n[1] \"sf\"         \"data.frame\"\n\nnames(World)\n\n [1] \"iso_a3\"       \"name\"         \"sovereignt\"   \"continent\"    \"area\"        \n [6] \"pop_est\"      \"pop_est_dens\" \"economy\"      \"income_grp\"   \"gdp_cap_est\" \n[11] \"life_exp\"     \"well_being\"   \"footprint\"    \"HPI\"          \"inequality\"  \n[16] \"gender\"       \"press\"        \"geometry\"    \n\nWorld\n\nSimple feature collection with 177 features and 17 fields\nAttribute-geometry relationships: constant (12), aggregate (3), identity (2)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.645\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   iso_a3        name  sovereignt     continent               area  pop_est\n1     AFG Afghanistan Afghanistan          Asia   642393.20 [km^2] 38041754\n2     ALB     Albania     Albania        Europe    28298.42 [km^2]  2854191\n3     DZA     Algeria     Algeria        Africa  2312302.79 [km^2] 43053054\n4     AGO      Angola      Angola        Africa  1249442.03 [km^2] 31825295\n5     ATA  Antarctica  Antarctica    Antarctica 12258324.81 [km^2]     4490\n6     ARG   Argentina   Argentina South America  2783123.36 [km^2] 44938712\n7     ARM     Armenia     Armenia          Asia    29555.06 [km^2]  2957731\n8     AUS   Australia   Australia       Oceania  7705931.67 [km^2] 25364307\n9     AUT     Austria     Austria        Europe    83755.95 [km^2]  8877067\n10    AZE  Azerbaijan  Azerbaijan          Asia    86152.91 [km^2] 10023318\n   pop_est_dens                    economy              income_grp gdp_cap_est\n1  5.921880e+01  7. Least developed region           5. Low income    507.1007\n2  1.008604e+02       6. Developing region  4. Lower middle income   5353.1806\n3  1.861912e+01       6. Developing region  3. Upper middle income   3973.9573\n4  2.547161e+01  7. Least developed region  3. Upper middle income   2790.7047\n5  3.662817e-04       6. Developing region 2. High income: nonOECD 200000.0000\n6  1.614686e+01    5. Emerging region: G20  3. Upper middle income   9912.2779\n7  1.000753e+02       6. Developing region  4. Lower middle income   4622.4623\n8  3.291530e+00 2. Developed region: nonG7    1. High income: OECD  55060.3255\n9  1.059873e+02 2. Developed region: nonG7    1. High income: OECD  50137.6187\n10 1.163433e+02       6. Developing region  3. Upper middle income   4793.5225\n   life_exp well_being footprint      HPI inequality gender press\n1    61.982   2.436034  1.139635 16.21067         NA  0.665 19.09\n2    76.463   5.255482  3.674262 46.33355       29.4  0.116 54.10\n3    76.377   5.217018  3.110979 47.44608       27.6  0.460 41.98\n4        NA         NA        NA       NA       51.3  0.520 52.44\n5        NA         NA        NA       NA         NA     NA    NA\n6    75.390   5.908279  6.391003 43.79233       40.7  0.292 63.13\n7    72.043   5.300569  3.429790 43.00195       27.9  0.198 71.60\n8    84.526   7.111599 17.703905 39.47980       34.3  0.063 73.42\n9    81.580   7.079641 12.147266 45.34394       30.7  0.048 74.69\n10   69.366   5.200000  6.102844 34.37222       26.6  0.329 27.99\n                         geometry\n1  MULTIPOLYGON (((66.217 37.3...\n2  MULTIPOLYGON (((20.605 41.0...\n3  MULTIPOLYGON (((-4.923 24.9...\n4  MULTIPOLYGON (((12.322 -6.1...\n5  MULTIPOLYGON (((-61.883 -80...\n6  MULTIPOLYGON (((-68.634 -52...\n7  MULTIPOLYGON (((46.483 39.4...\n8  MULTIPOLYGON (((147.689 -40...\n9  MULTIPOLYGON (((16.88 48.47...\n10 MULTIPOLYGON (((46.144 38.7...\n\n\nWe see the World object is stored as a data frame with an additional geometry column (note that the name of the geometry column doesn’t need to be ‘geometry’). The content of the geometry column is displayed using the WKT format. A programmer would say these objects are instances of the sf class, and I will thus call them sf objects. R is also giving us more information, like the coordinate reference system used (more on that later) and the number of dimensions (i.e. XY, XYZ or XYZM).\n\n\n\n\n\n\nQuestion: why is the MULTIPOLYGON type appropriate?\n\n\n\n\n\nDon’t forget that each feature (i.e. each row of the data frame) represents a country, and some countries are made up of several distinct pieces of land (e.g., islands, exclaves). That’s why we need the MULTIPOLYGON type. And since the type apply to the whole data set, even countries with a single geometry (like Switzerland) will need to be MULTIPOLYGONS.\n\n\n\nIt is also easy to plot the data using the usual command.\n\nplot(World)\n\n\n\n\n\n\n\n\nBy default R will take the first 9 attributes of the sf object and plot them using the available geometries. Since these objects inherit from the data base class, you can use all the typical data frame functions such as summary, head, merge, rbind, etc. Subsetting is also possible using the standard [] operators. Therefore you can use the following code if you only want to plot the well-being index, for the whole world, only for countries with a high index, or just for Australia.\n\nplot(World[,\"well_being\"])\n\n\n\n\n\n\n\nplot(World[World$well_being &gt; 6,\"well_being\"])\n\n\n\n\n\n\n\nplot(World[World$name == \"Australia\",\"well_being\"])\n\n\n\n\n\n\n\n\nNote that the color scale was adapted depending on the available values in the filtered data set. If you only need the geometries without any attributes, then you can use the st_geometry() function.\n\nplot(st_geometry(World))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe haven’t done it here, but, as we will see later, it is better to first project everything using an appropriate projection when you want to plot global data (like the previous world maps).\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nPlay a little bit with the World data set, try different functions that you would normally use with a data frame. Import the redlist data from the file red_list_index.csv (source: https://stats.oecd.org) and join it to the World data frame to add new attributes. Plot a world map using one of the new attributes.\n\n\nCode\nredlist &lt;- read.csv(\"data/red_list_index.csv\")\nworld2 &lt;- merge(World, redlist, by.x = \"iso_a3\", by.y = \"code\")\nplot(world2[,\"index_2020\"])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector data</span>"
    ]
  },
  {
    "objectID": "vector_data.html#structure-of-sf-objects",
    "href": "vector_data.html#structure-of-sf-objects",
    "title": "2  Vector data",
    "section": "2.3 Structure of sf objects",
    "text": "2.3 Structure of sf objects\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\n\n\n\n\nMost of the time you won’t need to create your own sf objects from scratch since you’ll import some existing GIS data. But if you need to, there are special functions to help you. This is also a good way to get a better understanding of the structure of sf objects. The standard process is shown in the following figure (source: Lovelace et al., 20191):\n\nYou first need to create each feature geometry using some constructor functions. Each of these features will be of class sfg (simple feature geometry). Then you collect all these geometries in a list using the st_sfc() function. You get a new object of class sfc (simple feature list-column). After that you combine the newly created simple feature list-column with the attributes (stored as a data frame, or a tibble) using the st_sf() function in order to get an sf object.\nSince this is rather abstract, let’s look at a simple example. Imagine we want to create a point data set containing three bird observations, and each observation will have the following attributes: species and sex. We start by creating our point geometries using x and y coordinates:\n\npt1 &lt;- st_point(c(2657000, 1219000))\npt2 &lt;- st_point(c(2658000, 1218000))\npt3 &lt;- st_point(c(2659000, 1217000))\n\nLet’s have a look at what we’ve just created:\n\npt1\n\nPOINT (2657000 1219000)\n\nclass(pt1)\n\n[1] \"XY\"    \"POINT\" \"sfg\"  \n\ntypeof(pt1)\n\n[1] \"double\"\n\nstr(pt1)\n\n 'XY' num [1:2] 2657000 1219000\n\n\nOur first object is a 2D point (otherwise we would see XYZ or XYZM) of class sfg. If we look a bit more into the details of the structure, we see that it is actually stored as vector of type double (with length 2).\nNow we need to collect our points inside an sfc object. This is simply a list of sfg objects with an associated coordinate reference system (CRS). Since we collected our data in Switzerland, we will use the standard Swiss coordinate reference system. As we will see later, most coordinate reference systems are identified by a specific number.\n\npts &lt;- st_sfc(pt1, pt2, pt3, crs = \"EPSG:2056\")\n\nLet’s have a look at our new creation:\n\npts\n\nGeometry set for 3 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2657000 ymin: 1217000 xmax: 2659000 ymax: 1219000\nProjected CRS: CH1903+ / LV95\n\n\nPOINT (2657000 1219000)\n\n\nPOINT (2658000 1218000)\n\n\nPOINT (2659000 1217000)\n\nclass(pts)\n\n[1] \"sfc_POINT\" \"sfc\"      \n\ntypeof(pts)\n\n[1] \"list\"\n\nstr(pts)\n\nsfc_POINT of length 3; first list element:  'XY' num [1:2] 2657000 1219000\n\n\nThis confirms that our sfc object is actually a list, and this object will be the geometry column of the soon to be created sf object. Since our object is a list, it is easy to extract individual elements if needed:\n\n# Extract the second item of the list\npts[[2]]\n\nPOINT (2658000 1218000)\n\nclass(pts[[2]])\n\n[1] \"XY\"    \"POINT\" \"sfg\"  \n\n\nThe feature geometries (stored in an sfc object) are only half of what we need to create an sf object. We also need to define the attributes of each feature. We store them in a data frame using the same order as the geometries.\n\npts_data &lt;- data.frame(species = c(\"wallcreeper\", \"alpine chough\", \"kingfisher\"),\n                       sex = c(\"male\", \"female\", \"female\"))\npts_data\n\n        species    sex\n1   wallcreeper   male\n2 alpine chough female\n3    kingfisher female\n\n\nAnd as a last step we combine the feature geometries with the related attributes using the st_sf() function. We now have a typical GIS data set stored as an sf object.\n\npts_sf &lt;- st_sf(pts_data, geometry = pts)\npts_sf\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2657000 ymin: 1217000 xmax: 2659000 ymax: 1219000\nProjected CRS: CH1903+ / LV95\n        species    sex                geometry\n1   wallcreeper   male POINT (2657000 1219000)\n2 alpine chough female POINT (2658000 1218000)\n3    kingfisher female POINT (2659000 1217000)\n\n\nSince everything is stored as lists, it is again easy to access individual elements of the sf object:\n\n# Extract the 3rd geometry\npts_sf$geometry[[3]]\n\nPOINT (2659000 1217000)\n\n\nThere’s some sort of tradition to call geometry columns geom or geometry, but you’re entirely free to chose another name. However, you need to be a bit careful since the sf package must always know the correct name. For example, using the standard names() function will not work for geometry columns since sf won’t be informed of the change. To modify the name of the geometry column, always use the st_geometry() function.\n\nnames(pts_sf)[3] &lt;- \"my_beautiful_points\"\npts_sf\n\nError in st_geometry.sf(x): attr(obj, \"sf_column\") does not point to a geometry column.\nDid you rename it, without setting st_geometry(obj) &lt;- \"newname\"?\n\nst_geometry(pts_sf) &lt;- \"my_beautiful_points\"\npts_sf\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2657000 ymin: 1217000 xmax: 2659000 ymax: 1219000\nProjected CRS: CH1903+ / LV95\n        species    sex     my_beautiful_points\n1   wallcreeper   male POINT (2657000 1219000)\n2 alpine chough female POINT (2658000 1218000)\n3    kingfisher female POINT (2659000 1217000)\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also create sf objects directly from a data frame containing a column of type sfc using the st_as_sf() function.\n\npts_data$geometry &lt;- pts\npts_sf &lt;- st_as_sf(pts_data)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou’ve now probably noticed that most functions in the sf package have an st_ prefix. This is a reference (and probably homage) to PostGIS, an extension allowing to store and query GIS data in the PostgreSQL database management system. All PostGIS functions start with the ST_ prefix, which stands for “Spatial Type”.\n\n\nWe process similarly to create other geometry types from scratch, the only difference is that we now need matrices to store the vertices of the lines and polygons instead of a simple vector, and for multilinestrings, (multi-)polygons and geometry collections, we need more lists to encapsulate everything. If you’re not sure how to create geometries, the sf documentation provides examples for all the geometry types. Look for the following functions: st_point(), st_linestring(), st_polygon(), st_multipoint(), st_multilinestring(), st_multipolygon(), st_geometrycollection(). Here’s a more complex example showing how to create a multipolygon (including one geometry with a hole) inside an sfg object. The next steps (collecting geometries in an sfc object, adding attributes and store as an sf object) are exactly the same as before.\n\n# rbind creates matrices and makes the coding easier\npol1_border &lt;- rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))\npol1_hole &lt;- rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))\npol1 &lt;- list(pol1_border, pol1_hole)\npol2 &lt;- list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))\nmultipolygon_list &lt;- list(pol1, pol2)\nmultipol &lt;- st_multipolygon(multipolygon_list)\nmultipol\n\nMULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)), ((0 2, 1 2, 1 3, 0 3, 0 2)))\n\n\n\nplot(multipol, col = \"navy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also create sfc and sf objects from scratch using the WKT format and the st_as_sfc() and st_as_sf() functions. The following example creates an sfc object using a character vector, without needing to create an sfg object first.\n\npts &lt;- st_as_sfc(c(\"POINT(2657000 1219000)\", \"POINT(2658000 1218000)\", \"POINT(2659000 1217000)\"), crs = \"EPSG:2056\")\n\nAnd you can use a similar approach to create an sf object. In this case we add a new column (as a character vector) to the data frame containing the attributes. Note the use of the wkt argument inside the st_as_sf() function.\n\npts_data$geometry &lt;- c(\"POINT(2657000 1219000)\", \"POINT(2658000 1218000)\", \"POINT(2659000 1217000)\")\npts_sf &lt;- st_as_sf(pts_data, wkt = \"geometry\", crs = \"EPSG:2056\")\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nTry to build your own sfc and sf objects using either st_sfc() and st_sf() or st_as_sfc() and st_as_sf().\n\n\n\n\n\n\n1. Lovelace R, Nowosad J, Muenchow J. Geocomputation with R. Second edition. CRC Press 2025.\n\n\n2. Tennekes M, Nowosad J. Elegant and Informative Maps with Tmap. 2025. https://tmap.geocompx.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector data</span>"
    ]
  },
  {
    "objectID": "raster_data.html",
    "href": "raster_data.html",
    "title": "3  Raster data",
    "section": "",
    "text": "As we saw above, a raster data set is basically an image, which is the same as a grid of pixels. These pixels are often called cells. Most raster data sets you will encounter will have square cells with a constant size (also called resolution), and we will only focus on these in this tutorial. However don’t forget that other kind of grids, for example sheared or curvilinear, also exist. This is sometimes needed depending on the coordinate reference system used to store the data, or can be caused by some reprojections.\nRasters are perfect for storing continuous values contained in a large area (called the extent of the raster). Digital elevation models are a typical example of such data, each cell is used to store an elevation value. You will also find rasters containing discrete values, these are often used to store landcover or landuse data sets. Note that, unlike vector data, it is impossible to store overlapping features in the same data set.\nWe saw that vector data sets can store multiple attributes for a single feature. We can use a similar technique for raster data sets with the help of raster bands. You can think of raster bands as different layers of the same grid, each layer containing a different information. This is mainly use for spectral data, for example the red, green and blue intensity values in an aerial picture; satellite imagery will have even more bands depending on the number of sensors. Multiband rasters are also often used to store environment variables that change through time (e.g. a temperature raster, with one band per day). Such rasters are often called datacubes.\n\nPerforming computations on raster data sets is usually very efficient and faster than using vector data. This is due to the fact that rasters are stored in some kind of matrix formats with some extra information such as the coordinate reference system and the origin of the raster. It this thus possible to use highly efficient linear algebra libraries. The mathematical operations performed on raster cells are called map algebra.\nWe will use the terra package to work with raster data. This package has everything needed to import, analyse, visualize and export raster data sets. Like the sf package, it is also using the GDAL library for the import/export operations, which means it can open almost every raster data format. Unlike the sf package, terra will not import the full data sets in memory but only create a pointer to the data and then read smaller blocks of data successively. This allows working with very large rasters with a relatively small memory footprint. The amount of functions available in the terra package is similar to typical GIS software.\n\n\n\n\n\n\nRasters and R workspaces\n\n\n\nSince terra only stores a pointer to the raster data set, this means the actual data set won’t be included if you save your session in an R workspace (.Rdata files). If you really want to include it in your workspace, you can use the wrap() function. Note that this is also needed if you want to pass raster data over a connection that serializes, e.g. to a computer cluster.\n\n\nThere is another famous R package to process raster data, the stars package. It is especially useful if you need to work with “irregular” rasters (sheared, curvilinear, etc.) or with complex datacubes. It is also tidyverse-friendly and the syntax is closed to the one used in sf. However the number of available functions is (still) much lower than in terra. If you need to use both packages, it is fortunately easy to convert raster objects from terra to stars (using the function st_as_stars()), and the other way round (using the function rast()).\n\n\n\n\n\n\nA bit of history\n\n\n\nA revolution happened in 2010 in the small world of R-spatial when the raster package was released. People were able to perform analyses using raster data sets in R instead of using a standard GIS software. The package has been maintained during many years, and many functions were added. However its developer decided to create a new package from scratch in order to improve speed and memory efficiency, the terra package was born. You will often find a lot of R code using the raster package on the web. Fortunately it is quite easy to adapt code written for the raster package to terra. The functions have similar names (sometimes even identical) and everything that was available in raster should also be available in terra. Actually the recent versions of raster even uses terra in the background instead of the original raster code.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Raster data</span>"
    ]
  },
  {
    "objectID": "coordinate_reference_systems.html",
    "href": "coordinate_reference_systems.html",
    "title": "4  Coordinate reference systems",
    "section": "",
    "text": "If you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\n\n\n\n\nThe majority of normal people will get scared if there’s some problem to solve involving coordinate reference systems or projections. That’s why I will keep this part really short and only show you the stuff you will need to perform standard GIS analyses with R. If you want to read more about this (extremely interesting) topic, I invite you to read the following book chapters: https://r.geocompx.org/spatial-class.html#crs-intro and https://r-tmap.github.io/tmap-book/geodata.html#crs.\nThere is a famous expression saying “spatial is special”… One of the main reasons is that such data will have an associated location and you thus need a reference frame to describe this location. This reference frame is called a coordinate reference system (CRS) in the GIS world. CRSs can be either geographic or projected.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you’re working with GIS data, you should always know the CRS you’re using. Otherwise coordinates are just numbers without a reference frame. When you share GIS data, make sure the CRS is always defined in the data set or documented in some other way. The CRS of sf objects can be queried with the st_crs() function, for terra objects you should use the crs() function.\n\n\nA geographic CRS will identify locations on a spheroid or an ellipsoid using 2 values: latitude and longitude. The shape of the Earth is actually a geoid, but it is too complex to perform computations and thus one has to use approximations. The spheroid is making the assumption that the Earth is a sphere, while the ellipsoid is a better approximation accounting for the fact that our planet is a bit compressed (flatter at the North and South Poles). Geographic coordinate systems are not using a projection! All the computations (distances, buffers, etc.) have to happen on the spheroid/ellipsoid, which makes things more complex. It is easy to make mistakes when working with geographic CRSs, and even smart people fell in this trap (e.g. https://georeferenced.wordpress.com/2014/05/22/worldmapblunders).\nProjected CRSs are based on geographic CRSs but include an additional step: a projection on a flat surface. When using a projected CRS, locations are described using Cartesian coordinates called easting and northing (x and y). Projecting a spherical or ellipsoidal surface on a plane will cause deformations. These will affect four properties of the surface: areas, distances, shapes and directions. A projected CRS can preserve only one or two of these properties. There exists a ton of different projections and all of them make different compromises, some are even totally useless (check this beautiful xkcd comic: https://xkcd.com/977). Choosing a projection can be challenging, especially if your data covers a very large area. The following websites allow you to visualize the main projection types: https://www.geo-projections.com and https://map-projections.net/singleview.php. The second website also provides a nice tool to visualize distortions called a Tissot indicatrix. Fortunately, if your data is within a “smallish” area, it is relatively easy to find a good projected CRS that has only minimal distortions. Almost every country has its own recommended projected CRS (or CRSs), and if your data covers several countries, you can use a UTM (Universal Transverse Mercator) projection, or even better a Lambert azimuthal equal-area projection (set the latitude and the longitude of origin to the center of the study area).\n\n\n\n\n\n\nTip\n\n\n\nIt is almost always easier to work with a projected CRS, except if your data is really global (or covering a really large area, like a continent). Moreover, most GIS software will (still) make the assumption that you’re data is on a flat plane, even if you’re working with a geographic CRS. The sf package is kind of an exception since it will actually perform calculations on a spheroid if you use a geographic CRS, thanks to the s2 library.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe CRS used by almost all mapping websites (OpenStreetMap, Google Maps, etc.) should never be used for any analysis. It is a slightly modified version of the Mercator projection called Web Mercator or Pseudo-Mercator. It has some advantages allowing good visualization speed, but the distortions are massive. Check the following website: https://www.thetruesize.com.\n\n\nWith so many CRSs available, we need a way to classify them. That’s what the EPSG (European Petroleum Survey Group) started doing a few years ago. They collected and documented most available CRSs in a data set which is now called the EPSG Geodetic Parameter Dataset (https://epsg.org/home.html). In this data set, every CRS has a unique identification number that can be used in a GIS software instead of writing the full definition of the CRS. The best available transformations between CRSs are also defined. Sadly this data set is still missing a few interesting CRSs and was thus completed by other companies such as ESRI. This is the reason why you’ll sometimes see ESRI codes instead of EPSG for some CRSs. To avoid confusion, CRSs are usually referenced by an SRID (Spatial Reference System Identifier), which is made of two components, an authority (such as EPSG or ESRI) and an identifier. If no authority is mentioned you can usually assume it’s coming from the EPSG data set (especially in the open-source GIS world). For clarity, I recommend always specifying the full SRID when working with CRSs. With sf and terra (and most other GIS packages), the SRID has to be written in the form “authority:identifier”.\nThe following CRSs are especially interesting for us:\n\n\n\n\n\n\n\n\nSRID\nName\nDescription\n\n\n\n\nEPSG:2056\nCH1903+/LV95\nProjected CRS currently used in Switzerland\n\n\nEPSG:21781\nCH1903/LV03\nFormer projected CRS used in Switzerland, you will still find data sets using this one\n\n\nEPSG:4326\nWGS84\nGeographic CRS used for most global data sets, and by GPS devices\n\n\nEPSG:3857\nPseudo-Mercator\nProjected CRS used by online maps\n\n\nEPSG:8857\nEqual Earth Greenwich\nNice equal-area projection for world maps\n\n\nESRI:54030\nRobinson\nAesthetically pleasing projection for world maps\n\n\n\n\n\n\n\n\n\nProj4strings\n\n\n\nWhen looking for examples on the web, you will often find code snippets using what is called a proj4string to define a CRS or to reproject data. For example the proj4string for the current Swiss CRS looks like this: +proj=somerc +lat_0=46.9524055555556 +lon_0=7.43958333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs +type=crs. This was the standard way of describing CRSs until a few years ago. You should NOT use these strings, instead always use the EPSG (or another authority) number to be on the safe side. Otherwise you may get small to medium position errors when reprojecting your data.\nSimilarly, you will sometimes see some CRS definitions using the +init= syntax (e.g., +init=EPSG:2056). This should also be avoided for similar reasons, moreover this can also cause problems with other GIS software not recognizing the CRS properly.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you search for the EPSG database on your favorite search engine, you may find the website https://epsg.io. Please do not use it for EPSG codes (it’s still useful for ESRI codes, though)! It is not the official EPSG website, it doesn’t use the latest version of the EPSG database, and therefore some definitions of CRSs are outdated.\n\n\nYou can easily explore how CRSs are stored in modern GIS software. For example if you want to inspect the current Swiss CRS:\n\nst_crs(\"EPSG:2056\")\n\nCoordinate Reference System:\n  User input: EPSG:2056 \n  wkt:\nPROJCRS[\"CH1903+ / LV95\",\n    BASEGEOGCRS[\"CH1903+\",\n        DATUM[\"CH1903+\",\n            ELLIPSOID[\"Bessel 1841\",6377397.155,299.1528128,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4150]],\n    CONVERSION[\"Swiss Oblique Mercator 1995\",\n        METHOD[\"Hotine Oblique Mercator (variant B)\",\n            ID[\"EPSG\",9815]],\n        PARAMETER[\"Latitude of projection centre\",46.9524055555556,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8811]],\n        PARAMETER[\"Longitude of projection centre\",7.43958333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8812]],\n        PARAMETER[\"Azimuth at projection centre\",90,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8813]],\n        PARAMETER[\"Angle from Rectified to Skew Grid\",90,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8814]],\n        PARAMETER[\"Scale factor at projection centre\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8815]],\n        PARAMETER[\"Easting at projection centre\",2600000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8816]],\n        PARAMETER[\"Northing at projection centre\",1200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8817]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping (large and medium scale).\"],\n        AREA[\"Liechtenstein; Switzerland.\"],\n        BBOX[45.82,5.96,47.81,10.49]],\n    ID[\"EPSG\",2056]]\n\n\nWe get the full description of the CRS with all its parameters. The format used is unfortunately also named WKT, but this has nothing to do with the WKT format used to define geometries. If you use EPSG codes, you can also simply enter the code as an integer (please don’t do this to avoid confusion).\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nTry to understand some of the elements of the output of the st_crs() function. Try it with a geographic CRS.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Coordinate reference systems</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html",
    "href": "vectors_tips.html",
    "title": "5  Tips and tricks for vectors",
    "section": "",
    "text": "5.1 Reading vector data\nWe had a look at the gory details of the internal structure sf objects. However most of the time you will not create such objects on your own but rather rely on the sf package to create the right structure when you import existing GIS data. The sf package is using the GDAL (Geospatial Data Abstraction Library) library to read GIS files, and this means you will be able to import almost all existing GIS vector formats. If you want to check all the available formats, you can use the st_drivers() function. Sometimes you will not get a standard GIS data set but a simple CSV (or Excel) file containing coordinates and related attributes. We will now see how to import these different data types in order to use them with the sf package.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#reading-vector-data",
    "href": "vectors_tips.html#reading-vector-data",
    "title": "5  Tips and tricks for vectors",
    "section": "",
    "text": "If you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\n\n\n\n\n\n\n5.1.1 Import a GeoPackage\nThe GeoPackage format is the best available open format to store vector data. It is based on the SQLite database format which is the most used file-based database nowadays. You can think of it as a special folder containing one or several GIS data sets. Since we normally don’t know in advance if a GeoPackage contains one or more data sets, we first have to inspect it.\n\nst_layers(\"data/geodata.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n      layer_name     geometry_type features fields       crs_name\n1        streets Multi Line String     7690      3 CH1903+ / LV95\n2      landcover     Multi Polygon     1038      2 CH1903+ / LV95\n3      buildings     Multi Polygon    14125      2 CH1903+ / LV95\n4 municipalities     Multi Polygon        7      3 CH1903+ / LV95\n5            wtf           Polygon        3      0 CH1903+ / LV95\n6        cantons  3D Multi Polygon       26      2 CH1903+ / LV95\n\n\nYou should not always trust the reported number of features. Some GIS format such as the GeoPackage report this number, some don’t. If the GeoPackage was produced by a software that doesn’t properly implement the standard, the reported number of features could be wrong (but this shouldn’t have any other bad consequence). If you want to be sure to get the correct number, you can use the do_count = TRUE argument of the st_layers() function, but this will be slower.\nTo read the data, you use the st_read() function, the first argument is the path of the GeoPackage, and the second argument is the layer you want to import. The function will return an sf object. By default you’ll get some information about the data being imported. If you don’t need them, you can use the argument quiet = TRUE.\n\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\")\n\nReading layer `municipalities' from data source \n  `C:\\Users\\jgu\\Desktop\\Spatial Data Processing with R\\data\\geodata.gpkg' \n  using driver `GPKG'\nSimple feature collection with 7 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2648317 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\n\nstreets &lt;- st_read(\"data/geodata.gpkg\", \"streets\", quiet = TRUE)\n\nLet’s check the object we’ve just created. To inspect sf objects, you can either call them directly or use the print() function. The head() and tail() functions will also work since sf objects are based on data frames. By default, only the first 10 rows will be displayed. If you want to see more (or less) rows, use the n argument of the print() function.\n\nmuni\n\nSimple feature collection with 7 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2648317 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\n   bfs       name popsize                           geom\n1 1093 Neuenkirch    7194 MULTIPOLYGON (((2654554 121...\n2 1094    Nottwil    4089 MULTIPOLYGON (((2654554 121...\n3 1102    Sempach    4186 MULTIPOLYGON (((2656062 121...\n4 1095  Oberkirch    5014 MULTIPOLYGON (((2649729 122...\n5 1084       Eich    1610 MULTIPOLYGON (((2654640 122...\n6 1099   Schenkon    3088 MULTIPOLYGON (((2652397 122...\n7 1103     Sursee   10382 MULTIPOLYGON (((2648801 122...\n\nprint(muni, n = 2)\n\nSimple feature collection with 7 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2648317 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\nFirst 2 features:\n   bfs       name popsize                           geom\n1 1093 Neuenkirch    7194 MULTIPOLYGON (((2654554 121...\n2 1094    Nottwil    4089 MULTIPOLYGON (((2654554 121...\n\n\nThe output contains basic information about the data set, and the first features are shown with all the attributes. This municipalities data set is an extract of the swissBOUNDARIES3D data set provided by Swisstopo, the streets data set is an extract of the swissTLM3D data set, also provided by Swisstopo.\n\n\n5.1.2 Import a Shapefile\nIf you really need to import a Shapefile, you should also use the st_read() function. Since Shapefiles cannot contain more than one data set, we only need to provide the first argument of the function. A Shapefile consists of several files with different extensions (.shp, .shx, etc.), we use the .shp extension by default when importing.\n\nmuni2 &lt;- st_read(\"data/municipalities.shp\", quiet = TRUE)\nmuni2\n\nSimple feature collection with 7 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2648317 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\n  fid  bfs       name                       geometry\n1   1 1093 Neuenkirch POLYGON ((2654554 1217985, ...\n2   2 1094    Nottwil POLYGON ((2654554 1217985, ...\n3   3 1102    Sempach POLYGON ((2656062 1219916, ...\n4   4 1095  Oberkirch POLYGON ((2649729 1221262, ...\n5   5 1084       Eich POLYGON ((2654640 1224638, ...\n6   6 1099   Schenkon POLYGON ((2652397 1224195, ...\n7   7 1103     Sursee POLYGON ((2648801 1225672, ...\n\n\nThis is actually the same data set as the one in the GeoPackage, however, note that sf is now using polygons instead of multipolygons. This is caused by the fact that the Shapefile format does not distinguish properly between the two types. The GDAL library will use the type polygons in this case, but you can still have a combination of polygons and multipolygons in the same data set.\n\n\n5.1.3 Import a CSV file with coordinates\nIf you have a table containing coordinates of point data (e.g. sites or bird sightings), you should use the st_as_sf() function. The first argument should be the data frame containing the data, and you also need to specify the names (or the numbers) of the columns containing the geographic coordinates, and the CRS used. The following data set was extracted from the bird sightings database of the Swiss Ornithological Institute.\n\nobs &lt;- read.csv(\"data/observations.csv\")\nhead(obs)\n\n  species_id               name       date       x       y\n1       4240 Eurasian Blackbird 2022-04-12 2658433 1220946\n2       3800  Eurasian Blue Tit 2022-04-12 2658442 1221138\n3       4240 Eurasian Blackbird 2022-05-09 2658607 1221189\n4       4240 Eurasian Blackbird 2022-05-09 2658597 1221137\n5       4240 Eurasian Blackbird 2022-05-09 2658569 1220956\n6       3800  Eurasian Blue Tit 2022-05-09 2658476 1220904\n\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nobs\n\nSimple feature collection with 530 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2649549 ymin: 1218571 xmax: 2660110 ymax: 1225531\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n   species_id               name       date                geometry\n1        4240 Eurasian Blackbird 2022-04-12 POINT (2658433 1220946)\n2        3800  Eurasian Blue Tit 2022-04-12 POINT (2658442 1221138)\n3        4240 Eurasian Blackbird 2022-05-09 POINT (2658607 1221189)\n4        4240 Eurasian Blackbird 2022-05-09 POINT (2658597 1221137)\n5        4240 Eurasian Blackbird 2022-05-09 POINT (2658569 1220956)\n6        3800  Eurasian Blue Tit 2022-05-09 POINT (2658476 1220904)\n7        4240 Eurasian Blackbird 2022-05-09 POINT (2658514 1221205)\n8        3800  Eurasian Blue Tit 2022-05-09 POINT (2658517 1221195)\n9        4240 Eurasian Blackbird 2022-05-23 POINT (2658570 1221219)\n10       4240 Eurasian Blackbird 2022-05-23 POINT (2658455 1220911)\n\n\nIf you have data in WGS84, your geometry columns will probably be named longitude and latitude. In this case remember that longitude corresponds to the x coordinate and latitude to the y coordinate. This can be sometimes confusing because most geographic CRSs have a “reversed” axis order (which means latitude is stored before longitude). To be honest the situation is even more complicated than that, since in geodesy, the convention is to let the x axis point to the North and the y axis to the East (more information: https://wiki.osgeo.org/wiki/Axis_Order_Confusion).\nFortunately, thanks to the great job done by the programmers of the PROJ and GDAL libraries, you don’t really have to think about all this chaos. Just remember the rule mentioned above and you should be safe 99.99% of the time. You should thus use (note the different value for the crs argument):\n\nobs_wgs &lt;- read.csv(\"data/observations_wgs.csv\")\nobs_wgs &lt;- st_as_sf(obs_wgs, coords = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")\n\n\n\n5.1.4 Import from a PostGIS database\nPostGIS is a famous open-source extension for the PostgreSQL database management system. It allows storing all kind of GIS data inside a database and perform hundreds of typical GIS analyses. The Swiss Ornithological Institute is using PostGIS to store almost all its bird data and a lot of other GIS data sets. If you have a laptop provided by the institute and you already accessed our database via QGIS, you should be able to run the following code. The process will be the same for all PostGIS databases.\nFirst we need to load the RPostgres package which provides function to access PostgreSQL databases (and hence PostGIS, too). There is another package providing similar functionality called RPostgreSQL, but in my opinion the RPostgres is better maintained and I experienced less problems.\nAfter storing all the connection details in some variables, we can finally create a connection to the database using the dbConnect() function.\n\nlibrary(RPostgres)\n\n# Login data\nuser &lt;- \"replace_with_your_login\"\npassword &lt;- \"replace_with_your_password\"\nhost &lt;- \"dbspatialdata1\"\ndatabase &lt;- \"research\"\n\n# Connection to the database\ndbcon &lt;- dbConnect(Postgres(), dbname = database, host = host, user = user, password = password)\n\nAfter that we need to import the data with a query, using again the st_read() function. Note that the first argument of the function must be the database connection object. The first possibility consists of importing the whole layer (called table in the database lingo) with all its attributes. This is what we do for the cantons1 data set. We need to use the Id() function to specify the location of the table inside the database. In a PostgreSQL database, a schema is a bit like a folder where we store tables, this allows us to implement some structure inside the database. In our case the table cantonal_boundaries_ch is stored inside the schema perimeter. Note that we don’t need the Id() function if the table are stored in the public schema.\nWe can also specify a SQL query to import the data, like for the cantons2 data set. Using this kind of query, we are fully flexible. We can for example specify the attributes we want to import, specific data filters, we can even join different tables together (by attributes or even spatially). Once again we have to specify the schema, but this is done a bit differently.\nOnce we have our sf objects, we still need to disconnect the database. The cantonal_boundaries_ch data set contains all the cantonal boundaries in Switzerland. The data is provided by Swisstopo (swissBOUNDARIES3D).\n\n# Load cantonal boundaries\ncantons1 &lt;- st_read(dbcon, layer = Id(schema = \"perimeter\", table = \"cantonal_boundaries_ch\"))\ncantons2 &lt;- st_read(dbcon, query = \"SELECT id, name, geom\n                                    FROM perimeter.cantonal_boundaries_ch\n                                    WHERE name = 'Fribourg'\")\n\n# Disconnect database\ndbDisconnect(dbcon)\n\n# Show sf objects\ncantons1\ncantons2\n\n\n\n5.1.5 Import from WKB\nYou probably won’t need to import geometries in the WKB format very often. GIS data should not be shared directly in this format. However, since it’s the default format used to store geometries in the PostGIS database, you will maybe get one day a table with attributes and a single WKB column. The following table is a direct extract from a PostGIS database.\n\nobs_wkb &lt;- read.csv(\"data/observations_wkb.csv\")\nhead(obs_wkb)\n\n  species_id               name       date\n1       3800  Eurasian Blue Tit 2020-12-07\n2       3800  Eurasian Blue Tit 2020-04-13\n3       3800  Eurasian Blue Tit 2020-09-12\n4       4240 Eurasian Blackbird 2020-10-22\n5       4240 Eurasian Blackbird 2020-07-04\n6       3800  Eurasian Blue Tit 2020-02-02\n                                                 wkb\n1 01010000200808000000000000DD3D444100000000FCA13241\n2 01010000200808000000000000C63D44410000000030A23241\n3 01010000200808000000000000E03F44410000000059953241\n4 010100002008080000000000001D3E44410000000077A13241\n5 010100002008080000000000009E3E4441000000007CA33241\n6 01010000200808000000000000E83C4441000000001CA43241\n\n\nUnfortunately it is not possible to use the st_read() function to import such data, and the st_as_sf() function won’t work either. In this case we need to first convert the WKB geometries into sfc objects. To do that we need to add an extra attribute using the structure() function to inform sf that we’re using WKB geometries. After that we can use the st_as_sfc() function. The EWKB = TRUE means that we are using a WKB dialect called EWKB (Extended WKB) which also includes the SRID of the geometries. Once we have a vector of sfc objects, we remove the now useless column containing the WKB geometries and use the st_sf() function to combine the data frame with the geometries.\n\ngeom &lt;- st_as_sfc(structure(as.list(obs_wkb$wkb), class = \"WKB\"), EWKB = TRUE)\nobs_wkb &lt;- subset(obs_wkb, select = -wkb)\nobs_wkb &lt;- st_sf(obs_wkb, geom)\nobs_wkb\n\nSimple feature collection with 21 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2652408 ymin: 1217881 xmax: 2654144 ymax: 1221958\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n   species_id               name       date                    geom\n1        3800  Eurasian Blue Tit 2020-12-07 POINT (2653114 1221116)\n2        3800  Eurasian Blue Tit 2020-04-13 POINT (2653068 1221168)\n3        3800  Eurasian Blue Tit 2020-09-12 POINT (2654144 1217881)\n4        4240 Eurasian Blackbird 2020-10-22 POINT (2653242 1220983)\n5        4240 Eurasian Blackbird 2020-07-04 POINT (2653500 1221500)\n6        3800  Eurasian Blue Tit 2020-02-02 POINT (2652624 1221660)\n7        4240 Eurasian Blackbird 2020-04-10 POINT (2653388 1218372)\n8        3800  Eurasian Blue Tit 2020-10-22 POINT (2653088 1221156)\n9        3800  Eurasian Blue Tit 2020-10-22 POINT (2652943 1221333)\n10       3800  Eurasian Blue Tit 2020-10-22 POINT (2653180 1220958)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#writing-vector-data",
    "href": "vectors_tips.html#writing-vector-data",
    "title": "5  Tips and tricks for vectors",
    "section": "5.2 Writing vector data",
    "text": "5.2 Writing vector data\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nWhen you create or modify a GIS data set with sf you’ll need to export it to some standard GIS format if you want to share it with colleagues, open it in another GIS software, or simply archive it. I do not recommend using R workspaces (.Rdata files) to share or store GIS data. For exporting vector data, we are going to use the st_write() function. Like the st_read() function, it uses the GDAL library, so you’ll be able to export in many different formats. You can specify the format explicitly, otherwise sf will try to guess it based on the file extension. It is also possible to export to a PostGIS database using an approach similar to the one we used for importing.\n\n5.2.1 Export to GeoPackage\nFor a GeoPackage, you need to specify the name of the GeoPackage first (it will be automatically created if it doesn’t exist) and the name of the data set that will be stored inside the GeoPackage. If you specify a GeoPackage that already exists, the data set will be added to it as a new table.\n\nst_write(obs, \"export/birds.gpkg\", \"observations\")\n\nWriting layer `observations' to data source `export/birds.gpkg' using driver `GPKG'\nWriting 530 features with 3 fields and geometry type Point.\n\nobs2 &lt;- obs[1:10,]\nst_write(obs2, \"export/birds.gpkg\", \"observations2\", quiet = TRUE)\nst_layers(\"export/birds.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n     layer_name geometry_type features fields       crs_name\n1  observations         Point      530      3 CH1903+ / LV95\n2 observations2         Point       10      3 CH1903+ / LV95\n\n\nIf you want to delete a data set, you can use the st_delete() function. Think twice before doing it, there will be no warning!\n\nst_delete(\"export/birds.gpkg\", \"observations2\")\n\nDeleting layer `observations2' using driver `GPKG'\n\n\n\n\n5.2.2 Export to CSV\nIt is usually a better option to export a GeoPackage, but sometimes you’ll still need to export your data to CSV. When you share such a file, always add metadata about the CRS you used. Since CSV is not a GIS format, we need a way to store the geometries. This is easy for point data since we can always add columns with the coordinates. For line and polygon data we need to find another solution, for example store the WKT in a new column.\nWe can use the layer_options argument of the st_write() function to export point data with the additional columns for the x and y coordinates. These additional options are sent directly to the GDAL library which does the export.\n\nst_write(obs, \"export/birds.csv\", layer_options = \"GEOMETRY=AS_XY\")\n\nWriting layer `birds' to data source `export/birds.csv' using driver `CSV'\noptions:        GEOMETRY=AS_XY \nWriting 530 features with 3 fields and geometry type Point.\n\n\nFor polygon and line data, we can do something similar to get a new column with the WKT geometry. Since commas are used in the WKT format, it might be a good idea to use another separator for the CSV file. Note that you’ll probably get into troubles if your geometries have a lot of vertices.\n\nst_write(muni, \"export/municipalities.csv\", layer_options = c(\"GEOMETRY=AS_WKT\", \"SEPARATOR=SEMICOLON\"), quiet = TRUE)\n\n\n\n5.2.3 Export to Shapefile\nReally??? Please don’t!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#basic-geometric-computations",
    "href": "vectors_tips.html#basic-geometric-computations",
    "title": "5  Tips and tricks for vectors",
    "section": "5.3 Basic geometric computations",
    "text": "5.3 Basic geometric computations\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nstreets &lt;- st_read(\"data/geodata.gpkg\", \"streets\", quiet = TRUE)\n\n\n\n\nIn this section we’ll see how to perform some basic geometric computations on spatial data, such as computing area, perimeter, length and centroids. We’ll also learn how to display the coordinates of sf geometries.\n\n5.3.1 Areas and lengths\nAs a first example, let’s how we can compute the area and perimeter of polygons, or the length of lines.\n\nst_area(muni)\n\nUnits: [m^2]\n[1] 26267403 14834107 11676584 10954968  9218391  7710103  6050585\n\nst_perimeter(muni)\n\nUnits: [m]\n[1] 29717.62 17281.70 16758.24 15754.38 14897.75 15462.88 13398.87\n\nhead(st_length(streets))\n\nUnits: [m]\n[1] 1915.1026  378.9425  345.9435 1836.5854  901.2759  232.4974\n\n\nNote that the results always have a unit of measurement. This is a feature that is provided by sf and will occur will all functions giving some sort of measurement. This is compatible with the units package which allows easy conversions between different unit types. However this can sometimes be a problem if you need a “raw” value. In this case you can use the as.numeric() function to remove the units.\n\n\n\n\n\n\nNote\n\n\n\nThe st_perimeter() function was not available in older versions of the sf package. The lwgeom package was needed to perform this computation.\n\n\nIf you use unprojected data (i.e., with a geographic CRS), sf will automatically use the s2 library to compute areas, perimeters and lengths.\n\nst_area(World[1:5,])\n\nUnits: [m^2]\n[1] 6.524913e+11 2.965392e+10 2.319313e+12 1.250265e+12 1.223080e+13\n\nst_perimeter(World[1:5,])\n\nUnits: [m]\n[1]  4552620.2   831564.7  6782465.9  6031452.9 29709970.9\n\n\nThe s2 library performs its computations on a spheroid. For areas and lengths, it is possible to obtain a better approximation by using an ellipsoid. You can do this by turning off the s2 library with the sf_use_s2(FALSE) function. In this case, sf will automatically use equivalent functions provided by the lwgeom1 package. These functions use algorithms from the GeographicLib library which are to my knowledge the most precise approximations you can currently get. Note that the st_perimeter() function won’t be available if you do that. You’ll need to transform the polygons to lines first (see Section 5.5 for details), and then use the st_length() function. Don’t forget to reactivate s2 (using the sf_use_s2(TRUE) function) when you’re done.\n\nsf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\nst_area(World[1:5,])\n\nUnits: [m^2]\n[1] 6.522790e+11 2.969314e+10 2.315867e+12 1.245470e+12 1.233046e+13\n\nst_length(st_cast(World[1:5,], \"MULTILINESTRING\"))\n\nUnits: [m]\n[1]  4553652.7   831351.2  6779702.5  6020175.2 29826094.4\n\nsf_use_s2(TRUE)\n\nSpherical geometry (s2) switched on\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should normally not turn s2 off. Computing areas and lengths (and distances, as we will see later) are probably the only valid cases where turning s2 off is a good idea. For all other computations based on geographic CRSs you should NOT deactivate s2, otherwise you’ll get results that will most probably be wrong.\n\n\n\n\n5.3.2 Centroids\nComputing the centroid of polygons is another useful operation that is easily computed using the st_centroid() function.\n\nmuni_centroid &lt;- st_centroid(muni)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nplot(st_geometry(muni))\nplot(st_geometry(muni_centroid), add = TRUE)\n\n\n\n\n\n\n\n\nFor this example, you can safely ignore the warning about attributes assumed to be constant. The output of the st_centroid() function will be a point data set with the same number of features and the same attributes as the data set used to compute the centroids. The function simply warns you that if the value of some attribute is not constant for some polygon, then the value of this attribute for the centroid probably doesn’t make a lot of sense.\n\n\n\n\n\n\nTip\n\n\n\nCentroids can be used to place labels inside polygons, but don’t forget that polygons with strange shapes may not contain their own centroid. If you need to be sure that the point will be inside the polygon, use the st_point_on_surface() function.\n\n\n\n\n5.3.3 Extract coordinates\nSometimes we also need to know the coordinates of the sf objects we’re using. For points we of course get the coordinates of the points, for lines and polygons we get the coordinates of the vertices, with additional columns showing how to reconstruct the features (check the st_coordinates() help page to understand the meaning of L1, L2 and L3).\n\nst_coordinates(muni_centroid)\n\n           X       Y\n[1,] 2657961 1217280\n[2,] 2653276 1220227\n[3,] 2657334 1221089\n[4,] 2650683 1222874\n[5,] 2655114 1223114\n[6,] 2652705 1225584\n[7,] 2650517 1225371\n\nhead(st_coordinates(muni))\n\n           X       Y L1 L2 L3\n[1,] 2654554 1217985  1  1  1\n[2,] 2654481 1218031  1  1  1\n[3,] 2654492 1218051  1  1  1\n[4,] 2654495 1218063  1  1  1\n[5,] 2654493 1218067  1  1  1\n[6,] 2654494 1218083  1  1  1\n\n\n\n\n5.3.4 Common pitfalls\nUnfortunately geometric computations are not always that easy… Let’s have a look at another example. We load another polygon layer and try to compute the area of each polygon.\n\nbug &lt;- st_read(\"data/geodata.gpkg\", \"wtf\", quiet = TRUE)\nplot(bug, col = 1:nrow(bug))\n\n\n\n\n\n\n\nst_area(bug)\n\nUnits: [m^2]\n[1] 547988.3 200939.1      0.0\n\n\nOops, these polygons look big enough but one of them seems to have an area of 0… Why is this happening? To understand the problem, we first need to talk a bit about geometric validity…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#geometric-validity",
    "href": "vectors_tips.html#geometric-validity",
    "title": "5  Tips and tricks for vectors",
    "section": "5.4 Geometric validity",
    "text": "5.4 Geometric validity\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nbug &lt;- st_read(\"data/geodata.gpkg\", \"wtf\", quiet = TRUE)\n\n\n\n\nWhen we had a look a the vector data model, we discovered the Simple Feature standard but we forgot an important part: geometric validity. Validity is defined a bit differently depending on the geometry engine used for the computations. First the good news: points are always valid! Lines are always valid for the GEOS engine used by sf but they are considered invalid by QGIS if they have self-intersections (such lines are called non-simple). Polygons are definitely invalid if they have self-intersections (like our example). The other invalid cases are shown on this website: https://postgis.net/docs/using_postgis_dbmanagement.html#Valid_Geometry. Using invalid geometries can be problematic for some analyses, such as computing areas.\nNormally we should expect official data sets to be valid but this is often not the case. You can check the validity of each feature using the st_is_valid() function. If you want a short description of the problems, you can add the argument reason = TRUE.\n\nbug\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\n                            geom\n1 POLYGON ((2656382 1221396, ...\n2 POLYGON ((2656903 1220285, ...\n3 POLYGON ((2658317 1221618, ...\n\nst_is_valid(bug)\n\n[1]  TRUE  TRUE FALSE\n\nst_is_valid(bug, reason = TRUE)\n\n[1] \"Valid Geometry\"                        \n[2] \"Valid Geometry\"                        \n[3] \"Self-intersection[2658817 1221117.875]\"\n\n\nIf there are only a few invalid features, we can correct them manually in QGIS. But sometimes this is not feasible and we need some automatic way of correcting. This is where the st_make_valid() function shines. Even though it’s fully automatic, it will perform the appropriate changes 99% of the time. The function can use two different algorithms to correct geometries, you can choose which one will be used with the argument geos_method. The default algorithm (“valid_structure”) is more recent and should produce better results in most cases. Try the older one (“valid_linework”) if you’re not happy with the results. Check the following webpage to see the differences between the two algorithms (it is written for PostGIS but sf uses the same geometry engine): https://www.crunchydata.com/blog/waiting-for-postgis-3.2-st_makevalid\n\nbug_valid &lt;- st_make_valid(bug)\nst_is_valid(bug_valid)\n\n[1] TRUE TRUE TRUE\n\nbug_valid\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\n                            geom\n1 POLYGON ((2656382 1221396, ...\n2 POLYGON ((2656903 1220285, ...\n3 MULTIPOLYGON (((2658317 122...\n\nst_geometry_type(bug_valid)\n\n[1] POLYGON      POLYGON      MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nst_geometry_type(bug_valid, by_geometry = FALSE)\n\n[1] GEOMETRY\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\nWhen we look at the corrected data set, we see that the invalid polygon was converted to a multipolygon. We can also check it using the st_geometry_type() function. This is however a problem since we normally don’t want a data set with mixed geometry types. When we use the by_geometry = FALSE argument, we see that sf is now using a generic GEOMETRY type for the data set. The solution would be to convert all the other polygons to multipolygons. To do that we need to understand type casting.\n\n\n\n\n\n\nImportant\n\n\n\nYou’ve maybe heard of the buffer trick. It consists of computing a 0 meter buffer around each geometry to make them valid. This does work and make everything valid, but you may lose some parts of the geometries. For example, if you have a polygon with a self-intersection. The smaller part will not be retained.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#sec-typecasting",
    "href": "vectors_tips.html#sec-typecasting",
    "title": "5  Tips and tricks for vectors",
    "section": "5.5 Vector type casting",
    "text": "5.5 Vector type casting\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nbug &lt;- st_read(\"data/geodata.gpkg\", \"wtf\", quiet = TRUE)\nbug_valid &lt;- st_make_valid(bug)\n\n\n\n\nChanging the type of vector features is done with the st_cast() function. Using this function you can not only disaggregate geometries with MULTI* types into several unique features (e.g., multipolygon to polygon) or extract simpler types (e.g., extract polygon borders or vertices), but also construct geometries using “simpler” geometry types (e.g., build a line from points).\n\n5.5.1 Polygons to Multipolygons\nWe can now solve our previous problem and convert everything to multipolygons (the existing multipolygon will be left untouched). Using the st_as_text() function we can see the WKT representation of the features geometry, and confirm that we’re now using the same vector type for all features. The st_geometry_type() function also tells us that the data set type is now multipolygon.\n\nbug_multipoly &lt;- st_cast(bug_valid, to = \"MULTIPOLYGON\")\nst_as_text(st_geometry(bug_valid))\n\n[1] \"POLYGON ((2656382 1221396, 2657007 1221587, 2657250 1221414, 2657389 1221014, 2656730 1220824, 2656313 1221084, 2656382 1221396))\"                            \n[2] \"POLYGON ((2656903 1220285, 2657441 1220077, 2657632 1220459, 2657754 1220129, 2657458 1219730, 2656903 1220285))\"                                             \n[3] \"MULTIPOLYGON (((2658317 1221618, 2658817 1221118, 2658317 1220618, 2658317 1221618)), ((2658817 1221118, 2659317 1221618, 2659317 1220618, 2658817 1221118)))\"\n\nst_as_text(st_geometry(bug_multipoly))\n\n[1] \"MULTIPOLYGON (((2656382 1221396, 2657007 1221587, 2657250 1221414, 2657389 1221014, 2656730 1220824, 2656313 1221084, 2656382 1221396)))\"                     \n[2] \"MULTIPOLYGON (((2656903 1220285, 2657441 1220077, 2657632 1220459, 2657754 1220129, 2657458 1219730, 2656903 1220285)))\"                                      \n[3] \"MULTIPOLYGON (((2658317 1221618, 2658817 1221118, 2658317 1220618, 2658317 1221618)), ((2658817 1221118, 2659317 1221618, 2659317 1220618, 2658817 1221118)))\"\n\nst_geometry_type(bug_multipoly, by_geometry = FALSE)\n\n[1] MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\nIf you want to be absolutely sure that you have only one feature type in your data set, you can combine the unique() function with the st_geometry_type() function.\n\nunique(st_geometry_type(bug_valid))\n\n[1] POLYGON      MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nunique(st_geometry_type(bug_multipoly))\n\n[1] MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\n\n\n5.5.2 Polygons to other types\nWe can go a bit further and convert our multipolygons to other types. Converting polygons to lines will extract the rings, and converting to points will extract the vertices (similarly, casting a linestring to points will extract its vertices). Note: for some reasons, it is not possible to convert a multipolygon directly to a linestring. You’ll need to convert it to a multilinestring object first.\n\nbug_poly &lt;- st_cast(bug_multipoly, to = \"POLYGON\")\nbug_multiline &lt;- st_cast(bug_multipoly, to = \"MULTILINESTRING\")\nbug_line &lt;- st_cast(bug_multiline, to = \"LINESTRING\")\nbug_multipts &lt;- st_cast(bug_multipoly, to = \"MULTIPOINT\")\nbug_pts &lt;- st_cast(bug_multipoly, to = \"POINT\")\n\nbug_poly\n\nSimple feature collection with 4 features and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\n                            geom\n1 POLYGON ((2656382 1221396, ...\n2 POLYGON ((2656903 1220285, ...\n3 POLYGON ((2658317 1221618, ...\n4 POLYGON ((2658817 1221118, ...\n\nbug_multiline\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\n                            geom\n1 MULTILINESTRING ((2656382 1...\n2 MULTILINESTRING ((2656903 1...\n3 MULTILINESTRING ((2658317 1...\n\nbug_line\n\nSimple feature collection with 4 features and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\n                            geom\n1 LINESTRING (2656382 1221396...\n2 LINESTRING (2656903 1220285...\n3 LINESTRING (2658317 1221618...\n4 LINESTRING (2658817 1221118...\n\nbug_multipts\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\n                            geom\n1 MULTIPOINT ((2656382 122139...\n2 MULTIPOINT ((2656903 122028...\n3 MULTIPOINT ((2658317 122161...\n\nbug_pts\n\nSimple feature collection with 21 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2656313 ymin: 1219730 xmax: 2659317 ymax: 1221618\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n                      geom\n1  POINT (2656382 1221396)\n2  POINT (2657007 1221587)\n3  POINT (2657250 1221414)\n4  POINT (2657389 1221014)\n5  POINT (2656730 1220824)\n6  POINT (2656313 1221084)\n7  POINT (2656382 1221396)\n8  POINT (2656903 1220285)\n9  POINT (2657441 1220077)\n10 POINT (2657632 1220459)\n\n\nIn the following figure, each feature has a unique color. It is thus easy to visualize the difference between the MULTI* types and the other ones.\n\npar(mfrow = c(2, 3))\nplot(st_geometry(bug_multipoly), col = rainbow(nrow(bug_multipoly)), main = \"Multipolygons\")\nplot(st_geometry(bug_multiline), col = rainbow(nrow(bug_multiline)), main = \"Multilines\")\nplot(st_geometry(bug_multipts), col = rainbow(nrow(bug_multipts)), pch = 16, main = \"Multipoints\")\nplot(st_geometry(bug_poly), col = rainbow(nrow(bug_poly)), main = \"Polygons\")\nplot(st_geometry(bug_line), col = rainbow(nrow(bug_line)), main = \"Lines\")\nplot(st_geometry(bug_pts), col = rainbow(nrow(bug_pts)), pch = 16, main = \"Points\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Points to lines\nIt is of course not possible to convert points directly to polygons, but if you have an sf object with points in the right order, you can easily build a linestring. As an example, let’s extract the first 10 vertices of the Sempach multipolygon. Once you have an sf object with ordered points, you need to group them into a single multipoints geometry using the st_combine() function, and then call the st_cast() function on this new object.\n\nsempach_pts &lt;- st_cast(muni[muni$name == \"Sempach\",], to = \"POINT\")[1:10,]\nsempach_multipts &lt;- st_combine(sempach_pts)\nsempach_line &lt;- st_cast(sempach_multipts, \"LINESTRING\")\npar(mfrow = c(1, 2))\nplot(st_geometry(sempach_pts), pch = 16, main = \"Points\")\ntext(sempach_pts, 1:nrow(sempach_pts), pos = 4, cex = 0.8)\nplot(sempach_line, lwd = 2, col = \"navy\", main = \"Linestring\")\n\n\n\n\n\n\n\n\n\n\n5.5.4 Lines to polygons\nIf you have a linestring or multilinestring geometry forming a closed ring, you can easily convert it to a polygon. As an example, let’s use the outer ring of the Sempach multipolygon.\n\nsempach_multiline &lt;- st_cast(muni[muni$name == \"Sempach\",], to = \"MULTILINESTRING\")\nsempach_poly &lt;- st_cast(sempach_multiline, to = \"POLYGON\")\npar(mfrow = c(1, 2))\nplot(st_geometry(sempach_multiline), col = \"navy\", main = \"Multilinestring\")\nplot(st_geometry(sempach_poly), col = \"navy\", main = \"Polygon\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nCreate a new object containing only the polygon for Sursee, check its validity and compute its perimeter using the st_length() function, compare with the result of the st_perimeter() function.\n\n\nCode\nsursee_poly &lt;- muni[muni$name == \"Sursee\",]\nsursee_multiline &lt;- st_cast(sursee_poly, to = \"MULTILINESTRING\")\nst_length(sursee_multiline)\nst_perimeter(sursee_poly)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#spatial-predicates",
    "href": "vectors_tips.html#spatial-predicates",
    "title": "5  Tips and tricks for vectors",
    "section": "5.6 Spatial predicates",
    "text": "5.6 Spatial predicates\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nstreets &lt;- st_read(\"data/geodata.gpkg\", \"streets\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nTopology describes the spatial relationships between vector objects. For example, two features can intersect, or one feature can contain another one. The existence of such relationships between features is tested by functions called spatial (binary) predicates. Many are available in the sf package, use ?geos_binary_pred if you want to see the full list.\n\n\n\n\n\n\nImportant\n\n\n\nWhen using spatial predicates you must be sure that both objects use the same CRS.\n\n\nWe can for example easily test whether bird sightings are located in Sempach, or somewhere else.\n\nsempach &lt;- muni[muni$name == \"Sempach\",]\nobs_in_sempach &lt;- st_intersects(obs, sempach)\nobs_in_sempach\n\nSparse geometry binary predicate list of length 530, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 1\n 2: 1\n 3: 1\n 4: 1\n 5: 1\n 6: 1\n 7: 1\n 8: 1\n 9: 1\n 10: 1\n\nsummary(lengths(obs_in_sempach) &gt; 0)\n\n   Mode   FALSE    TRUE \nlogical     222     308 \n\n\nThe output is stored in an memory efficient sparse matrix format which is not always easily readable by humans. We can use the sparse = FALSE argument to get a non-sparse matrix and perform standard operations (e.g. computing the number of sightings in Sempach).\n\nobs_in_sempach &lt;- st_intersects(obs, sempach, sparse = FALSE)\ntail(obs_in_sempach)\n\n        [,1]\n[525,] FALSE\n[526,] FALSE\n[527,]  TRUE\n[528,] FALSE\n[529,]  TRUE\n[530,] FALSE\n\nsum(obs_in_sempach)\n\n[1] 308\n\n\nUsing another predicate (st_disjoint), we can get a list of all sightings that are in other municipalities. Of course, this computation is superfluous in this case since the output of the st_disjoint() function is the complement of the set provided by the st_intersects() function.\n\nobs_not_in_sempach &lt;- st_disjoint(obs, sempach, sparse = FALSE)\nsum(obs_not_in_sempach)\n\n[1] 222\n\n\nWe can easily find all the sightings that are located within 1km of the Swiss Ornithological Institute.\n\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\nst_is_within_distance(soi, obs, dist = 1000)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `is_within_distance'\n 1: 62, 63, 89, 90, 92, 113, 114, 123, 135, 136, ...\n\n\nThings get a bit more complex when the two elements used inside the predicate contain multiple features. In this example we test for intersections between two municipalities and all the highway segments stored in the streets data set.\n\nmuni_extract &lt;- muni[6:7,]\nhighways &lt;- streets[streets$type == 2,]\nst_intersects(muni_extract, highways)\n\nSparse geometry binary predicate list of length 2, where the predicate\nwas `intersects'\n 1: 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...\n 2: 1, 2, 3, 4, 5, 19, 20, 58\n\n\nDon’t hesitate to try other predicates (e.g. st_within(), st_contains()). The difference between some of them is sometimes quite subtle (e.g., the influence of the feature border). If you need even more flexibility you should use the st_relate() function. This flexibility comes with a price, though. The st_relate() function is much slower since it doesn’t use spatial indices. If you want an in-depth explanation of all the possibilities, you should check the following website: https://en.wikipedia.org/wiki/DE-9IM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#spatial-subsetting",
    "href": "vectors_tips.html#spatial-subsetting",
    "title": "5  Tips and tricks for vectors",
    "section": "5.7 Spatial subsetting",
    "text": "5.7 Spatial subsetting\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nsempach &lt;- muni[muni$name == \"Sempach\",]\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\n\n\n\n\nNow that we know how to test different topological properties, we can use them to subset data spatially. The sf package allows doing that using the usual [] notation. The st_intersects predicate is used by default if you don’t specify anything. This is how we create a new sf object containing only the sightings in Sempach.\n\nobs_in_sempach &lt;- obs[sempach,]\n# Equivalent to\nobs_in_sempach &lt;- obs[sempach, , op = st_intersects]\n\nThe empty argument can be used to specify the desired attribute columns.\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nWithin a 2 km radius around Swiss Ornithological Institute, how many bird sightings are in Neuenkirch? Try to make a map of the municipalities with the filtered sightings.\n\n\nCode\nneuenkirch &lt;- muni[muni$name == \"Neuenkirch\",]\nobs_within_2000m &lt;- obs[soi, , op = st_is_within_distance, dist = 2000]\nobs_filtered &lt;- obs_within_2000m[neuenkirch,]\nnrow(obs_filtered)\nplot(st_geometry(muni))\nplot(st_geometry(obs_filtered), add = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#spatial-joins",
    "href": "vectors_tips.html#spatial-joins",
    "title": "5  Tips and tricks for vectors",
    "section": "5.8 Spatial joins",
    "text": "5.8 Spatial joins\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nWe’ve already seen how to join a spatial object to another table using attributes. Now we’ll do something similar but instead of using attributes, we’ll perform a join between spatial objects based on their topological relationships. As a first example we will join the bird sightings data set with the municipalities data set. As output we will get the bird sightings with additional attributes corresponding to their respective municipality. We’ll do this using the st_join() function.\n\nobs_muni &lt;- st_join(obs, muni, join = st_intersects)\nobs_muni\n\nSimple feature collection with 530 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2649549 ymin: 1218571 xmax: 2660110 ymax: 1225531\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n   species_id             name.x       date  bfs  name.y popsize\n1        4240 Eurasian Blackbird 2022-04-12 1102 Sempach    4186\n2        3800  Eurasian Blue Tit 2022-04-12 1102 Sempach    4186\n3        4240 Eurasian Blackbird 2022-05-09 1102 Sempach    4186\n4        4240 Eurasian Blackbird 2022-05-09 1102 Sempach    4186\n5        4240 Eurasian Blackbird 2022-05-09 1102 Sempach    4186\n6        3800  Eurasian Blue Tit 2022-05-09 1102 Sempach    4186\n7        4240 Eurasian Blackbird 2022-05-09 1102 Sempach    4186\n8        3800  Eurasian Blue Tit 2022-05-09 1102 Sempach    4186\n9        4240 Eurasian Blackbird 2022-05-23 1102 Sempach    4186\n10       4240 Eurasian Blackbird 2022-05-23 1102 Sempach    4186\n                  geometry\n1  POINT (2658433 1220946)\n2  POINT (2658442 1221138)\n3  POINT (2658607 1221189)\n4  POINT (2658597 1221137)\n5  POINT (2658569 1220956)\n6  POINT (2658476 1220904)\n7  POINT (2658514 1221205)\n8  POINT (2658517 1221195)\n9  POINT (2658570 1221219)\n10 POINT (2658455 1220911)\n\n\nIn this example both data sets have an attributed called “name”. When we join them together, R is automatically renaming these columns to “name.x” and “name.y”. The “x” and “y” corresponds to the order of the data sets when calling the st_join() function. We can now easily compute the number of sightings per municipality.\n\ntable(obs_muni$name.y)\n\n\n      Eich Neuenkirch    Nottwil  Oberkirch   Schenkon    Sempach     Sursee \n         7         52         10         86         11        308         56 \n\n\nLet’s try another spatial join, this time we will join the sightings with a landcover data set, which is an extract of the swissTLM3D data set provided by Swisstopo. The goal of the analysis it to add a new attribute to the bird sightings data set corresponding to the landcover value.\n\nlandcover &lt;- st_read(\"data/geodata.gpkg\", \"landcover\", quiet = TRUE)\nobs_landcover &lt;- st_join(obs, landcover, join = st_intersects)\nobs_landcover\n\nSimple feature collection with 548 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2649549 ymin: 1218571 xmax: 2660110 ymax: 1225531\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n   species_id               name       date type year                geometry\n1        4240 Eurasian Blackbird 2022-04-12   NA   NA POINT (2658433 1220946)\n2        3800  Eurasian Blue Tit 2022-04-12   14 2013 POINT (2658442 1221138)\n3        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658607 1221189)\n4        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658597 1221137)\n5        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658569 1220956)\n6        3800  Eurasian Blue Tit 2022-05-09   10 2013 POINT (2658476 1220904)\n7        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658514 1221205)\n8        3800  Eurasian Blue Tit 2022-05-09   12 2013 POINT (2658517 1221195)\n9        4240 Eurasian Blackbird 2022-05-23   12 2013 POINT (2658570 1221219)\n10       4240 Eurasian Blackbird 2022-05-23   NA   NA POINT (2658455 1220911)\n\ntable(obs_landcover$type, useNA = \"always\")\n\n\n   6   10   11   12   14 &lt;NA&gt; \n   4   19   55   55   54  361 \n\n\nBy default the st_join() function will use st_intersects as a predicate, but you can of course specify a different one. Moreover it performs what is called a left join, which means the output will contain all rows from the first object (obs in our example). It is also possible to perform an inner join by adding the attribute left = FALSE. In this case the output will contain only the rows for which a value was found (i.e. where a spatial match occurred).\n\nobs_landcover_inner &lt;- st_join(obs, landcover, join = st_intersects, left = FALSE)\nobs_landcover_inner\n\nSimple feature collection with 187 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2649549 ymin: 1218822 xmax: 2659732 ymax: 1225026\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n     species_id               name       date type year                geometry\n2          3800  Eurasian Blue Tit 2022-04-12   14 2013 POINT (2658442 1221138)\n3          4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658607 1221189)\n4          4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658597 1221137)\n5          4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658569 1220956)\n6          3800  Eurasian Blue Tit 2022-05-09   10 2013 POINT (2658476 1220904)\n7          4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658514 1221205)\n8          3800  Eurasian Blue Tit 2022-05-09   12 2013 POINT (2658517 1221195)\n9          4240 Eurasian Blackbird 2022-05-23   12 2013 POINT (2658570 1221219)\n11         4240 Eurasian Blackbird 2022-06-06   11 2013 POINT (2658530 1220902)\n11.1       4240 Eurasian Blackbird 2022-06-06    6 2013 POINT (2658530 1220902)\n\ntable(obs_landcover_inner$type, useNA = \"always\")\n\n\n   6   10   11   12   14 &lt;NA&gt; \n   4   19   55   55   54    0 \n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nHave a look at the number of features (rows) of the original obs data set and compare it with the number of features of the obs_landcover data set. Why are they different?\n\n\nCode\nnrow(obs)\nnrow(obs_landcover)\n# This difference is caused by overlapping polygons in the landcover data set. If a bird sighting\n# is inside 2 polygons, the st_join() function will create 2 rows (one row for each intersecting polygon).\n# This means some sightings will be duplicated. We call this kind of joins \"one-to-many\".\n# You can find these duplicated sightings by looking at the row names with the rownames() function, or by\n# adding an unique ID to all the sightings before the spatial join and then looking for duplicated IDs.\n\ndupl &lt;- grep(\".\", rownames(obs_landcover), fixed = TRUE)\ndupl &lt;- sort(c(dupl, dupl - 1))\nobs_landcover[dupl,]\n\n\n\n\nSince the landcover data is not a complete coverage of our study area, which leads to NA values in our joined data set, we can maybe try to get more complete results by using another spatial predicate. The st_nearest_feature predicate will join the sightings to the nearest landcover polygon. Polygons containing points will be considered to be the closest ones. I honestly don’t know what is happening when a point is within overlapping polygons. My first thoughts were that it would take the attributes from the highest or lowest polygon in the stack of overlapping polygons, but there’s no clear pattern.\n\nobs_landcover2 &lt;- st_join(obs, landcover, join = st_nearest_feature)\nobs_landcover2\n\nSimple feature collection with 530 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2649549 ymin: 1218571 xmax: 2660110 ymax: 1225531\nProjected CRS: CH1903+ / LV95\nFirst 10 features:\n   species_id               name       date type year                geometry\n1        4240 Eurasian Blackbird 2022-04-12   11 2013 POINT (2658433 1220946)\n2        3800  Eurasian Blue Tit 2022-04-12   14 2013 POINT (2658442 1221138)\n3        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658607 1221189)\n4        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658597 1221137)\n5        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658569 1220956)\n6        3800  Eurasian Blue Tit 2022-05-09   10 2013 POINT (2658476 1220904)\n7        4240 Eurasian Blackbird 2022-05-09   12 2013 POINT (2658514 1221205)\n8        3800  Eurasian Blue Tit 2022-05-09   12 2013 POINT (2658517 1221195)\n9        4240 Eurasian Blackbird 2022-05-23   12 2013 POINT (2658570 1221219)\n10       4240 Eurasian Blackbird 2022-05-23   10 2013 POINT (2658455 1220911)\n\ntable(obs_landcover2$type, useNA = \"always\")\n\n\n   6   10   11   12   14 &lt;NA&gt; \n   8  144   61   60  257    0 \n\n\nIn this example we joined a point data set to a polygon data set and this is the most common application for a spatial join. However we’re not restricted to these combinations, we can join all the vector types (e.g. polygons with polygons). If you’re joining polygons to polygons, st_join() is also able to perform joins based on the maximum area overlay (it joins with the polygon having the largest overlap). To do this, you need to add the largest = TRUE argument.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#distance-operations",
    "href": "vectors_tips.html#distance-operations",
    "title": "5  Tips and tricks for vectors",
    "section": "5.9 Distance operations",
    "text": "5.9 Distance operations\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(tmap)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\n\n\n\n\nCalculating distances between sf objects is done with the st_distance() function.\n\nst_distance(obs[1,], soi)\n\nUnits: [m]\n         [,1]\n[1,] 1664.665\n\n\nOnce again, we see that sf is using units. We also note that the results are stored in a matrix instead of a vector. This format is actually needed since st_distance() can also be used to compute all the distance combinations between two sf objects containing multiple features.\n\nst_distance(obs[1:5,], muni[1:3,])\n\nUnits: [m]\n         [,1]     [,2] [,3]\n[1,] 438.6257 2585.144    0\n[2,] 424.5501 2675.472    0\n[3,] 254.1488 2845.707    0\n[4,] 286.7366 2813.814    0\n[5,] 302.5583 2714.238    0\n\n\nThe distances are measured between the points and the nearest edge of the polygons if they are located outside the polygons. The distance will be 0 if they are within the polygon.\nWe will now make a short excursion in the world of geographic CRS with a quick example showing what sf is doing when we don’t use standard Euclidean geometry. We will compute the distance between the centroids of Switzerland and Australia. By default sf is using the s2 library to perform its computations on a spheroid when we use a geographic CRS. However for distances, it is possible to obtain better approximations by using an ellipsoid. To do this, we need to tell sf to avoid using s2 with the function sf_use_s2(FALSE). The st_distance() will thus be forced to use a more precise algorithm (by automatically using a similar function from the lwgeom package). We can of course compare the two estimates.\n\nworld_centroids &lt;- st_centroid(World)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\npt1 &lt;- world_centroids[world_centroids$name == \"Switzerland\",]\npt2 &lt;- world_centroids[world_centroids$name == \"Australia\",]\n\nsf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\n(d1 &lt;- st_distance(pt1, pt2))\n\nUnits: [m]\n         [,1]\n[1,] 14776830\n\nsf_use_s2(TRUE)\n\nSpherical geometry (s2) switched on\n\n(d2 &lt;- st_distance(pt1, pt2))\n\nUnits: [m]\n         [,1]\n[1,] 14779871\n\nabs(d2 - d1)\n\nUnits: [m]\n         [,1]\n[1,] 3040.018\n\n\nThe distance we’re measuring is shown on the following map. Since the earth is not flat, the shortest distance between two points is not a straight line. These shortest distance lines are called great circles.\n\ngcLine &lt;- st_cast(st_combine(rbind(pt1, pt2)), \"LINESTRING\")\ngcLine &lt;- st_segmentize(gcLine, 1000)\n\nextent &lt;- World[World$name == \"Iceland\" | World$name == \"Norway\" | World$name == \"Australia\",]\nplot(st_geometry(World), extent = extent, col = \"grey90\")\nplot(gcLine, add = TRUE, lwd = 2, col = \"red\")\nplot(st_geometry(pt1), add = TRUE, pch = 16, col = \"red\")\nplot(st_geometry(pt2), add = TRUE, pch = 16, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll the sf objects plotted on this map have the same geographic CRS but a map is a plane… To plot this kind of data, sf thus needs to perform some projection. Almost all GIS software (including sf) use a simple projection called plate carrée which maps x to be the value of the longitude and y to be the value of the latitude. This is not a good projection if you want to publish a world map. The distortions can get quite large (e.g., Switzerland will be too elongated). We will see better alternatives later in this tutorial.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe’ve already seen examples where turning s2 off will provide more precise results when using a geographic CRS. But keep in mind that computing areas, lengths or distances are probably the only valid cases where turning s2 off is a good idea. For all other computations based on geographic CRSs you should NOT deactivate s2, otherwise you’ll get results that will most probably be wrong.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#buffers",
    "href": "vectors_tips.html#buffers",
    "title": "5  Tips and tricks for vectors",
    "section": "5.10 Buffers",
    "text": "5.10 Buffers\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nBuffering objects is one of the most common operations in GIS. A buffer is a polygon showing the area within a given distance of a spatial object. You can buffer all the existing vector types with the st_buffer() function. For the distance, you can either specify a numeric value, or an object of class units. If you use a numeric value, the unit will the one of the used CRS (i.e., meters for the Swiss CRS we’re using). You can also specify a vector of distances (one distance for each feature).\n\nobs_buff &lt;- st_buffer(obs[c(1, 2, 5),], dist = 80)\nobs_buff_var &lt;- st_buffer(obs[c(1, 2, 5),], dist = c(60, 80, 100))\npar(mfrow = c(1, 2))\nplot(st_geometry(obs_buff), col = 2:4, main = \"Fixed width\")\nplot(st_geometry(obs_buff_var), col = 2:4, main = \"Variable width\")\n\n\n\n\n\n\n\n\nIn the GIS world curves are often approximated using small segments, even though curves exists in the Simple Feature standard (but they’re often poorly implemented in GIS software). The number of segments used to create the buffers is controlled by the argument nQuadSegs (= number of segments per quadrant). You can change this value to create buffers with octagonal shapes. You can also increase the default value if you need better precision.\n\nobs_buff_oct &lt;- st_buffer(obs[c(1, 2, 5),], dist = 80, nQuadSegs = 2)\nplot(st_geometry(obs_buff_oct), col = 2:4)\n\n\n\n\n\n\n\n\nEach feature will get its own buffer, which means that, if they’re close enough and/or the distance is large enough, the buffers will overlap but they’re still separate features. It is possible to merge overlapping buffers using the st_union() function. The function will produce a single multipolygon, that’s why we need to transform the output into a polygon to recover the non-overlapping buffers as separate features. Note that you will lose the attributes of the original features with this operation. If you need to recover some of them, you can use a spatial join (but be careful: what do you really want for the merged buffers?).\n\nobs_buff_merged &lt;- st_union(obs_buff)\nobs_buff_merged &lt;- st_cast(obs_buff_merged, \"POLYGON\")\nplot(obs_buff_merged, col = 2:3)\n\n\n\n\n\n\n\n\nWhen using polygons, we can also specify negative distances to create “inside” buffers.\n\nmuni_buff &lt;- st_buffer(muni, dist = -500)\nplot(st_geometry(muni))\nplot(st_geometry(muni_buff), col = \"navy\", add = TRUE)\n\n\n\n\n\n\n\n\nWhen buffering lines or polygons, we have three different options to control what will happen at corners: ROUND, BEVEL and MITRE. The default is to use rounded corners but we can easily change that with the joinStyle argument. When using the MITRE style, we can control the maximum distance from the original geometry with the mitreLimit argument. This is expressed as a ratio of the buffer distance and the default value is 1 (e.g., a value of 2 means that we allow distances up to twice the chosen buffer distance for corners).\n\npol &lt;- st_as_sfc(\"POLYGON((2656000 1221000, 2657000 1221000, 2657000 1220000, 2658000 1220000, 2658000 1219000, 2656000 1219000, 2656000 1221000))\", crs = \"EPSG:2056\")\n\npol_buff1 &lt;- st_buffer(pol, 400, joinStyle = \"ROUND\")\npol_buff2 &lt;- st_buffer(pol, 400, joinStyle = \"BEVEL\")\npol_buff3 &lt;- st_buffer(pol, 400, joinStyle = \"MITRE\")\npol_buff4 &lt;- st_buffer(pol, 400, joinStyle = \"MITRE\", mitreLimit = 2)\n\npar(mfrow = c(2, 2))\nplot(pol_buff1, main = \"joinStyle = ROUND\")\nplot(pol, col = \"navy\", add = TRUE)\nplot(pol_buff2, main = \"joinStyle = BEVEL\")\nplot(pol, col = \"navy\", add = TRUE)\nplot(pol_buff3, main = \"joinStyle = MITRE (limit=1)\")\nplot(pol, col = \"navy\", add = TRUE)\nplot(pol_buff4, main = \"joinStyle = MITRE (limit=2)\")\nplot(pol, col = \"navy\", add = TRUE)\n\n\n\n\n\n\n\n\nSimilarly when buffering lines that don’t form a closed ring, we also have three options to control how line endings are handled: ROUND, FLAT and SQUARE. The default is to use rounded line endings but we can easily change that with the endCapStyle argument.\n\nline &lt;- st_as_sfc(\"LINESTRING(2656000 1220000, 2657000 1221000, 2658000 1220000)\", crs = \"EPSG:2056\")\n\nline_buff1 &lt;- st_buffer(line, 400, endCapStyle = \"ROUND\")\nline_buff2 &lt;- st_buffer(line, 400, endCapStyle = \"FLAT\")\nline_buff3 &lt;- st_buffer(line, 400, endCapStyle = \"SQUARE\")\n\npar(mfrow = c(1, 3), mar = c(0, 1, 3, 1))\nplot(line_buff1, main = \"endCapStyle = ROUND\")\nplot(line, col = \"navy\", lwd = 2, add = TRUE)\nplot(line_buff2, main = \"endCapStyle = FLAT\")\nplot(line, col = \"navy\", lwd = 2, add = TRUE)\nplot(line_buff3, main = \"endCapStyle = SQUARE\")\nplot(line, col = \"navy\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nOne last interesting possibility with lines consists of buffering only one side of the line features. We can do that by setting the singleSide argument to TRUE. Positive distance values will buffer the left-hand side (considering the line direction) of the line feature, while negative values will buffer the right-hand side.\n\nline_singlebuff1 &lt;- st_buffer(line, 400, singleSide = TRUE)\nline_singlebuff2 &lt;- st_buffer(line, -400, singleSide = TRUE)\n\npar(mfrow = c(1, 2), mar = c(0, 1, 3, 1))\nplot(line_singlebuff1)\nplot(line, col = \"navy\", lwd = 2, add = TRUE)\nplot(line_singlebuff2)\nplot(line, col = \"navy\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you use a geographic CRS and sf_use_s2() is TRUE, a numeric value for the distance will be taken as a distance in meters. If sf_use_s2() is FALSE, the unit will be degrees and the output probably won’t make any sense.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#affine-transformations",
    "href": "vectors_tips.html#affine-transformations",
    "title": "5  Tips and tricks for vectors",
    "section": "5.11 Affine transformations",
    "text": "5.11 Affine transformations\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\n\n\n\n\nsf provide methods to transform sfc and sfg objects, this is however not possible with sf objects (but have a look at the trick at the end of this section). Shifting a geometry is the easiest operation. The next example will shift the borders of Sempach, 500 meters to the north and 200 meters to the east. The shift is always done using the unit defined in the CRS used by the spatial object.\n\nsempach_sfc &lt;- st_geometry(sempach)\nsempach_sfc_shift &lt;- sempach_sfc + c(500, 200)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe output of an affine transformation will unfortunately lose its CRS information (it will be NA). This is a problem, especially if you plan to do further analyses or make maps with the transformed data. You can use the st_crs() function to reassign it based on the original data.\n\nst_crs(sempach_sfc_shift) &lt;- st_crs(sempach_sfc)\n\n\n\nWe can also scale the geometries, but we need a reference point (such as the centroid) for each feature. Once we have these reference points, we can center the geometries by shifting them, apply the scaling, and shift them back to their original locations.\n\nmuni_sfc &lt;- st_geometry(muni)\nmuni_sfc_centroids &lt;- st_centroid(muni_sfc)\nmuni_sfc_scale &lt;- (muni_sfc - muni_sfc_centroids) * 0.5 + muni_sfc_centroids\n\nplot(muni_sfc, col = \"grey90\")\nplot(muni_sfc_scale, col = \"navy\", add = TRUE)\n\n\n\n\n\n\n\n\nRotation is another common affine transformation. We also need to define a reference point for each feature and perform the same shifting operations. However, instead of multiplying with a scalar we now use a rotation matrix. Remember that a 2D vector (i.e. coordinates) will be rotated by an angle \\(\\theta\\) when we multiply it with the following matrix:\n\\[R=\n\\begin{pmatrix}\n\\cos(\\theta) & -\\sin(\\theta)\\\\\n\\sin(\\theta) & \\cos(\\theta)\n\\end{pmatrix}\n\\]\nWe can of course combine different transformations, such as scaling and rotation…\n\nrotation &lt;- function(theta){\n  theta_rad &lt;- theta * pi / 180\n  matrix(c(cos(theta_rad), sin(theta_rad), -sin(theta_rad), cos(theta_rad)), nrow = 2, ncol = 2)\n}\n\nmuni_sfc_rotate &lt;- (muni_sfc - muni_sfc_centroids) * rotation(30) + muni_sfc_centroids\nmuni_sfc_scale_rotate &lt;- (muni_sfc - muni_sfc_centroids) * 0.5 * rotation(30) + muni_sfc_centroids\n\npar(mfrow = c(1, 2))\nplot(muni_sfc, col = \"grey90\")\nplot(muni_sfc_rotate, col = \"navy\", add = TRUE)\n\nplot(muni_sfc, col = \"grey90\")\nplot(muni_sfc_scale_rotate, col = \"navy\", add = TRUE)\n\n\n\n\n\n\n\n\nOnce all the transformations are performed, we can use the st_set_geometry() function to create a new sf object combining the new geometry with the attributes of the original sf object.\n\nmuni2 &lt;- st_set_geometry(muni, muni_sfc_scale_rotate)\nst_crs(muni2) &lt;- st_crs(muni)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#combine-geometries",
    "href": "vectors_tips.html#combine-geometries",
    "title": "5  Tips and tricks for vectors",
    "section": "5.12 Combine geometries",
    "text": "5.12 Combine geometries\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\nstreets &lt;- st_read(\"data/geodata.gpkg\", \"streets\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nCombining geometries from two different data sets is one of the most common analysis performed in a GIS. The four main operations are the following ones: union, intersection, difference and symmetric difference. The following figure shows a graphical overview of these operations with the function names (source: Lovelace et al., 20192):\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen you combine geometries from different data sets, you need to be sure that they all have the same CRS.\n\n\n\n5.12.1 Merge geometries\nAs we’ve seen earlier, we can use the st_union() function with a single data set to merge its geometries. All polygons with common borders will be merged together. If you have line geometries, the function will merge all the lines that are touching or intersecting.\n\nmuni_merged &lt;- st_union(muni)\nmuni_merged\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2648317 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\n\n\nPOLYGON ((2659099 1221637, 2659104 1221610, 265...\n\nplot(muni_merged, col = \"navy\")\n\n\n\n\n\n\n\n\nIf you use the st_union() function on a polygon data set where some geometries don’t have common borders, you’ll get a single multipolygon geometry as output. You can easily disaggregate the parts using the st_cast() function. This operation is sometimes called “exploding” a multipart geometry.\n\nmuni_merged &lt;- st_union(muni[c(1, 2, 6, 7),])\nmuni_merged\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2648801 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\n\n\nMULTIPOLYGON (((2656850 1219147, 2656890 121912...\n\nmuni_merged_exploded &lt;- st_cast(muni_merged, \"POLYGON\")\nmuni_merged_exploded\n\nGeometry set for 2 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2648801 ymin: 1213352 xmax: 2660750 ymax: 1227618\nProjected CRS: CH1903+ / LV95\n\n\nPOLYGON ((2656850 1219147, 2656890 1219126, 265...\n\n\nPOLYGON ((2648835 1225721, 2648864 1225750, 264...\n\nplot(muni_merged_exploded, col = 2:3)\n\n\n\n\n\n\n\n\nThe st_union() function can also be used to merge geometries between two sf objects having the same vector type.\n\ntemp_poly &lt;- st_geometry(st_buffer(obs[1,], 2000))\nplot(st_union(temp_poly, sempach))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nCreate an sf object with several points, include some points with the exact same location (but different attributes). Run the st_union() function on this new object and “explode” it to get the individual points. What happened?\n\n\nCode\npts &lt;- st_as_sfc(c(\"POINT(2657000 1219000)\", \"POINT(2658000 1218000)\", \"POINT(2659000 1217000)\", \"POINT(2659000 1217000)\"), crs = \"EPSG:2056\")\npts_data &lt;- data.frame(species = c(\"wallcreeper\", \"alpine chough\", \"kingfisher\", \"wallcreeper\"))\npts_sf &lt;- st_sf(pts_data, geometry = pts)\n\nst_cast(st_union(pts_sf), \"POINT\")\n# The points with identical location were merged (even though the other attribute was different)\n\n\n\n\n\n\n5.12.2 Intersect geometries\nComputing the intersection of geometries is another common GIS operation. For example, let’s calculate the intersection between the new polygon we’ve just created and the municipality of Sempach.\n\nmuni_inter &lt;- st_intersection(sempach, temp_poly)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nmuni_inter\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2656433 ymin: 1219021 xmax: 2659072 ymax: 1222900\nProjected CRS: CH1903+ / LV95\n   bfs    name popsize                           geom\n3 1102 Sempach    4186 POLYGON ((2657979 1222854, ...\n\nplot(st_geometry(muni_inter))\n\n\n\n\n\n\n\n\nThe returned object has the same class as that of the first argument. If you intersect two sf objects, the attributes of both objects will also be returned.\nIn a perfect world you’ll get polygons as output when you combine polygon (or multipolygon) objects with one of these functions. However, you may also get something more exotic. Let’s have a look at the following example which comes from one of the sf vignettes.\n\na &lt;- st_polygon(list(cbind(c(0, 0, 7.5, 7.5, 0), c(0, -1, -1, 0, 0))))\nb &lt;- st_polygon(list(cbind(c(0, 1, 2, 3, 4, 5, 6, 7, 7, 0), c(1, 0, 0.5, 0, 0, 0.5, -0.5, -0.5, 1, 1))))\n(inter &lt;- st_intersection(a, b))\n\nGEOMETRYCOLLECTION (POLYGON ((7 0, 7 -0.5, 6 -0.5, 5.5 0, 7 0)), LINESTRING (4 0, 3 0), POINT (1 0))\n\npar(mfrow = c(1, 2), mar = c(0, 1, 3, 1))\nplot(a, ylim = c(-1, 1))\ntitle(\"Intersecting two polygons:\")\nplot(b, add = TRUE, border = \"red\")\nplot(a, ylim = c(-1, 1))\ntitle(\"GEOMETRYCOLLECTION\")\nplot(b, add = TRUE, border = \"red\")\nplot(inter, add = TRUE, col = \"green\", lwd = 2)\n\n\n\n\n\n\n\n\nGeometry collections are defined in the Simple Feature standard as a collection of different vector types. For example, one feature (row) could contain 2 polygons, 1 multiline and 3 points. The output of the previous intersection is clearly a geometry collection since the st_intersection() function produced 1 polygon, 1 line and 1 point. When performing an intersection or a similar operation, it is important to check that all the produced features have the expected geometry type. An geometry collection output is maybe not so common, but you may get a combination of polygons and multipolygons. Use the st_geometry_type() function (maybe in combination with unique()) to inspect the types.\nIdeally we would use geometry collections directly in our analyses but you’ll quickly notice that most functions don’t work with them (and most GIS software don’t really know what to do with them either). Therefore you’ll need to decide which part of the collection is important for you. If you intersect polygons with polygons, most of the time you’ll be interested in the polygons and multipolygons outputs. You can use the st_collection_extract() function to perform this task.\n\nst_collection_extract(inter, type = \"POLYGON\")\n\nPOLYGON ((7 0, 7 -0.5, 6 -0.5, 5.5 0, 7 0))\n\n\nThis function will also return multipolygons (if they exist), even though we specified type = \"POLYGON\". More generally, if some parts of the geometry collection are MULTI*, then all of the parts in the output will be MULTI*.\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nCreate a line object that is intersecting the streets data set. Compute the intersection using the st_intersection() function.\n\n\nCode\ntemp_line &lt;- st_as_sfc(\"LINESTRING(2657290 1219789, 2657090 1219789, 2657290 1219889)\", crs = 2056)\ntemp_inter &lt;- st_intersection(temp_line, streets)\nst_geometry_type(temp_inter)\n\nplot(temp_line, col = \"navy\")\nplot(st_geometry(streets), col = \"purple\", add = TRUE)\nplot(temp_inter[1:3], col = \"red\", add = TRUE)\nplot(temp_inter[4], col = \"blue\", add = TRUE)\n# We get a multipoint because our line intersected the same line twice\n\n\n\n\n\n\n5.12.3 Erase geometries\nThe st_difference() function allows you to erase some parts of a geometry using another geometry (the intersection will be removed from the first one). As an example, let’s create a hole in the Sempach municipality using another polygon:\n\ntemp_poly &lt;- st_geometry(st_buffer(obs[1,], 200))\nsempach_hole &lt;- st_difference(sempach, temp_poly)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(sempach_hole), col = \"navy\")\n\n\n\n\n\n\n\n\nThe results of this function can get extremely confusing if there are multiple overlaps since st_difference() (and st_sym_difference()) will perform the difference operation on each pair of features (note that this isn’t a problem with the st_intersection() function). Let’s have a look at the following example:\n\nobj1 &lt;- st_as_sfc(c(\"POLYGON((-180 -20, -140 55, -50 0, -140 -60, -180 -20))\", \"POLYGON((-10 0, 140 60, 160 0, 140 -55, -10 0))\"))\nobj2 &lt;- st_as_sfc(\"POLYGON((-125 0, 0 60, 40 5, 15 -45, -125 0))\")\n\nplot(obj1, col = rgb(0, 0, 1, 0.4))\nplot(obj2, col = rgb(0, 1, 0, 0.4), add = TRUE)\n\n\n\n\n\n\n\n\nOur goal now is to remove the intersecting areas of the two sfc objects from the second object (in green). Most people would expect the following code to perform the appropriate task.\n\ndiff1 &lt;- st_difference(obj2, obj1)\ndiff1\n\nGeometry set for 2 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -125 ymin: -45 xmax: 40 ymax: 60\nCRS:           NA\n\n\nPOLYGON ((0 60, 40 5, 15 -45, -74.39759 -16.265...\n\n\nPOLYGON ((0 60, 31.5493 16.61972, -10 0, 30.140...\n\npar(mfrow = c(1, 2))\nplot(diff1[1,], col = \"navy\")\nplot(diff1[2,], col = \"navy\")\n\n\n\n\n\n\n\n\nHowever, because the st_difference() function is working on each pair of features, we get something that might look a bit strange. The function first computed the difference between obj2 and the first feature of obj1, and then the difference between obj2 and the second feature of obj1. We thus get two features instead of a single one where all intersecting areas were removed. If you have large data sets with a lot of overlaps, the number of output features will explode. To achieve what we want, we first need to group the features in obj1 in a single feature using the st_combine() function.\n\ndiff2 &lt;- st_difference(obj2, st_combine(obj1))\ndiff2\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -82.99389 ymin: -45 xmax: 31.5493 ymax: 60\nCRS:           NA\n\n\nPOLYGON ((0 60, 31.5493 16.61972, -10 0, 30.140...\n\nplot(diff2, col = \"navy\")\n\n\n\n\n\n\n\n\nDepending on the data you’re using, you’ll sometimes get an error warning using this trick (due to some invalid geometries generated in the background). If this happens you can try replacing the st_combine() function with st_union() or even combine the two (i.e., st_union(st_combine(x))).\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nHere’s another output of the st_difference() function. Try to understand what is happening.\n\nobj3 &lt;- st_as_sfc(c(\"POLYGON((-150 0, -100 10, -60 -5, -150 0))\", \"POLYGON((50 0, 120 40, 100 0, 120 -20, 50 0))\"))\n\nplot(obj1, col = rgb(0, 0, 1, 0.4))\nplot(obj3, col = rgb(0, 1, 0, 0.4), add = TRUE)\n\ndiff1 &lt;- st_difference(obj1, obj3)\ndiff2 &lt;- st_difference(obj1, st_combine(obj3))\n\ndiff1\ndiff2\n\npar(mfrow = c(2, 2))\nplot(diff1[1,], col = \"navy\")\nplot(diff1[2,], col = \"navy\")\nplot(diff1[3,], col = \"navy\")\nplot(diff1[4,], col = \"navy\")\n\npar(mfrow = c(1, 2))\nplot(diff2[1,], col = \"navy\")\nplot(diff2[2,], col = \"navy\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#aggregate-by-attributes",
    "href": "vectors_tips.html#aggregate-by-attributes",
    "title": "5  Tips and tricks for vectors",
    "section": "5.13 Aggregate by attributes",
    "text": "5.13 Aggregate by attributes\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(tmap)\nstreets &lt;- st_read(\"data/geodata.gpkg\", \"streets\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nAggregation can be based purely on the attributes, treating the sf object as a data frame using the standard aggregate() function (but the geometry column will be lost). However, the sf package also extends the aggregate() function if you use an sf object as the first argument. Note that the nice formula notation is not possible in this case.\n\naggregate(pop_est ~ continent, FUN = sum, data = World, na.rm = TRUE)\n\n                continent    pop_est\n1                  Africa 1306370215\n2              Antarctica       4490\n3                    Asia 4550277153\n4                  Europe  745412452\n5           North America  583756036\n6                 Oceania   41204874\n7 Seven seas (open ocean)        140\n8           South America  427066661\n\nworld_agg &lt;- aggregate(World[\"pop_est\"], list(World$continent), FUN = sum, na.rm = TRUE)\nworld_agg\n\nSimple feature collection with 8 features and 2 fields\nAttribute-geometry relationships: aggregate (1), identity (1)\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.645\nGeodetic CRS:  WGS 84\n                  Group.1    pop_est                       geometry\n1                  Africa 1306370215 MULTIPOLYGON (((-1.064 5.00...\n2              Antarctica       4490 MULTIPOLYGON (((-59.572 -80...\n3                    Asia 4550277153 MULTIPOLYGON (((59.181 22.9...\n4                  Europe  745412452 MULTIPOLYGON (((-53.555 2.3...\n5           North America  583756036 MULTIPOLYGON (((-155.688 18...\n6                 Oceania   41204874 MULTIPOLYGON (((147.914 -43...\n7 Seven seas (open ocean)        140 POLYGON ((68.867 -48.83, 68...\n8           South America  427066661 MULTIPOLYGON (((-68.64 -55....\n\nplot(world_agg[,\"pop_est\"])\n\n\n\n\n\n\n\n\nNote that the merging is not perfect since some country borders are not perfectly contiguous in the World data set.\nCombining aggregation with intersections allows us to compute interesting parameters for some area. For example we can compute the total street length in buffers around some bird sightings.\n\nobs_buff &lt;- st_buffer(obs[c(1, 100, 400), 1], dist = 1000)\nobs_buff$site &lt;- 1:3\nstreets_clip &lt;- st_intersection(obs_buff, st_geometry(streets))\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\naggregate(st_length(streets_clip), by = list(site = streets_clip$site), FUN = \"sum\")\n\n  site            x\n1    1 32446.10 [m]\n2    2 44945.34 [m]\n3    3 21131.22 [m]\n\n\nIf you get an error about attributes when computing the intersection, you’ll need to first tell sf how the attributes of the data set should be considered (you should only get a warning with newer sf versions). Here we consider that the attributes are constant within each buffer. You can use the st_agr() function to do this. Use the following code before calling st_intersection():\n\nst_agr(obs_buff) &lt;- \"constant\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#generate-sample-points",
    "href": "vectors_tips.html#generate-sample-points",
    "title": "5  Tips and tricks for vectors",
    "section": "5.14 Generate sample points",
    "text": "5.14 Generate sample points\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\n\n\n\n\nIf you need to generate points inside a polygon according to some sampling design, you’ll need the st_sample() function. Have a look at the help file to see all the sampling types available. In the next example we sample 100 points in Sempach based on two different designs: random and regular. Note that the final number of points can be slightly different when using a regular sampling.\n\nsamples_random &lt;- st_sample(sempach, 100, type = \"random\")\nsamples_regular &lt;- st_sample(sempach, 100, type = \"regular\")\n\npar(mfrow = c(1, 2))\nplot(st_geometry(sempach), col = \"grey90\")\nplot(samples_random, pch = 16, cex = 0.5, add = TRUE)\nplot(st_geometry(sempach), col = \"grey90\")\nplot(samples_regular, pch = 16, cex = 0.5, add = TRUE)\n\n\n\n\n\n\n\n\nIf you need more sampling methods, you can also use the ones provided by the spatstat.random package (you’ll need to install it first). The next example shows how to use a simple sequential inhibition process (SSI) to sample 100 random points that are at least 50 meters apart. You can easily check that the constraint was applied by using the st_distance() function.\n\nsamples_ssi &lt;- st_sample(sempach, r = 50, n = 100, type = \"SSI\")\ntemp_dist &lt;- st_distance(samples_ssi)\nmin(temp_dist[temp_dist &gt; 0])\n\n[1] 51.82655\n\nplot(st_geometry(sempach), col = \"grey90\")\nplot(st_geometry(samples_ssi), pch = 16, cex = 0.5, add = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#convex-and-concave-hulls",
    "href": "vectors_tips.html#convex-and-concave-hulls",
    "title": "5  Tips and tricks for vectors",
    "section": "5.15 Convex and concave hulls",
    "text": "5.15 Convex and concave hulls\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nSometimes you’ll need to use a polygon enclosing all your geometries. This is for example often used as a crude way to estimate an animal home range based on its sightings. Convex hulls are often called minimum convex polygons (MCP) in the ecological literature. Here we extract all the White Wagtail sightings in Neuenkirch and compute the respective minimum convex polygon. Note that we need to group the sightings into a single multipolygon using the st_union() or st_combine() function first. Otherwise we’ll get a separate convex hull for each point (and that’s rather useless since the convex hull of a single point is also a point).\n\nobs_wagtail &lt;- obs[obs$name == \"White Wagtail\",]\nobs_wagtail_neuenkirch &lt;- obs_wagtail[muni[muni$name == \"Neuenkirch\",],]\nconv_hull &lt;- st_convex_hull(st_combine(obs_wagtail_neuenkirch))\nplot(conv_hull, col = \"grey90\")\nplot(st_geometry(obs_wagtail_neuenkirch), pch = 16, add = TRUE)\n\n\n\n\n\n\n\n\nIf you want to compute separate convex hull for groups of points (based on some grouping factor such as individual identity), you need to use the aggregate() function. Let’s imagine we need separate convex hulls for the White Wagtail sightings in Sursee, Schenkon and Oberkirch. We first need to compute a spatial join to know the municipality of all the points, then we can aggregate and compute the three convex hulls.\n\nobs_wagtail &lt;- st_intersection(obs_wagtail, muni[muni$name %in% c(\"Sursee\", \"Oberkirch\", \"Schenkon\"),])\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nobs_wagtail_agg &lt;- aggregate(obs_wagtail, list(obs_wagtail$name.1), FUN = length)\nconv_hull &lt;- st_convex_hull(obs_wagtail_agg)\n\nplot(st_geometry(conv_hull), col = 2:4)\nplot(st_geometry(obs_wagtail_agg), pch = 15:17, add = TRUE)\n\n\n\n\n\n\n\n\nIf you’re computing home ranges and need a minimum convex polygon enclosing only a given percentage of the points (e.g. 95%), have a look at the mcp() function in the adehabitatHR package.\nConcave hulls (also known as alpha-shapes) are more flexible than convex hulls since the enclosing polygon can be, as the name suggests, concave and have holes. The concavity is controlled by the ratio argument: a value of 0 returns a maximally concave hull while a value of 1 returns a convex hull. Holes are controlled by the allow_holes argument.\n\nconc_hull1 &lt;- st_concave_hull(st_combine(obs_wagtail_neuenkirch), ratio = 0)\nconc_hull2 &lt;- st_concave_hull(st_combine(obs_wagtail_neuenkirch), ratio = 0.5)\n\npar(mfrow = c(1, 2))\nplot(conc_hull1, col = \"grey90\")\nplot(st_geometry(obs_wagtail_neuenkirch), pch = 16, add = TRUE)\nplot(conc_hull2, col = \"grey90\")\nplot(st_geometry(obs_wagtail_neuenkirch), pch = 16, add = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#neighborhood-analyses",
    "href": "vectors_tips.html#neighborhood-analyses",
    "title": "5  Tips and tricks for vectors",
    "section": "5.16 Neighborhood analyses",
    "text": "5.16 Neighborhood analyses\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\n\n\n\n\nWe’ve already seen that the sf package provides some nice functions to study the topology of geometries. If you need more you can have a look at the spdep package which excels at neighborhood analyses. First we compute a neighbor list based on municipalities sharing a common boundary using the poly2nb() function.\n\nlibrary(spdep)\n\nWarning: package 'spdep' was built under R version 4.4.3\n\n\nLoading required package: spData\n\n\nTo access larger datasets in this package, install the spDataLarge\npackage with: `install.packages('spDataLarge',\nrepos='https://nowosad.github.io/drat/', type='source')`\n\nmuni_neigh &lt;- poly2nb(muni)\nmuni_neigh\n\nNeighbour list object:\nNumber of regions: 7 \nNumber of nonzero links: 22 \nPercentage nonzero weights: 44.89796 \nAverage number of links: 3.142857 \n\nsummary(muni_neigh)\n\nNeighbour list object:\nNumber of regions: 7 \nNumber of nonzero links: 22 \nPercentage nonzero weights: 44.89796 \nAverage number of links: 3.142857 \nLink number distribution:\n\n2 3 4 \n2 2 3 \n2 least connected regions:\n1 7 with 2 links\n3 most connected regions:\n2 4 5 with 4 links\n\nstr(muni_neigh)\n\nList of 7\n $ : int [1:2] 2 3\n $ : int [1:4] 1 3 4 5\n $ : int [1:3] 1 2 5\n $ : int [1:4] 2 5 6 7\n $ : int [1:4] 2 3 4 6\n $ : int [1:3] 4 5 7\n $ : int [1:2] 4 6\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:7] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = muni)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"snap\")= num 0.01\n - attr(*, \"sym\")= logi TRUE\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : int 1\n  ..$ comp.id: int [1:7] 1 1 1 1 1 1 1\n\n\nThe output is stored as a list with the same ordering as the input data sets. For example we see that the municipality 1 (Neuenkirch) is neighboring municipalities 2 (Nottwil) and 3 (Sempach). Since it’s stored as a list, we can easily get the number of neighbors for all the municipalities.\n\nsapply(muni_neigh, length)\n\n[1] 2 4 3 4 4 3 2\n\n\nWe can also compare the results of the poly2nb() function with the results of the st_touches() function provided by sf. Fortunately, the outputs are identical.\n\nst_touches(muni)\n\nSparse geometry binary predicate list of length 7, where the predicate\nwas `touches'\n 1: 2, 3\n 2: 1, 3, 4, 5\n 3: 1, 2, 5\n 4: 2, 5, 6, 7\n 5: 2, 3, 4, 6\n 6: 4, 5, 7\n 7: 4, 6\n\n\nThe spdep package provides plotting functions to visualize the neighbor lists. When working with polygons, we need to provide the coordinates of the centroids to the plot() function.\n\nmuni_centroids_coords &lt;- st_coordinates(st_centroid(muni))\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nplot(st_geometry(muni))\nplot(muni_neigh, muni_centroids_coords, add = TRUE)\ntext(muni_centroids_coords, labels = 1:nrow(muni), pos = 3)\n\n\n\n\n\n\n\n\nLooking for the closest neighbors to some feature is also a common neighborhood analysis (called K-nearest neighbors). Here we look for the two closest nearest neighbors of each municipality. To do that we use the knearneigh() function. This function only accepts point geometries so we need to use the centroids (and hence all the distances will be computed between centroids and not between the polygon boundaries). To get a neighbor list we use the knn2nb() function on the output of the knearneigh() function.\n\nmuni_knb &lt;- knn2nb(knearneigh(muni_centroids_coords, k = 2))\nmuni_knb\n\nNeighbour list object:\nNumber of regions: 7 \nNumber of nonzero links: 14 \nPercentage nonzero weights: 28.57143 \nAverage number of links: 2 \nNon-symmetric neighbours list\n\nis.symmetric.nb(muni_knb)\n\n[1] FALSE\n\nmuni_knb2 &lt;- make.sym.nb(muni_knb)\nis.symmetric.nb(muni_knb2)\n\n[1] TRUE\n\n\nThe resulting neighbor list will often be asymmetric. For example the two closest neighbors from municipality 1 (Neuenkirch) are municipalities 2 (Nottwil) and 3 (Sempach). However the two closest neighbors of municipality 2 are municipalities 4 and 5. Hence the link between municipalities 1 and 2 is asymmetric. We can use the make.sym.nb() function to make everything symmetric, but after applying this function we will have some municipalities with more than two neighbors. Let’s have a look at the plot of the two computed neighborhoods (symmetric and asymmetric).\n\npar(mfrow = c(1, 2))\nplot(st_geometry(muni))\nplot(muni_knb, muni_centroids_coords, pch = 16, col = \"blue\", arrows = TRUE, add = TRUE)\nplot(st_geometry(muni))\nplot(muni_knb2, muni_centroids_coords, pch = 16, col = \"blue\", arrows = TRUE, add = TRUE)\n\n\n\n\n\n\n\n\nThe K-nearest neighbor analysis also allows us to compute other quantities. For example we can easily compute the minimum distance so that each community has at least one neighbor. We use the nbdists() function to compute all the neighborhood distances, and since its output is a list we need to use the unlist() function to convert it to a vector.\n\nmuni_knb3 &lt;- knn2nb(knearneigh(muni_centroids_coords, k = 1))\n\nWarning in knn2nb(knearneigh(muni_centroids_coords, k = 1)): neighbour object\nhas 2 sub-graphs\n\ndistances &lt;- unlist(nbdists(muni_knb3, muni_centroids_coords))\n(dist1nb &lt;- max(distances))\n\n[1] 3860.315\n\n\nThe dnearneigh() function is similar to the knearneigh function. It computes a list of all the neighbors within a specific distance. Similarly, the dnearneigh() function only accepts point geometries so we need to work with the centroids. Using this function we can also check that our computed minimum distance to have at least one neighbor is correct.\n\ndnb1 &lt;- dnearneigh(muni_centroids_coords, d1 = 0, d2 = 0.75 * dist1nb)\n\nWarning in dnearneigh(muni_centroids_coords, d1 = 0, d2 = 0.75 * dist1nb):\nneighbour object has 5 sub-graphs\n\ndnb2 &lt;- dnearneigh(muni_centroids_coords, d1 = 0, d2 = 1 * dist1nb)\ndnb3 &lt;- dnearneigh(muni_centroids_coords, d1 = 0, d2 = 1.25 * dist1nb)\n\npar(mfrow = c(1, 3), mar = c(1, 1, 1, 1))\nplot(st_geometry(muni))\nplot(dnb1, muni_centroids_coords, pch = 16, col = \"blue\", add = TRUE)\nplot(st_geometry(muni))\nplot(dnb2, muni_centroids_coords, pch = 16, col = \"blue\", add = TRUE)\nplot(st_geometry(muni))\nplot(dnb3, muni_centroids_coords, pch = 16, col = \"blue\", add = TRUE)\n\n\n\n\n\n\n\n\nNeighborhood lists provide an efficient way to store neighborhood information thanks to their sparse structure, but sometimes it’s nice to work with the full neighborhood matrix. You can use the nb2mat() function to get the matrix.\n\nnb2mat(muni_neigh, style = \"B\")\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n1    0    1    1    0    0    0    0\n2    1    0    1    1    1    0    0\n3    1    1    0    0    1    0    0\n4    0    1    0    0    1    1    1\n5    0    1    1    1    0    1    0\n6    0    0    0    1    1    0    1\n7    0    0    0    1    0    1    0\nattr(,\"call\")\nnb2mat(neighbours = muni_neigh, style = \"B\")\n\n\nIf you want to save the neighborhood as an sf line object, you can create the lines using the nb2lines() function.\n\nneigh_sf &lt;- nb2lines(muni_neigh, coords = st_geometry(muni_centroid))\n\nNeighborhood information is sometimes needed to fit more complex statistical models accounting for spatial autocorrelation (e.g. CAR models). You can fit some of these models in R, but sometimes you’ll need another software such as INLA or WinBUGS. The spdep package provide functions to export the neighborhood lists so that they can be used with these software.\n\nnb_winbugs &lt;- nb2WB(muni_neigh)\nnb2INLA(file = \"export/nbinla.txt\", muni_neigh)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#geocoding",
    "href": "vectors_tips.html#geocoding",
    "title": "5  Tips and tricks for vectors",
    "section": "5.17 Geocoding",
    "text": "5.17 Geocoding\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\n\n\n\n\nSometimes the only geographical information you have is an address. The operation of translating an address into a geographical location (with coordinates) is called geocoding. For Switzerland, Swisstopo offers a free online tool to do that. To communicate with this tool we will use an API that is also documented by Swisstopo, with the help of the httr2 package. First let’s define a function that will allow us to query the online geocoder.\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nswissgeocode &lt;- function(address, nresults = 3){\n\n    # URL of the search service\n    base &lt;- \"https://api3.geo.admin.ch/\"\n    endpoint &lt;- \"rest/services/api/SearchServer\"\n\n    # Encode address (needed because of spaces and exotic characters in the address)\n    address_url &lt;- URLencode(address)\n\n    # Request that will be sent to the server\n    request_string &lt;- paste0(base, endpoint, \"?searchText=\", address_url, \"&type=locations\", \"&limit=\", nresults, \"&sr=2056\")\n\n    # Send request\n    req &lt;- request(request_string)\n    resp &lt;- req_perform(req)\n    # Convert JSON to R objects\n    resp_json &lt;- resp_body_json(resp, simplifyVector = TRUE)\n\n    # The address was not found\n    if (is.null(resp_json$results) == TRUE || length(resp_json$results) == 0) {\n        warning(\"The address could not be located.\")\n        output &lt;- data.frame(address_origin = address, address_found = NA, x = NA, y = NA, lat = NA, lon = NA, weight = NA)\n     }\n    # The address was found\n    else {\n        # Merge results in a data frame\n        output &lt;- data.frame(address)\n        output &lt;- cbind(output, resp_json$results$attrs[, c(\"detail\", \"y\", \"x\", \"lat\", \"lon\")], resp_json$results$weight)\n        # The search service switches the coordinates axes (x=y and y=x)\n        names(output) &lt;- c(\"address_origin\", \"address_found\", \"x\", \"y\", \"lat\", \"lon\", \"weight\")\n    }\n    return(output)\n}\n\nWe can test it with a random address.\n\ntest_address &lt;- \"Seerose 1 6204 Sempach\"\nresults &lt;- swissgeocode(test_address, nresults = 5)\nresults &lt;- st_as_sf(results, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nresults\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2657266 ymin: 1219688 xmax: 2657300 ymax: 1219722\nProjected CRS: CH1903+ / LV95\n          address_origin                               address_found      lat\n1 Seerose 1 6204 Sempach   seerose 1 6204 sempach 1102 sempach ch lu 47.12601\n2 Seerose 1 6204 Sempach seerose 1.1 6204 sempach 1102 sempach ch lu 47.12601\n3 Seerose 1 6204 Sempach seerose 1.2 6204 sempach 1102 sempach ch lu 47.12593\n4 Seerose 1 6204 Sempach seerose 3.1 6204 sempach 1102 sempach ch lu 47.12579\n5 Seerose 1 6204 Sempach   seerose 2 6204 sempach 1102 sempach ch lu 47.12571\n       lon weight                geometry\n1 8.193561    100 POINT (2657282 1219722)\n2 8.193356      4 POINT (2657266 1219722)\n3 8.193375      4 POINT (2657268 1219713)\n4 8.193802      4 POINT (2657300 1219698)\n5 8.193670      1 POINT (2657290 1219688)\n\n\nIf you need to geocode addresses outside of Switzerland, you can try the geocode_OSM() function in the tmaptools package. It uses a geocoder called Nominatim that is provided by OpenStreetMap. Also check the tidygeocoder package.\n\nlibrary(tmaptools)\nresults &lt;- geocode_OSM(test_address, as.sf = TRUE)\nresults\n\nSimple feature collection with 1 feature and 7 fields\nActive geometry column: point\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 8.193418 ymin: 47.12631 xmax: 8.193418 ymax: 47.12631\nGeodetic CRS:  WGS 84\n                   query      lat      lon  lat_min  lat_max  lon_min  lon_max\n1 Seerose 1 6204 Sempach 47.12631 8.193418 47.12598 47.12649 8.193269 8.193966\n                            bbox                     point\n1 POLYGON ((8.193269 47.12598... POINT (8.193418 47.12631)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "vectors_tips.html#crs-transformations",
    "href": "vectors_tips.html#crs-transformations",
    "title": "5  Tips and tricks for vectors",
    "section": "5.18 CRS transformations",
    "text": "5.18 CRS transformations\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\nIf the data sets you’re using have different CRSs, it’s usually a good idea to transform some of them so that they all have the same CRS. We need the st_transform() function to do this. Sometimes we also say that we “project” the data to another CRS (however, transformations between geographic CRSs are not really projections).\nHere we first check the CRS of the World data set using the st_crs() function and compute a graticule (grid). Then we project the data set to three different projected CRSs, the Swiss CRS, the Equal Earth projection and the Robinson projection. If we use a CRS with an EPSG number, we can also use the integer value directly (without specifying “EPSG:”). However it’s a good habit to always specify the provider to avoid confusions.\n\nst_crs(World)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\ngrat &lt;- st_graticule()\nworld_ch &lt;- st_transform(World, \"EPSG:2056\")\ngrat_ch &lt;- st_transform(grat, \"EPSG:2056\")\nworld_equal &lt;- st_transform(World, \"EPSG:8857\")\ngrat_equal &lt;- st_transform(grat, \"EPSG:8857\")\nworld_rob &lt;- st_transform(World, \"ESRI:54030\")\ngrat_rob &lt;- st_transform(grat, \"ESRI:54030\")\n\nWhen we plot that data, we first see that, not surprisingly, the Swiss CRS is not adapted at all for global data, and that the Equal Earth and Robinson projections give a pleasing result. Both of these projections are good choices for world maps.\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1, 1))\nplot(st_geometry(World), col = \"grey90\")\nplot(st_geometry(grat), col = \"lightgrey\", add = TRUE)\nplot(st_geometry(world_ch), col = \"grey90\")\nplot(st_geometry(grat_ch), col = \"lightgrey\", add = TRUE)\nplot(st_geometry(world_equal), col = \"grey90\")\nplot(st_geometry(grat_equal), col = \"lightgrey\", add = TRUE)\nplot(st_geometry(world_rob), col = \"grey90\")\nplot(st_geometry(grat_rob), col = \"lightgrey\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe st_crs() function is used to query or assign a CRS, but it does not perform any CRS transformation!\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nThe Bonne projection is one of the most beautiful projection available. To understand why, try to make a world map with a graticule using this projection.\n\n\nCode\nworld_bonne &lt;- st_transform(World, \"ESRI:54024\")\ngrat_bonne &lt;- st_transform(grat, \"ESRI:54024\")\n\nplot(st_geometry(world_bonne), col = \"grey90\")\nplot(st_geometry(grat_bonne), col = \"lightgrey\", add = TRUE)\n\n\n\n\n\n\n\n\n1. Pebesma E. Lwgeom: Bindings to Selected liblwgeom Functions for Simple Features. Published online 2023. https://github.com/r-spatial/lwgeom/\n\n\n2. Lovelace R, Nowosad J, Muenchow J. Geocomputation with R. Second edition. CRC Press 2025.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tips and tricks for vectors</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html",
    "href": "rasters_tips.html",
    "title": "6  Tips and tricks for rasters",
    "section": "",
    "text": "6.1 Reading and writing raster data\nAs we saw earlier, we are going to use another package to import raster data sets. The rast() function from the terra package is what we need. It doesn’t matter if our raster is continuous, discrete, or contains several bands, the rast() function will create the correct terra object. Calling the object will display some basic information about the data set. However don’t forget that it’s only a pointer to the data set, the full raster will not be imported into memory.\nThe digital elevation model is an extract of the DHM25 data set provided by Swisstopo, the orthophoto is an extract of the SWISSIMAGE data set also provided by Swisstopo.\nlibrary(terra)\n\nWarning: package 'terra' was built under R version 4.4.3\n\n\nterra 1.8.42\n\nelev &lt;- rast(\"data/dem.tif\")\nelev\n\nclass       : SpatRaster \ndimensions  : 604, 840, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 2643988, 2664988, 1212913, 1228013  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 (EPSG:2056) \nsource      : dem.tif \nname        : dem \n\nplot(elev)\n\n\n\n\n\n\n\northo &lt;- rast(\"data/sempach_ortho.tif\")\northo\n\nclass       : SpatRaster \ndimensions  : 1427, 1996, 3  (nrow, ncol, nlyr)\nresolution  : 0.1000104, 0.1000196  (x, y)\nextent      : 2657219, 2657419, 1219699, 1219842  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 (EPSG:2056) \nsource      : sempach_ortho.tif \ncolors RGB  : 1, 2, 3 \nnames       : sempach_ortho_1, sempach_ortho_2, sempach_ortho_3 \n\nplot(ortho)\nNote that the orthophoto was plotted in a different way (no axes, no legend). The terra package automatically detected that the raster had 3 bands and made the assumption that the bands corresponded to red, green and blue intensity values. If this assumption is not correct, you can use the plotRGB() function and specify the RGB bands manually to get the desired plot.\nSimilarly we can import a raster containing discrete values, such as a landcover/landuse data set. The following one is an extract from the Swiss Land Use Statistics, provided by the Federal Statistical Office. Note that the CRS is not defined, this will happen more often with raster data found in the wild than with vector data. Fortunately we know which CRS was used and we can add this information to the data set. Moreover, terra is not recognizing automatically that we’re processing a discrete raster. That’s why we need to use the as.factor() function.\nlandcover &lt;- rast(\"data/landcover.tif\")\nlandcover\n\nclass       : SpatRaster \ndimensions  : 151, 210, 1  (nrow, ncol, nlyr)\nresolution  : 100, 100  (x, y)\nextent      : 2644000, 2665000, 1212900, 1228000  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource      : landcover.tif \nname        :  X \nmin value   :  1 \nmax value   : 26 \n\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\nplot(landcover)\nIf a raster data set is stored online, you can also directly use it without needing to download the full data set. This works especially well with rasters stored as Cloud Optimized GeoTiff (COG) files. A COG file is a regular GeoTiff file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. To read such rasters, you’ll need to the vsi = TRUE argument inside the rast() function.\nurl_swissimage &lt;- \"https://data.geo.admin.ch/ch.swisstopo.swissimage-dop10/swissimage-dop10_2021_2666-1211/swissimage-dop10_2021_2666-1211_0.1_2056.tif\"\northo_web &lt;- rast(url_swissimage, vsi = TRUE)\nplot(ortho_web)\nRasters are nice objects to work with but it’s sometimes nice to have more familiar R objects. You can easily convert SpatRaster objects to data frames containing the geographic coordinates of all pixels and the related pixel values.\nelev_vals &lt;- values(elev)\nhead(elev_vals)\n\n     dem\n[1,] 544\n[2,] 544\n[3,] 543\n[4,] 544\n[5,] 545\n[6,] 546\n\nelev_df &lt;- as.data.frame(elev, xy = TRUE)\nhead(elev_df)\n\n        x       y dem\n1 2644000 1228000 544\n2 2644025 1228000 544\n3 2644050 1228000 543\n4 2644075 1228000 544\n5 2644100 1228000 545\n6 2644125 1228000 546\n\nwrite.csv(elev_df, \"export/dem.csv\")\nSimilarly, if you have a data frame containing geographic coordinates located on a regular grid and associated values, you can easily convert it to a terra object with the help of the rast() function, by adding the argument type = \"xyz\". This is often useful if you want to plot the predictions of a statistical model on a map, for example the distribution of a species. Here we add a new column to some data frame filled with random values and we convert it to a 2-band raster.\nelev_df$rand &lt;- rnorm(nrow(elev_df))\n\nelev2 &lt;- rast(elev_df, type = \"xyz\")\nelev2\n\nclass       : SpatRaster \ndimensions  : 604, 840, 2  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 2643988, 2664988, 1212913, 1228013  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nnames       : dem,      rand \nmin values  : 425, -4.775576 \nmax values  : 930,  5.066735 \n\nplot(elev2, mar = c(0, 1, 0, 3.5))\nExporting a raster data set is really easy thanks to the writeRaster() function. The format will be automatically recognized based on the file name extension. As we saw in the introduction, the GeoTiff format is almost always the format we should use.\nwriteRaster(elev, \"export/dem1.tif\")\nwriteRaster(elev2, \"export/dem2.tif\")\nRemember that the first raster was a single band raster and the second one had two bands. If you want to only export a single band, you can subset the raster data set using [[]], either using the band number or name.\nwriteRaster(elev2[[1]], \"export/dem2.tif\", overwrite = TRUE)\nwriteRaster(elev2[[\"dem\"]], \"export/dem2.tif\", overwrite = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#summarize-rasters",
    "href": "rasters_tips.html#summarize-rasters",
    "title": "6  Tips and tricks for rasters",
    "section": "6.2 Summarize rasters",
    "text": "6.2 Summarize rasters\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\n\n\n\n\nIn the GIS world, functions acting on the whole raster (using all pixels) to produce some output are called global functions. They are mostly used to produce some descriptive statistics on the data set. It is therefore not so surprising that terra provides such a function. If you prefer plots, the hist() and boxplot() functions have also been extended to support SpatRaster objects.\n\nglobal(elev, fun = \"max\")\n\n    max\ndem 930\n\nhist(elev)\n\n\n\n\n\n\n\nboxplot(elev)\n\nWarning: [boxplot] taking a sample of 1e+05 cells\n\n\n\n\n\n\n\n\n\nYou can define you own function when using the global() function. However they will be much slower than the standard functions provided by terra, or may even fail.\nFor discrete rasters, you can easily get the frequency distribution of all the available categories.\n\nfreq(landcover)\n\n   layer value count\n1      1     1   506\n2      1     2  1136\n3      1     3   149\n4      1     4   741\n5      1     5   151\n6      1     6  1107\n7      1     7    81\n8      1     8     9\n9      1     9   186\n10     1    10   387\n11     1    11   795\n12     1    12     8\n13     1    13    84\n14     1    14 11097\n15     1    15  4950\n16     1    16  3272\n17     1    19  4175\n18     1    20   318\n19     1    22   580\n20     1    23  1771\n21     1    24    91\n22     1    25   109\n23     1    26     7",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#spatial-subsetting",
    "href": "rasters_tips.html#spatial-subsetting",
    "title": "6  Tips and tricks for rasters",
    "section": "6.3 Spatial subsetting",
    "text": "6.3 Spatial subsetting\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\nelev &lt;- rast(\"data/dem.tif\")\n\n\n\n\nIf you need to spatially subset a raster, you can either use another raster defining the zone you want to extract, or a polygon enclosing the area. First we’ll have a look at the former. We first import our raster mask.\n\nrmask &lt;- rast(\"data/sempach_raster.tif\")\nplot(rmask, col = \"blue\")\n\n\n\n\n\n\n\n\nDon’t forget that regular raster data sets are always rectangular. The raster mask we’re now using is the municipality of Sempach. We see its shape, but the data set is still a rectangle. All the other pixels in the raster extent are NAs (you can quickly check it using the values() function).\nFirst we need to use the crop() function, which will crop our DEM raster to the extent of the mask. This will also work if the mask is not perfectly align with the other raster, but in this case terra will perform a slight shift of the extent of the mask so that everything is aligned.\n\nelev_crop &lt;- crop(elev, rmask)\n# Identical to\nelev_crop &lt;- elev[rmask, , drop = FALSE]\nplot(elev_crop)\n\n\n\n\n\n\n\n\nWe can then perform the masking once we have two rasters with the same extent and alignment. If your mask is not perfectly aligned, you’ll need to shift (have a look at the shift() function) it or even resample it (see next section).\n\nelev_mask &lt;- mask(elev_crop, rmask)\nplot(elev_mask)\n\n\n\n\n\n\n\n\nNow let’s have a look at the second possibility: masking and cropping using a vector data set. This is considerably easier since you don’t need to have perfect alignment but this can be a bit slower for complex masks.\n\nelev_crop2 &lt;- crop(elev, sempach)\nplot(elev_crop2)\n\n\n\n\n\n\n\nelev_mask2 &lt;- mask(elev, sempach)\nplot(elev_mask2)\n\n\n\n\n\n\n\n\nWith vector data, you can directly use the mask() function, without clipping the extent with the crop() function first. However you’ll still get the original extent, which means a lot of pixels with NAs. The trim() function allows cleaning things a bit by removing outer rows and columns full of NAs.\n\nelev_mask2 &lt;- trim(elev_mask2)\nplot(elev_mask2)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#aggregation-and-resampling",
    "href": "rasters_tips.html#aggregation-and-resampling",
    "title": "6  Tips and tricks for rasters",
    "section": "6.4 Aggregation and resampling",
    "text": "6.4 Aggregation and resampling\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\n\n\n\n\nSometimes you will need to reduce the resolution of your rasters (e.g., if they’re too large to be processed efficiently). This can be easily achieved using the aggregate() function. The fact argument is the aggregation factor. For example a value of 2 means 2 pixels in the horizontal direction and 2 pixels in the vertical direction, so that 4 pixels will be aggregated into one. If you need to disaggregate a raster, have a look at the disagg() function.\n\nelev_agg &lt;- aggregate(elev, fact = 20)\nplot(elev_agg)\n\n\n\n\n\n\n\n\nBy default the aggregate() function will use the arithmetic mean to merge the pixel values, but you’re free to use other functions (including your own ones). As you can guess, we need to be a bit careful when using discrete rasters since most functions won’t make any sense. The following one (fun = \"modal\") will use the most frequent category as the new pixel value.\n\nlandcover_agg &lt;- aggregate(landcover, fact = 5, fun = \"modal\")\nplot(landcover_agg)\n\n\n\n\n\n\n\n\nCombining rasters is really easy when they’re perfectly aligned (same extent, same resolution). However most of the times we get data sets that don’t satisfy this condition. If possible we should first try to slightly shift and/or aggregate the rasters (or maybe crop/extend the extents). If the differences are too big, we need to transform the raster in a more radical way, we need resampling. This means taking the values of our original raster and calculate new values for our target extent and resolution.\nThe first argument of the resample() function is the raster that needs to be resampled, the second argument is used to provide the target extent and resolution, and the third one is used to indicate the method that will be used for the interpolation of the new values. The bilinear interpolation is the default and is appropriate for continuous rasters. It assigns a weighted average of the four nearest cells of the original raster to the cell of the target one. Other interpolation algorithms are available for continuous rasters (e.g. cubic, cubic spline, etc.). However all of this don’t make any sense when we need to resample a discrete raster. For them we need to use the nearest neighbor interpolation. It assigns the value of the nearest cell of the original raster to the cell of the target one.\n\nelev_resample &lt;- resample(elev, landcover, method = \"bilinear\")\nelev_resample\n\nclass       : SpatRaster \ndimensions  : 151, 210, 1  (nrow, ncol, nlyr)\nresolution  : 100, 100  (x, y)\nextent      : 2644000, 2665000, 1212900, 1228000  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 (EPSG:2056) \nsource(s)   : memory\nvarname     : landcover \nname        :      dem \nmin value   : 427.0625 \nmax value   : 920.8250 \n\ncompareGeom(elev_resample, landcover)\n\n[1] TRUE\n\nlandcover_resample &lt;- resample(landcover, elev, method = \"near\")\nlandcover_resample\n\nclass       : SpatRaster \ndimensions  : 604, 840, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 2643988, 2664988, 1212913, 1228013  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 (EPSG:2056) \nsource(s)   : memory\nvarname     : dem \ncategories  : X \nname        :  X \nmin value   :  1 \nmax value   : 26 \n\ncompareGeom(landcover_resample, elev)\n\n[1] TRUE\n\n\nThe compareGeom() function that the rasters have the same geometries (by default: same extent, number of rows and columns, projection, resolution, and origin), and hence are now aligned. Note how the extent and resolution changed after the resampling.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#classify-rasters",
    "href": "rasters_tips.html#classify-rasters",
    "title": "6  Tips and tricks for rasters",
    "section": "6.5 Classify rasters",
    "text": "6.5 Classify rasters\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\n\n\n\n\nSometimes you’ll need to classify/reclassify rasters. For a continuous raster we can easily do this by hand, but as we will see terra is providing more efficient functions.\n\nelev_recl &lt;- elev\nelev_recl[elev_recl &lt;= 500] &lt;- 1\nelev_recl[elev_recl &gt; 500 & elev_recl &lt;= 700] &lt;- 2\nelev_recl[elev_recl &gt; 700] &lt;- 3\nplot(elev_recl)\n\n\n\n\n\n\n\n\nThis is much easier if we use the classify() function. For a continuous raster, we just need to define the breaks. The last argument means that the pixels having a value equal to the lowest break will be included in the first class. This usually makes sense when categorizing continuous rasters (but this is not the default option). The output of the classify() function also provides better labels for the classes.\n\nelev_recl &lt;- classify(elev, c(0, 500, 700, Inf), include.lowest = TRUE)\nplot(elev_recl)\n\n\n\n\n\n\n\n\nIf we need to reclassify a discrete raster, we need to create a matrix with 2 columns, with the first column containing the old values and the second column containing the corresponding new values.\n\nreclMat_landcover &lt;- matrix(c(1, 1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9, 1, 10, 1, 11, 2, 12, 2,\n                    13, 2, 14, 2, 15, 2, 16, 2, 17, 2, 18, 2, 19, 3, 20, 3, 21, 3, 22, 3, 23, 4,\n                    24, 4, 25, 4, 26, 4), ncol = 2, byrow = TRUE)\nlandcover_recl &lt;- classify(landcover, reclMat_landcover)\nplot(landcover_recl)\n\n\n\n\n\n\n\n\nWith a discrete raster, we can easily create a new raster data set containing separate binary layers based on the categories. This process is often called one-hot encoding or dummy encoding (and is totally similar to what R is doing when you fit a linear model with a discrete variable).\n\nlandcover_layers &lt;- segregate(landcover_recl)\nnames(landcover_layers) &lt;- c(\"settlements\", \"agriculture\", \"forest\", \"unproductive\")\nplot(landcover_layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nExtract the forest raster and aggregate it to get a resolution of 1000 meters. Each pixel should contain the ratio of forest (0 = no forest, 1 = 100% forest) within the 1km area.\n\n\nCode\nforests &lt;- landcover_layers[[\"forest\"]]\nforests_agg &lt;- aggregate(forests, 10, fun = function(i) {sum(i) / 100})",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#extract-pixel-values",
    "href": "rasters_tips.html#extract-pixel-values",
    "title": "6  Tips and tricks for rasters",
    "section": "6.6 Extract pixel values",
    "text": "6.6 Extract pixel values\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nobs_buff &lt;- st_buffer(obs[c(1, 2, 5),], dist = 80)\nelev &lt;- rast(\"data/dem.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\nelev_resample &lt;- resample(elev, landcover, method = \"bilinear\")\n\n\n\n\nBefore doing a statistical analysis, it is quite common to collect all the covariates as GIS layers. We’ve already seen how to extract information from vector data sets using spatial joins. There is a similar procedure for raster data sets. The most common operation consists of extracting raster values at some given points/sites.\nNote that terra is using its own data model to store and process vector data (it doesn’t use the classes defined by sf). Hence, most terra functions expecting a vector data set won’t accept sf objects. The terra class for vector data is called SpatVector. Fortunately it is very easy to convert sf objects to SpatVector objects using the vect() function (actually the extract() function is an exception and is also accepting sf objects). If you need to convert a SpatVector object to sf, you can use the st_as_sf() function.\n\nvals &lt;- extract(elev, vect(obs), ID = FALSE)\nobs2 &lt;- obs\nobs2$elev &lt;- vals\n\nThe output of the extract() function is a standard data frame. The order of the data frame is the same as the order of the vector data set used to extract the data, you can therefore easily append the extracted values to the sf point object.\nIf you’re lucky enough to have properly aligned rasters, you can combine them in a multiband data set and use the extract() function on this new data set. You’ll also get a data frame, with one column for each band.\n\ncovariates &lt;- c(landcover, elev_resample)\nnames(covariates) &lt;- c(\"landcover\", \"elevation\")\nvals &lt;- extract(covariates, vect(obs), ID = FALSE)\n\nYou’re actually not restricted to using points to extract information from rasters. For example using a line you can easily compute an elevation profile. Using polygons is even more powerful and allow performing what is usually called zonal analyses. By default the extract() function will extract all the pixels whose centroid is within the polygons.\n\nvals &lt;- extract(elev, vect(obs_buff))\nhead(vals)\n\n  ID dem\n1  1 558\n2  1 559\n3  1 559\n4  1 559\n5  1 559\n6  1 559\n\n\nThe ID values corresponds to the ordering of the polygons of the sf object. This means that all rows with ID=1 are pixels intersection the first polygon. You’re then free to use these values, for example to characterize some habitat. If you only need some summary statistic for each polygon, you can use the fun argument to specify some aggregating function.\n\nvals_avg &lt;- extract(elev, vect(obs_buff), fun = mean)\n\nThe extract() function is really powerful (and fast) and has a lot of additional possibilities. Don’t hesitate to have a look at its help file.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#combine-rasters",
    "href": "rasters_tips.html#combine-rasters",
    "title": "6  Tips and tricks for rasters",
    "section": "6.7 Combine rasters",
    "text": "6.7 Combine rasters\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\nreclMat_landcover &lt;- matrix(c(1, 1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9, 1, 10, 1, 11, 2, 12, 2,\n                    13, 2, 14, 2, 15, 2, 16, 2, 17, 2, 18, 2, 19, 3, 20, 3, 21, 3, 22, 3, 23, 4,\n                    24, 4, 25, 4, 26, 4), ncol = 2, byrow = TRUE)\nlandcover_recl &lt;- classify(landcover, reclMat_landcover)\nlandcover_layers &lt;- segregate(landcover_recl)\nnames(landcover_layers) &lt;- c(\"settlements\", \"agriculture\", \"forest\", \"unproductive\")\n\n\n\n\nWe’ve already seen that, if they’re properly aligned and have the same extent, you can combine different rasters in a multiband raster data set.\n\nlayers &lt;- c(landcover_layers[[1]], landcover_layers[[3]])\n\nWe can also combine the pixel values of different rasters, but again, they need to be perfectly aligned and have the same extent. For this kind of analyses, it is easier to imagine all the rasters as overlapping layers, and some function is then applied to all the overlapping pixels to generate one or several new layers. These analyses are often called local analyses since they consider each pixel value within a raster separately.\nFor example we can easily find the areas with either forests or settlements by summing the two rasters. If we’d like to give more weights to the forests, this is also possible. The terra package extends most arithmetic functions to support SpatRaster objects.\n\nsettlements &lt;- landcover_layers[[1]]\nforests &lt;- landcover_layers[[3]]\n\ncombn &lt;- settlements + forests\ncombn2 &lt;- settlements + 2 * forests\nplot(combn2)\n\n\n\n\n\n\n\n\nIf you use multiband rasters, you can use the app() function to get the same result. You can also specify your own function. Note that the third example will produce a raster with 2 bands.\n\ncombn3 &lt;- app(layers, \"sum\")\ncombn4 &lt;- app(layers, \"max\")\ncombn5 &lt;- app(layers, function(i) {pi * sqrt(2 * i)})\n\nSometimes you’ll need to combine 2 or more discrete rasters but you’ll still want to be able to see the original categories in the output raster. You can use a simple trick to do that. First reclassify the first raster and use the following numbers as categories: 10, 20, 30, etc. Then reclassify the second raster, and use the following numbers as categories: 1, 2, 3, etc. After adding the two rasters, you’ll know immediately that, for example, a value of 24 means that you’re in an area within the second category of the first raster and fourth category of the second. If you have three rasters you simply need to use an higher order of magnitude for one of the rasters.\n\nreclMat_elev &lt;- matrix(c(0, 500, 100, 500, 700, 200, 700, Inf, 300), ncol = 3, byrow = TRUE)\nelev_recl2 &lt;- classify(elev, reclMat_elev, include.lowest = TRUE)\nlandcover_recl2 &lt;- classify(landcover_resample, reclMat_landcover)\nelev_land &lt;- elev_recl2 + landcover_recl2\nplot(as.factor(elev_land))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#focal-analysis",
    "href": "rasters_tips.html#focal-analysis",
    "title": "6  Tips and tricks for rasters",
    "section": "6.8 Focal analysis",
    "text": "6.8 Focal analysis\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\northo &lt;- rast(\"data/sempach_ortho.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\nreclMat_landcover &lt;- matrix(c(1, 1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9, 1, 10, 1, 11, 2, 12, 2,\n                    13, 2, 14, 2, 15, 2, 16, 2, 17, 2, 18, 2, 19, 3, 20, 3, 21, 3, 22, 3, 23, 4,\n                    24, 4, 25, 4, 26, 4), ncol = 2, byrow = TRUE)\nlandcover_recl &lt;- classify(landcover, reclMat_landcover)\nlandcover_layers &lt;- segregate(landcover_recl)\nnames(landcover_layers) &lt;- c(\"settlements\", \"agriculture\", \"forest\", \"unproductive\")\nforests &lt;- landcover_layers[[3]]\n\n\n\n\nUntil now we saw examples of global, zonal and local analyses with raster data. Now we’re going to dive in the beautiful world of focal analyses. Note that people doing data science and mathematics will usually use the word convolution instead. Focal means that we’re not only considering the pixel value, but also the values of the neighboring pixels. The neighborhood is called kernel or moving window and is usually a square (like a 3x3 square, the focal pixel and its 8 neighbors) or a circle. Once the neighborhood is defined, we perform an aggregation of the pixel values (e.g. by summing or averaging) within the neighborhood and store the output as the new value for the focal cell, and then we move the window to the next pixel. The following figure shows a example using a 3x3 moving window and the minimum as the aggregating function.\n\nNote that the output raster will usually be smaller than the original one (if the moving window is a \\(m\\)x\\(n\\) matrix, the output will have \\((m-1)\\) less rows and \\((n-1)\\) less columns). If you want a raster with the same size as the original, you’ll need to ignore the NA values, hence the window will be smaller in the margins of the raster.\nIn the next example we perform a smoothing of our DEM using a square moving window of size 21 (pixels) and the arithmetic mean as aggregating function. Note that the size of the moving window has to be an odd number.\n\nelev_focal_mean &lt;- focal(elev, w = 21, fun = \"mean\")\nelev_focal_mean2 &lt;- focal(elev, w = 21, fun = \"mean\", na.rm = TRUE)\nplot(c(elev, elev_focal_mean), mar = c(0, 1, 0, 3.5))\n\n\n\n\n\n\n\n\nFocal analyses are really interesting to characterize the habitat of a site since you also get information about the neighboring areas. Using a discrete raster we can for example compute the number of forest pixels in the neighborhood of each pixel.\n\nforests_focal_sum &lt;- focal(forests, 5, fun = \"sum\")\nplot(c(forests, forests_focal_sum))\n\n\n\n\n\n\n\n\nIn R, moving windows are stored as matrices whose values represents the weights of the neighbors. Until now we only used moving windows where all pixels in the neighborhood had the same weight. Changing these weights allows you to define your own kind of moving windows. The shape (rectangle or circle) usually doesn’t have a big influence but adjusting the weights allows all kind of different results (smoothing, sharpening, edge detection, etc.). The terra package provides the focalMat() function if you need to create specific windows. The first argument is the raster on which the focal analysis will be computed, the second is the size of the moving window (in the units of the CRS, not in pixels), and then you can specify the type. By default focalMat() will produce a matrix whose values sum to 1, this means that using the sum with these weights as aggregating function will actually compute the arithmetic mean. If we really want to compute the sum, we need to adjust the weights ourselves and set all non-zero values to 1.\n\n(fwin_rect &lt;- focalMat(forests, c(200, 200), type = \"rectangle\"))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,] 0.04 0.04 0.04 0.04 0.04\n[2,] 0.04 0.04 0.04 0.04 0.04\n[3,] 0.04 0.04 0.04 0.04 0.04\n[4,] 0.04 0.04 0.04 0.04 0.04\n[5,] 0.04 0.04 0.04 0.04 0.04\n\nfwin_rect[fwin_rect &gt; 0] &lt;- 1\nforests_focal_sum2 &lt;- focal(forests, fwin_rect, fun = \"sum\")\nidentical(values(forests_focal_sum), values(forests_focal_sum2))\n\n[1] TRUE\n\n(fwin_circle &lt;- focalMat(forests, 200, type = \"circle\"))\n\n           [,1]       [,2]       [,3]       [,4]       [,5]\n[1,] 0.00000000 0.00000000 0.07692308 0.00000000 0.00000000\n[2,] 0.00000000 0.07692308 0.07692308 0.07692308 0.00000000\n[3,] 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n[4,] 0.00000000 0.07692308 0.07692308 0.07692308 0.00000000\n[5,] 0.00000000 0.00000000 0.07692308 0.00000000 0.00000000\n\nfwin_circle[fwin_circle &gt; 0] &lt;- 1\nforests_focal_sum3 &lt;- focal(forests, fwin_circle, fun = \"sum\")\nplot(c(forests_focal_sum, forests_focal_sum3))\n\n\n\n\n\n\n\n\nWith focal analyses, we can actually perform standard image processing tasks similar to the ones you would find in Photoshop. For example we can easily apply a Gaussian blur to an orthophoto (here we only use the red band). The weights of a Gaussian moving window are based on the bivariate normal distribution, and in this case the second argument of the focalMat() function is the standard deviation of the distribution.\n\northo_red &lt;- ortho[[1]]\nfwin_gauss &lt;- focalMat(ortho_red, 1, type = \"Gauss\")\northo_red_focal &lt;- focal(ortho_red, fwin_gauss, type = \"sum\")\nplot(c(ortho_red, ortho_red_focal), col = grey.colors(256), legend = FALSE)\n\n\n\n\n\n\n\n\nIf you need to compute some typical indices used in landscape ecology within the moving window, then have a look at the landscapemetrics package.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#generate-raster-covariates",
    "href": "rasters_tips.html#generate-raster-covariates",
    "title": "6  Tips and tricks for rasters",
    "section": "6.9 Generate raster covariates",
    "text": "6.9 Generate raster covariates\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\n\n\n\n\nThe terra package is also really good at generating new covariates. For example you can use the terrain() function to compute terrain characteristics from elevation data. The available outputs are: slope, aspect, TPI, TRI, roughness and flow direction.\n\nslope &lt;- terrain(elev, \"slope\", unit = \"degrees\")\naspect &lt;- terrain(elev, \"aspect\", unit = \"degrees\")\n\nplot(c(slope, aspect), mar = c(0, 1, 0, 3.5))\n\n\n\n\n\n\n\n\nIf we look at the lake of Sempach, we see that the terrain() function is using an aspect value of 90° for all flat areas.\nAspect can be a bit tricky to work with because it is circular (a value of 355 is very similar to a value of 5). You’ll need to use circular statistics techniques to process it. Another solution that is often used is to decompose aspect in two orthogonal components: eastness and northness. Eastness is an index going from -1 to 1 representing the west-east component of aspect, while northness (also an index from -1 to 1) is representing the north-south component. To compute these values we use the sine and cosine functions on the aspect values, but these have to be in radians!\n\naspect_rad &lt;- terrain(elev, \"aspect\", unit = \"radians\")\neastness &lt;- sin(aspect_rad)\nnorthness &lt;- cos(aspect_rad)\n\nplot(c(eastness, northness), main = c(\"eastness\", \"northness\"), mar = c(0, 1, 0, 3.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nUse the slope raster to mask all the flat areas of the eastness raster. Use a value of 0 for these areas.\n\n\nCode\nflat_slope &lt;- slope\nflat_slope[flat_slope &lt; 0.1] &lt;- NA\neastness_corr &lt;- mask(eastness, flat_slope, updatevalue = 0)\n\n\n\n\nWe can also easily calculate distance rasters using the distance() function. In the next example, the value of each pixel in the output raster will be the distance to the nearest bird sighting. You can use this with other vector types, for example to compute the distance from streets or the distance to coastlines. The raster used as the first argument is there only to define the extent and resolution of the output (the values of the input raster are ignored).\n\ndist_from_obs &lt;- distance(elev, vect(obs))\nplot(dist_from_obs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nWhat is the largest distance from a bird sighting within the municipality of Sempach?\n\n\nCode\nobs_in_sempach &lt;- obs[sempach,]\ndist_sempach &lt;- distance(elev, obs_in_sempach)\ndist_sempach &lt;- mask(dist_sempach, sempach)\nglobal(dist_sempach, \"max\", na.rm = TRUE)\nplot(trim(dist_sempach))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#rasterize-vector-data",
    "href": "rasters_tips.html#rasterize-vector-data",
    "title": "6  Tips and tricks for rasters",
    "section": "6.10 Rasterize vector data",
    "text": "6.10 Rasterize vector data\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\n\n\n\n\nVector data provides a better precision but some operations can be really slow when using large data sets. Sometimes it can be interesting to convert your vector data sets to rasters. This will allow you to perform analyses that would have been to slow or even impossible using vectors.\nYou’ll need the rasterize() function to perform such transformations. The first argument is the vector data set you want to rasterize, the second argument is a template raster data set that will be used to provide the extent and resolution. You also need to specify a single attribute that will be used. It is not possible to rasterize a vector data set to a multiband raster.\n\nmuni_raster &lt;- rasterize(vect(muni), elev, field = \"popsize\")\nplot(muni_raster)\n\n\n\n\n\n\n\n\nThis rasterization process can also be used to aggregate vector data. In the next example we first create a template raster using the extent of Sempach and a resolution of 100 meters. Then we rasterize the sightings without specifying an attribute but using an aggregating function. Each pixel of the output raster will contain the number of sightings.\n\nr &lt;- rast(sempach, resolution = 100)\nnobs &lt;- rasterize(vect(obs), r, fun = \"sum\")\nnobs[is.na(nobs)] &lt;- 0\nplot(nobs)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#viewshed",
    "href": "rasters_tips.html#viewshed",
    "title": "6  Tips and tricks for rasters",
    "section": "6.11 Viewshed",
    "text": "6.11 Viewshed\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\nelev &lt;- rast(\"data/dem.tif\")\n\n\n\n\nUsing a digital elevation model and some knowledge about the curvature of the earth and refraction of the atmosphere, it is possible to estimate the area that is visible from a specific point of view. This is often called a viewshed analysis. Of course this doesn’t take buildings or trees/forests into account (except if you use a digital surface model which includes natural and human-made structures). All of this is easily computed with the viewshed() function. Note that you need to specify the coordinates of the observer as a vector (you can’t directly use an sf or sfc object). Let’s check what’s visible from our location.\n\nv &lt;- viewshed(elev, st_coordinates(soi))\nplot(v)\n\n\n\n\n\n\n\n\nThe function can also take into account that your point of view is above the ground (e.g., observation tower or building). Similarly you can also specify the height of a target. Let’s imagine that we’re standing on the roof of the building and we want to compute the area where we would see a Golden Eagle flying 100m above the ground (and making the assumption that we have really good binoculars).\n\nv &lt;- viewshed(elev, st_coordinates(soi), observer = 10, target = 100)\nplot(v)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "rasters_tips.html#crs-transformations",
    "href": "rasters_tips.html#crs-transformations",
    "title": "6  Tips and tricks for rasters",
    "section": "6.12 CRS transformations",
    "text": "6.12 CRS transformations\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(terra)\nelev &lt;- rast(\"data/dem.tif\")\nlandcover &lt;- rast(\"data/landcover.tif\")\ncrs(landcover) &lt;- \"EPSG:2056\"\nlandcover &lt;- as.factor(landcover)\n\n\n\n\nPerforming CRS transformations on vector data sets in relatively easy since you only have to transform the vertices. It is a bit more complex with raster data sets. Projecting a raster data involves a resampling, the geometric characteristics of your raster will change and the pixel values will be recomputed. If you have vector data in a specific CRS and rasters in another CRS but you want to use a common CRS, it’s usually better to transform the vectors. Since the operation involves resampling we also need to define the interpolation function. As we’ve seen before, you need to be careful when projecting discrete rasters and use nearest neighbor interpolation.\n\nelev_wgs84 &lt;- project(elev, \"EPSG:4326\", method = \"bilinear\")\nlandcover_wgs84 &lt;- project(landcover, \"EPSG:4326\", method = \"near\")\n\nplot(elev_wgs84)\n\n\n\n\n\n\n\n\nIf we have a look at some characteristics of the original and projected rasters, we see that some changes occurred…\n\nncell(elev)\n\n[1] 507360\n\nncell(elev_wgs84)\n\n[1] 434294\n\ndim(elev)\n\n[1] 604 840   1\n\ndim(elev_wgs84)\n\n[1] 463 938   1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks for rasters</span>"
    ]
  },
  {
    "objectID": "static_maps.html",
    "href": "static_maps.html",
    "title": "7  Static maps",
    "section": "",
    "text": "7.1 Basic maps\nAs we’ve seen in some previous examples, the basic function to make maps is simply the plot() function. Both sf and terra extend this function to support vector and raster data sets. When we create maps, we usually consider the different data sets as layers that we combine to produce a nice output. This is why we will often use the add = TRUE argument in the plot() function. Most of the standard arguments provided by the standard plot() function (such as pch, lwd, cex, etc.) are also supported for sf objects. If you want to see all the additional parameters, you can check the help files (?sf::plot and ?terra::plot). Here’s a simple example:\nplot(st_geometry(muni), col = \"grey90\", border = \"grey50\", lty = 3)\nplot(st_union(muni), col = NA, border = \"grey50\", lwd = 2, add = TRUE)\nplot(obs[\"name\"], pch = 16, cex = 2, add = TRUE)\nWhen we have polygons with some continuous attribute, we can create what is called a choropleth map. These maps are often used to represent spatially variables like population densities or per-capita income.\ncantons &lt;- st_read(\"data/geodata.gpkg\", \"cantons\", quiet = TRUE)\npar(mfrow = c(1, 3))\nplot(cantons[\"popsize\"])\n\n\n\n\n\n\n\nplot(cantons[\"popsize\"], nbreaks = 15)\n\n\n\n\n\n\n\nplot(cantons[\"popsize\"], breaks = \"quantile\")\nIf you need good colors for your maps, you should use the RColorBrewer and viridis packages. These were designed by color specialists and are highly recommended. For RColorBrewer you can visualize the existing color palettes on the following website: https://colorbrewer2.org.\nPlotting raster data is very similar\nplot(elev, breaks = 15, col = hcl.colors(15))\nYou need to be extra cautious when using data sets with different CRSs. Let’s have a look at this example:\ntemp_poly &lt;- st_buffer(soi, 2000000)\nplot(st_geometry(World))\nplot(temp_poly, col = \"grey90\", add = TRUE)\nplot(st_transform(temp_poly, 4326), col = \"grey90\", add = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "static_maps.html#basic-maps",
    "href": "static_maps.html#basic-maps",
    "title": "7  Static maps",
    "section": "",
    "text": "If you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\nelev &lt;- rast(\"data/dem.tif\")\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nTry a few options, try to change some colors, add some of the streets.\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nCompute the population density and plot the corresponding choropleth map. Find better values for the breaks and change the colors used.\n\n\nCode\ncantons$popdensity &lt;- cantons$popsize / st_area(cantons)\ncantons$popdensity &lt;- units::set_units(cantons$popdensity, 1/km^2)\nplot(cantons[\"popdensity\"], breaks = \"jenks\", nbreaks = 10, pal = hcl.colors(10))\n# Note: jenks usually generates breaks that produce good looking maps, but they're really hard to interpret...",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "static_maps.html#world-maps-and-globes",
    "href": "static_maps.html#world-maps-and-globes",
    "title": "7  Static maps",
    "section": "7.2 World maps and globes",
    "text": "7.2 World maps and globes\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\nAs we’ve seen before, the default projection that is used to plot data with a geographic CRS is not ideal. The distortions are large (especially near the poles) and the map will appear elongated. If you want to plot world maps, I recommend using either the Robinson projection, which was designed to be aesthetically pleasing, or the Equal-Earth projection, which has the advantage of having equal areas. The Winkel tripel projection can also be a good choice. We need to include a graticule to display the limits of the world map.\n\ngrat &lt;- st_geometry(st_graticule(lon = seq(-180, 180, by = 40), lat = seq(-91, 91, by = 30)))\n\nplot(st_transform(st_geometry(World), \"ESRI:54030\"))\nplot(st_transform(grat, \"ESRI:54030\"), col = \"grey50\", add = TRUE)\n\n\n\n\n\n\n\nplot(st_transform(st_geometry(World), \"EPSG:8857\"))\nplot(st_transform(grat, \"EPSG:8857\"), col = \"grey50\", add = TRUE)\n\n\n\n\n\n\n\n\nIf you want to plot a 2D globe, you can use the orthographic projection. You can center the globe on one point by modifying the lat_center and lon_center variables in the following code.\n\nlibrary(s2)\noptions(s2_oriented = TRUE) # don't change orientation of the geometries (since the orientation is already correct)\n\nlat_center &lt;- 40\nlon_center &lt;- -10\n\nearth &lt;- st_as_sfc(\"POLYGON FULL\", crs = \"EPSG:4326\")\n# If you specify the first argument (x = earth), some lines will be missing after clipping\ngrat &lt;- st_graticule(lon = seq(-180, 180, by = 20), lat = seq(-90, 90, by = 30))\ncountries &lt;- st_as_sf(s2_data_countries())\noceans &lt;- st_difference(earth, st_union(countries))\n\n# Clip the data using the visible half (otherwise problems can happen with the orthographic projection, for example countries with vertices in the non-visible part will disappear)\nbuff &lt;- st_buffer(st_as_sfc(paste0(\"POINT(\", lon_center, \" \", lat_center, \")\"), crs = \"EPSG:4326\"), 9800000) # visible half\ncountries_clipped &lt;- st_intersection(buff, countries)\noceans_clipped &lt;- st_intersection(buff, oceans)\ngrat_clipped &lt;- st_intersection(buff, grat)\n\nprojstring &lt;- paste0(\"+proj=ortho +lat_0=\", lat_center, \" +lon_0=\", lon_center)\n\nplot(st_transform(oceans_clipped, projstring), col = \"lightblue\")\nplot(st_transform(countries_clipped, projstring), col = NA, add = TRUE)\nplot(st_transform(grat_clipped, projstring), col = \"grey50\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou probably remember my warning about proj4strings, and that you should not use them for projections. I’m actually using one of these strings here to define the orthographic projection. Fortunately, for global maps, the inaccuracies are negligible. Moreover, it is easier to define the latitude and longitude of origin.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "static_maps.html#exporting-high-quality-maps",
    "href": "static_maps.html#exporting-high-quality-maps",
    "title": "7  Static maps",
    "section": "7.3 Exporting high quality maps",
    "text": "7.3 Exporting high quality maps\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\northo &lt;- rast(\"data/sempach_ortho.tif\")\n\n\n\n\nUsing the default R graphic device doesn’t always produce good quality maps, especially if you need to use them for publications. If you want high quality maps you can use the PNG cairo graphic device. If you’re producing maps containing orthophotos or topographic maps, you should use the JPEG cairo device instead.\n\npng(\"export/map.png\", width = 4200, height = 3000, res = 300, type = \"cairo\")\nplot(st_geometry(muni), col = \"grey90\", border = \"grey50\", lty = 3)\nplot(st_union(muni), col = NA, border = \"grey50\", lwd = 2, add = TRUE)\nplot(obs[\"name\"], pch = 16, cex = 2, add = TRUE)\ndev.off()\n\nWhen you include topographic maps or orthophotos in you maps, you might get disappointing results. By default, terra is using 500000 pixels to plot your map. If one of the layers has more pixels, terra will randomly sample them which can give the impression of a poor resolution. You can increase this number by using the maxcell argument. Be ready to wait a bit if you use really high values (small tip: to get the maximal resolution, use Inf).\n\npar(mfrow = c(1, 2))\nplot(ortho, maxcell = 10000)\nplot(ortho, maxcell = Inf)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "static_maps.html#swiss-map-with-hillshade",
    "href": "static_maps.html#swiss-map-with-hillshade",
    "title": "7  Static maps",
    "section": "7.4 Swiss map with hillshade",
    "text": "7.4 Swiss map with hillshade\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\n\n\n\n\nIf you need a nice Swiss map, you can use the following data and code snippet. The data and the hillshade were produced by Swisstopo and you should add the copyright if you use them. Note how the hillshade is customized and how the rivers get larger.\n\n# Import cartographic elements\nlakes &lt;- st_read(\"data/swiss_carto.gpkg\", \"lakes\", quiet = TRUE)\nrivers &lt;- st_read(\"data/swiss_carto.gpkg\", \"rivers\", quiet = TRUE)\nborder &lt;- st_read(\"data/swiss_carto.gpkg\", \"border\", quiet = TRUE)\n\n# Define the width of the rivers which will be used on the maps (rescale the attribute \"thickness\" between 0.5 and 2)\nriverswidth &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", rivers$thickness))\nwidth_std &lt;- (riverswidth - min(riverswidth)) * (2 - 0.5) / (max(riverswidth) - min(riverswidth)) + 0.5\n\n# Import hillshade (copyright swisstopo)\nhillshade &lt;- rast(\"data/hillshade.tif\")\n\n# Crop hillshade with the extent of Switzerland (add a small buffer to avoid clipping the map in some places)\nhillshade &lt;- crop(hillshade, ext(st_union(border, lakes)) + 2000)\n\n# Mask hillshade\nhillshade &lt;- mask(hillshade, border)\n\n# Simulate 70% opacity\nhillshade &lt;- 255 - 0.7*(255 - hillshade)\n\n# Plot\nplotRGB(hillshade, smooth = TRUE, mar = c(1, 1, 1, 1), axes = FALSE, legend = FALSE, maxcell = Inf)\nplot(st_geometry(border), col = NA, border = rgb(130, 130, 130, maxColorValue = 255), add = TRUE, lwd = 3)\nplot(st_geometry(rivers), col = rgb(69, 138, 230, maxColorValue = 255), add = TRUE, lwd = width_std)\nplot(st_geometry(lakes), col = rgb(162, 197, 243, maxColorValue = 255), border = rgb(69, 138, 230, maxColorValue = 255), add = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "static_maps.html#tmap",
    "href": "static_maps.html#tmap",
    "title": "7  Static maps",
    "section": "7.5 tmap",
    "text": "7.5 tmap\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(s2)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nsempach &lt;- muni[muni$name == \"Sempach\",]\nstreets &lt;- st_read(\"data/geodata.gpkg\", \"streets\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nobs_in_sempach &lt;- obs[sempach,]\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\nelev &lt;- rast(\"data/dem.tif\")\northo &lt;- rast(\"data/sempach_ortho.tif\")\n\n\n\n\nIf you want to make more complex maps, you can still achieve almost everything using standard plotting procedures but you’ll spend a lot of time trying to hack everything. If you’re a ggplot2 user, you can of course use it with spatial data. However there’s another package, tmap, that is similar to ggplot2 but more intuitive for maps and more GIS friendly. The tmap package is also based on the grammar of graphics approach that separates data and aesthetics.\n\n\n\n\n\n\nImportant\n\n\n\nThis tutorial was written for tmap version 4.x. It will not work with older versions of tmap.\n\n\n\n7.5.1 Mapping layers\nTo create a map with tmap, you first need to specify the data set you want to plot with the tm_shape() function. This function will accept the usual spatial objects created by sf and terra (and stars). Then you need to tell tmap how to represent the data using specific map layer functions (e.g. tm_polygons(), tm_dots()). An example should make things a bit clearer.\n\ntm_shape(muni) + tm_polygons()\n\n\n\n\n\n\n\n\nTo adapt the symbology (color, line width, etc.) of the layers, we can specify visual variables (also called aesthetic mappings) inside the map layer functions. For polygons (and points), we use the fill variable to change the fill color of the features and the col variable to change the border color. You can also specify opacity with the variables fill_alpha and col_alpha. For line width and type, we use the standard lwd and lty variables. For lty you can use either a number from 0 to 6 or the corresponding character strings: blank, solid, dashed, dotted, dotdash, longdash or twodash).\n\ntm_shape(muni) + tm_polygons(fill = \"lightgreen\", col = \"darkgreen\", lwd = 2, lty = \"dotted\")\n\n\n\n\n\n\n\n\nUntil now we only had one layer for one data set, but one data set can easily be represented by several layers. We simply need to add other map layer functions. Let’s improve the previous example by adding centroids and labels for each municipality (the ymod variable is adding a vertical offset).\n\ntm_shape(muni) + tm_polygons() + tm_dots(size = 1.5) + tm_text(\"name\", ymod = 1, col = \"navy\")\n\n\n\n\n\n\n\n\nOne nice feature of the package is that you can save map objects and modify them later\n\nmap1 &lt;- tm_shape(muni) + tm_polygons()\nmap1 + tm_dots(size = 1.5) + tm_text(\"name\", ymod = 1, col = \"navy\")\n\n\n\n\n\n\n\n\nIf you want to plot several data sets on the same map, you need to use tm_shape() for each data set. Of course, you will also need to use at least one map layer function for each object inside a tm_shape() function to define how the data will be plotted.\n\ntm_shape(muni) + tm_borders(col = \"black\", lwd = 2) +\ntm_shape(streets) + tm_lines(col = \"grey50\", lwd = 0.5) +\ntm_shape(obs) + tm_dots(size = 0.4, fill = \"navy\")\n\n\n\n\n\n\n\n\nBy default, tmap is using the first tm_shape() function to define the extent (and the CRS) of the map. It will referenced as the “main” shape. If you need to define your extent using another data set, you can add the is.main = TRUE argument to the desired tm_shape() function.\n\ntm_shape(sempach) + tm_polygons() + tm_shape(obs) + tm_dots()\n\n\n\n\n\n\n\ntm_shape(sempach) + tm_polygons() + tm_shape(obs, is.main = TRUE) + tm_dots()\n\n\n\n\n\n\n\n\nIf your spatial objects are using different CRSs, tmap will use the CRS of the object inside the first tm_shape() function for the map and automatically reproject the data sets used in the following tm_shape() functions.\n\ntm_shape(World) + tm_polygons() + tm_shape(st_buffer(soi, 2000000)) + tm_borders(col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nWe’re not limited to vector data, tmap can easily plot SpatRaster objects (and also rasters coming from the stars package). We just need to combine tm_shape() with tm_raster() or tm_rgb() if you have RGB multiband rasters. Internally tmap is using the stars package to process rasters, therefore an internal conversion is happening for SpatRaster objects. By default rasters will be classified, if you want a continuous map, you can add the col.scale = tm_scale_continuous() argument to the tm_raster() function. You can of course combine vector and raster data.\n\ntm_shape(elev) + tm_raster()\n\n\n\n\n\n\n\ntm_shape(elev) + tm_raster(col.scale = tm_scale_continuous())\n\n\n\n\n\n\n\n\nIf you have a large raster, tmap will downsample it to gain speed. If you need the full resolution, you can change the raster.max_cells option using the tmap_options() function (this will stay like this until you close R or change the option again).\n\ntmap_options(raster.max_cells = 10000)\ntm_shape(ortho) + tm_rgb()\n\nSpatRaster object downsampled to 85 by 119 cells.\n\n\n\n\n\n\n\n\ntmap_options(raster.max_cells = Inf)\ntm_shape(ortho) + tm_rgb()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe tmap package is extremely powerful and allows making complex maps. However the number of options is really large and it is easy to feel a bit lost. Fortunately tmap also provides the qtm() function for quick mapping with sensible defaults. You can also combine data sets with repeated qtm() calls.\n\nqtm(muni)\nqtm(muni) + qtm(obs, fill = \"navy\")\n\n\n\n\n\n7.5.2 Choropleth maps\nProducing a choropleth map is extremely easy with tmap. You just need to specify the attribute name inside the tm_polygons() function using the fill argument. This is called a data variable. The legend is automatically added.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\")\n\n\n\n\n\n\n\n\nChanging the classification method is also possible by changing the scale of the visual variable (not to be confused with the scale of the map). In this context, a scale defines how to map the data values to visual values. Since you want to change the fill color of the polygons, you need to use the fill.scale argument. This argument expects a function that will tell tmap how to map the data. Here we use the generic tm_scale() function which will automatically choose an appropriate scaling function based on the data type and the visual variable. The most important scaling functions are: intervals (tm_scale_intervals()), continuous values (tm_scale_continuous()) and categorical values (tm_scale_categorical() and tm_scale_ordinal()). A lot more are available… If you already know which scaling function you need, I’d recommend using the appropriate one directly, instead of relying too much on tm_scale().\nYou can for example change the number of breaks used for the color fill with the n argument.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.scale = tm_scale(n = 10))\n\n\n\n\n\n\n\n\nYou can also specify the breaks manually with the breaks argument.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.scale = tm_scale(breaks = c(0, 3000, 5000, 11000)))\n\n\n\n\n\n\n\n\nIt is of course also possible to change the algorithm used to define the breaks by changing the style argument inside the scaling function. By default tmap uses the pretty algorithm but you can also use quantile, equal, sd, log10_pretty and many more.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.scale = tm_scale(style = \"quantile\"))\n\n\n\n\n\n\n\n\nThe logic is similar if you want to modify the legend. Since you want to modify the legend for the fill color, you need to use the fill.legend argument. This argument expects a function that will tell tmap how to draw the legend. Here we use the tm_legend() function, the first argument is the legend title.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.legend = tm_legend(\"Population size\"))\n\n\n\n\n\n\n\n\nYou can easily map several attributes in the same plotting window.\n\nmuni$area &lt;- st_area(muni)\ntm_shape(muni) + tm_polygons(fill = c(\"popsize\", \"area\"))\n\n\n\n\n\n\n\n\nThe default color palettes used by tmap are quite good but sometimes they won’t be appropriate. As we’ve seen earlier, the RColorBrewer and viridis packages provide really good color palettes. The cols4all1 package provides a nice function to visualize most of the palettes available in tmap: c4a_gui()\n\ncols4all::c4a_gui()\n\nChanging the color fill is a bit different with choropleth maps since the fill variable is already used by the attribute name. With tmap, you do that inside the scaling function using the values argument (the name of this argument is a bit confusing in this case, but do not forget that scaling functions are also used for other visual variables such as line type or width). You can easily reverse the color palette by putting a - in front of the palette name.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.scale = tm_scale(values = \"viridis\"),\n                             fill.legend = tm_legend(\"Population size\"),\n                             col = \"white\", lwd = 0.5)\n\n\n\n\n\n\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.scale = tm_scale(values = \"-viridis\"),\n                             fill.legend = tm_legend(\"Population size\"),\n                             col = \"white\", lwd = 0.5)\n\n\n\n\n\n\n\n\nThe next example is not a choropleth map, but it shows you another way to plot a continuous variable on a map. The argument values.scale inside the scaling function is used to scale the numerical values (otherwise the bubbles would be too small).\n\ntm_shape(muni) + tm_borders() + tm_bubbles(size = \"popsize\",\n                                           size.scale = tm_scale(values.scale = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (5 minutes)\n\n\n\nTry to make a choropleth map of population size combined with tm_bubbles(). What is happening with the legend? Try to give a different title to both legends.\n\n\nCode\ntm_shape(muni) + tm_polygons(fill = \"popsize\", fill.legend = tm_legend(\"Population size (color)\")) +\n                 tm_bubbles(size = \"popsize\", size.scale = tm_scale(values.scale = 2), size.legend = tm_legend(\"Population size (circles)\"))\n\n\n\n\n\n\n7.5.3 Customizing layout\nWe can include extra objects in our map, such as titles, scale bars or north arrows. Each element has a position argument. In the next example we also include some styling using the tm_style() function. Currently 8 styles are available (have a look at the help file). We finally use the tm_layout() function to increase the right margin and draw a frame with sharp corners. For margins (specified as bottom, left, top, right), the units are relative to the map frame, so 0.05 means 5 percent of the frame height.\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.legend = tm_legend(title = \"Population size\",\n                                                     position = c(\"right\", \"top\"),\n                                                     frame = FALSE)) +\ntm_compass(position = c(\"left\", \"bottom\")) +\ntm_scalebar(breaks = c(0, 1, 2, 3, 4), position = c(\"left\", \"bottom\")) +\ntm_title(\"Population size around Sempach\", position = tm_pos_out(\"center\", \"top\", \"center\")) +\ntm_style(\"grey\") +\ntm_layout(inner.margins = c(0.05, 0.05, 0.05, 0.3), frame.r = 0)\n\n\n\n\n\n\n\n\nIn the next example we customize two maps in the same plotting window. We also use the tm_title() function to add nicer titles and the tm_credits() function to add some copyright information. Note the difference when using lowercase vs. uppercase for the positioning of the map components. We also tell tmap to remove the automatic title panels using the panel.show = FALSE argument inside the tm_layout() function. Lastly, note how we change the colors and the title of the legends for the two maps using list objects, and the use of an expression to properly display the units.\n\ntm_shape(muni) + tm_polygons(fill = c(\"popsize\", \"area\"),\n                             fill.scale = list(tm_scale(values = \"brewer.reds\"), tm_scale(values = \"brewer.blues\")),\n                             fill.legend = list(tm_legend(\"Population size\"), tm_legend(expression(\"Area [m\"^2*\"]\")))) +\ntm_title(c(\"Population size\", \"Area\"), size = 1.1, position = c(\"right\", \"top\")) + \ntm_credits(\"Data © swisstopo\", position = c(\"LEFT\", \"BOTTOM\")) +\ntm_layout(panel.show = FALSE)\n\n\n\n\n\n\n\n\nHaving multiple maps on the same plot can be achieved using the tmap_arrange() function. First create your maps and store them in tmap objects, then use them with tmap_arrange(). This function is especially useful since the usual par(mfrow) trick doesn’t work with tmap objects.\n\nmap1 &lt;- tm_shape(muni) + tm_polygons()\nmap2 &lt;- tm_shape(sempach) + tm_polygons() + tm_shape(obs_in_sempach) + tm_dots()\nmap3 &lt;- tm_shape(muni) + tm_polygons(fill = \"popsize\", fill.legend = tm_legend(position = c(\"left\", \"bottom\"), frame = FALSE))\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\nIf you need multiple maps with a common legend, you should use the functions tm_facets_wrap() or tm_facets_stack(), depending on the layout you want. The function tm_facets() will decide which one to use automatically based on your data. Facets can even be defined for two or three dimensions using the tm_facets_grid() function. In the next example we first generate a fake continuous attribute and then automatically create separate maps for each bird species, using the new attribute for the legend.\n\nobs$var1 &lt;- runif(nrow(obs), 1, 100)\ntm_shape(obs) + tm_symbols(fill = \"var1\") + tm_facets_wrap(\"name\")\n\n\n\n\n\n\n\ntm_shape(obs) + tm_symbols(fill = \"var1\") + tm_facets_stack(\"name\")\n\n\n\n\n\n\n\n\n\n\n7.5.4 World maps and globes\nThe default projection that is used to plot data with a geographic CRS in tmap is the same as before. We’ve seen that it’s not ideal since the distortions are large (especially near the poles) and the map will appear elongated.\n\ntm_shape(World) + tm_polygons(\"HPI\") +\ntm_title(\"Happy Planet Index\")\n\n\n\n\n\n\n\n\nAs in the previous section, I recommend using either the Robinson projection, which was designed to be aesthetically pleasing, or the Equal-Earth projection, which has the advantage of having equal areas. The Winkel tripel projection can also be a good choice. With tmap, you can use the tm_crs() function to define the projection that will be applied to the map. For world maps, you can use the argument earth_boundary = TRUE inside the tm_layout() function to display the earth boundary (this is only available for a few global projections, such as the ones we’re using). We also remove the map frame using frame = FALSE.\n\nmap &lt;- tm_shape(World) + tm_polygons(fill = \"HPI\") +\ntm_title(\"Happy Planet Index\") +\ntm_layout(earth_boundary = TRUE, frame = FALSE)\n\n# Equal-Earth\nmap + tm_crs(\"EPSG:8857\")\n\n\n\n\n\n\n\n# Robinson\nmap + tm_crs(\"ESRI:54030\")\n\n\n\n\n\n\n\n# Winkel tripel\nmap + tm_crs(\"+proj=wintri\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou probably remember my warning about proj4strings, and that you should not use them for projections. I’m actually using one of these strings here to define the Winkel tripel projection. The main reason is that there is no EPSG (or ESRI) number for this CRS. Moreover, for global maps, the inaccuracies are negligible.\n\n\nYou can also use the tm_style(\"natural\") function to specify a style adapted for world maps. Here’s an example using the Robinson projection.\n\ntm_shape(World) + tm_polygons(fill = \"HPI\") +\ntm_title(\"Happy Planet Index\") +\ntm_layout(earth_boundary = TRUE, frame = FALSE) +\ntm_style(\"natural\") +\ntm_crs(\"ESRI:54030\")\n\n\n\n\n\n\n\n\nYou can easily add graticule lines to your world map. We first need to generate them using the st_graticule() function (from the sf package). In theory you could use the tmap function tm_graticules() but unfortunately, depending on the projection, the lines don’t go all the way to the poles.\n\ngrat &lt;- st_graticule(lon = seq(-180, 180, by = 20), lat = seq(-90, 90, by = 30))\n\ntm_shape(World) +\n    tm_polygons(fill = \"HPI\") +\ntm_shape(grat) +\n    tm_lines(col = \"grey70\") +\ntm_title(\"Happy Planet Index\") +\ntm_layout(earth_boundary = TRUE, frame = FALSE) +\ntm_crs(\"ESRI:54030\")\n\n\n\n\n\n\n\n\nIf you want to plot a 2D globe, you can use the orthographic projection (which is also using a proj4string). You can center the globe on one point by modifying the lat_center and lon_center variables in the following code.\n\nlat_center &lt;- 40\nlon_center &lt;- -10\nprojstring &lt;- paste0(\"+proj=ortho +lat_0=\", lat_center, \" +lon_0=\", lon_center)\n\ntm_shape(World, crs = projstring, bbox = \"FULL\") +\n    tm_polygons() +\ntm_graticules(n.x = 20, n.y = 10, col = \"black\", lwd = 0.5, labels.show = FALSE) +\ntm_style(\"natural\") +\ntm_xlab(\"Longitudes\", size = 1.1) + tm_ylab(\"Latitudes\", size = 1.1)\n\n\n\n\n\n\n\n\nOr you can use the code we used in the previous section to create the data, but this time we will use tmap to plot everything.\n\noptions(s2_oriented = TRUE) # don't change orientation of the geometries (since the orientation is already correct)\n\nearth &lt;- st_as_sfc(\"POLYGON FULL\", crs = \"EPSG:4326\")\n# If you specify the first argument (x = earth), some lines will be missing after clipping\ngrat &lt;- st_graticule(lon = seq(-180, 180, by = 20), lat = seq(-90, 90, by = 30))\ncountries &lt;- st_as_sf(s2_data_countries())\noceans &lt;- st_difference(earth, st_union(countries))\n\n# Clip the data using the visible half (otherwise problems can happen with the orthographic projection, for example countries with vertices in the non-visible part will disappear)\nbuff &lt;- st_buffer(st_as_sfc(paste0(\"POINT(\", lon_center, \" \", lat_center, \")\"), crs = \"EPSG:4326\"), 9800000) # visible half\ncountries_clipped &lt;- st_intersection(buff, countries)\noceans_clipped &lt;- st_intersection(buff, oceans)\ngrat_clipped &lt;- st_intersection(buff, grat)\n\ntm_shape(oceans_clipped) +\n    tm_polygons(fill = \"lightblue\") +\ntm_shape(countries_clipped) +\n    tm_borders() +\ntm_shape(grat_clipped) +\n    tm_lines(col = \"grey50\") +\ntm_crs(projstring)\n\n\n\n\n\n\n\n\n\n\n7.5.5 Saving maps\nSaving maps is trivially easy using the tmap_save() function. The format is automatically recognized using the file name extension. You can also specify the height, width and resolution of the map.\n\nmap1 &lt;- tm_shape(muni) + tm_polygons()\ntmap_save(map1, \"mymap.png\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "static_maps.html#mapsf",
    "href": "static_maps.html#mapsf",
    "title": "7  Static maps",
    "section": "7.6 mapsf",
    "text": "7.6 mapsf\nOn the to-do list…\n\n\n\n\n1. Tennekes M, Puts MJH. cols4all: A color palette analysis tool. In: EuroVis (Short Papers). Wiley 2023.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Static maps</span>"
    ]
  },
  {
    "objectID": "dynamic_maps.html",
    "href": "dynamic_maps.html",
    "title": "8  Dynamic maps",
    "section": "",
    "text": "8.1 mapview\nThe mapview package provide a really simple interface to quickly create dynamic maps and it is thus my favorite for data exploration. It uses the leaflet package (which uses the leaflet JavaScript library) to do all the rendering. As we will see later it’s also more efficient than other packages when you have large data sets.\nOne of the default options (fgb) of the mapview package will cause some problems with several examples in this tutorial. We will thus deactivate it.\nlibrary(mapview)\nmapviewOptions(fgb = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic maps</span>"
    ]
  },
  {
    "objectID": "dynamic_maps.html#mapview",
    "href": "dynamic_maps.html#mapview",
    "title": "8  Dynamic maps",
    "section": "",
    "text": "If you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nelev &lt;- rast(\"data/dem.tif\")\n\n\n\n\n\n\n\n\n8.1.1 Simple maps\nThe main function used to create maps is simply called mapview(). It will accept any sf or sfc object.\n\nmapview(muni)\n\n\n\n\n\nIf you don’t like the default color, you can change it using the color.regions argument. It is also possible to specify the name of an attribute that will be shown on mouseover using the label argument. Here we will use the name of the municipalities.\n\nmapview(muni, col.regions = \"purple\", label = \"name\")\n\n\n\n\n\nYou can also easily plot multiple data sets using a list or by adding mapview objects together. Note how it’s possible to show/hide layers in the map.\n\nmapview(list(muni, obs))\nmapview(muni) + mapview(obs)\nmapview(muni) + obs\n\n\n\n\n\n\n\nIf you use a list to combine several data sets, you can easily customize some arguments per data set (also using lists).\n\nmapview(list(muni, obs), legend = list(FALSE, TRUE), homebutton = list(TRUE, FALSE))\n\n\n\n\n\nIf you want to produce a map for a specific data set but use the extent of another data set, you can use the hide = TRUE argument for the data set defining the extent.\n\nmapview(muni[muni$name == \"Sursee\",], layer.name = \"Sursee\") + mapview(muni, hide = TRUE)\n\n\n\n\n\nFor data exploration, it can sometimes be useful to “explode” a data set by columns. This is possible thanks to the burst argument. The result will be a map with one layer per attribute.\n\nmapview(muni, burst = TRUE)\n\n\n\n\n\nSometimes it’s more interesting to burst a data set by rows. For example if you have a data set containing data for several species, you can easily produce a map with one layer per species. To do that, the burst argument must be equal to the name of the splitting attribute (or you must specify something for the zcol argument).\n\nmapview(obs, burst = \"name\")\n# Equivalent\nmapview(obs, zcol = \"name\", burst = TRUE)\n\n\n\n\n\n\n\nThe mapview package can also plot raster data sets. It will accept terra and stars objects. Transparency should be possible but it’s currently not working (at least on my computer). Since the raster will be reprojected, you need to choose the resampling algorithm carefully (e.g., bilinear for continuous rasters and nearest neighbor for discrete ones). You can specify it using the method argument.\n\nmapview(elev, alpha.regions = 0.5)\n\nNumber of pixels is above 5e+05.Only about 5e+05 pixels will be shown.\nYou can increase the value of `maxpixels` to 507360 to avoid this.\n\n\n\n\n\n\nYou can easily change the available background maps using the map.types argument. All the available basemaps with their respective names are shown on the following website: https://leaflet-extras.github.io/leaflet-providers/preview. You can also get the full list by calling the function names(leaflet::providers). It is for example possible to use the Swiss national maps and orthophotos.\n\nmapview(obs, map.types = c(\"SwissFederalGeoportal.NationalMapColor\",\n                           \"SwissFederalGeoportal.NationalMapGrey\",\n                           \"SwissFederalGeoportal.SWISSIMAGE\"))\n\n\n\n\n\nIf you want to have the Swiss maps by default when using mapview, you can change the options for the current R session using the basemaps argument of the mapviewOptions() function. Restoring the default options is also possible.\n\nmapviewOptions(basemaps = c(\"SwissFederalGeoportal.NationalMapColor\",\n                            \"SwissFederalGeoportal.NationalMapGrey\",\n                            \"SwissFederalGeoportal.SWISSIMAGE\"))\nmapview(muni)\n\n\n\n\n# Restore defaults\nmapviewOptions(default = TRUE)\n\n\n\n8.1.2 Choropleth maps\nProducing choropleth maps is also possible. You need to specify the attributes using the zcol argument.\n\nmapview(muni, zcol = \"popsize\")\n\n\n\n\n\nYou can specify your own values for the breakpoints used for the visualization. Here we use the classInt package to compute the breakpoints based on the quantiles of one variable. Check the help page of the classIntervals() function to see all the available breakpoint types.\n\nlibrary(classInt)\nbreaks &lt;- classIntervals(muni$popsize, n = 4, style = \"quantile\")\nmapview(muni, zcol = \"popsize\", at = breaks$brks)\n\n\n\n\n\nUse a list for the zcol argument if you need to visualize several data sets together but using different attributes for the symbology.\n\nmapview(list(muni, obs), zcol = list(\"popsize\", \"name\"))\n\n\n\n\n\n\n\n8.1.3 Customize popups\nYou can easily change the attribute that is shown for mouseovers when using a vector data set. Just set the label argument to the name of the attribute.\n\nmapview(muni, zcol = \"popsize\", label = \"name\")\n\n\n\n\n\nIt is also possible to restrict the number of attributes shown when clicking on a feature thanks to the popup argument.\n\nmapview(muni, popup = c(\"name\", \"popsize\"))\n\n\n\n\n\nIt is also possible to completely change the display of the popup with the help of the leafpop package. Here’s one example where we display a photo of the species for some bird sightings in our data set.\n\nlibrary(leafpop)\n\nobs2 &lt;- obs[20:40,]\n\nblackbird_img &lt;- \"https://www.vogelwarte.ch/wp-content/assets/images/bird/species/4240_1.jpg\"\nbluetit_img &lt;- \"https://www.vogelwarte.ch/wp-content/assets/images/bird/species/3800_1.jpg\"\nwagtail_img &lt;- \"https://www.vogelwarte.ch/wp-content/assets/images/bird/species/5030_1.jpg\"\nimgs &lt;- c(blackbird_img, bluetit_img, wagtail_img)\n\nimgs &lt;- character(nrow(obs2))\nimgs[which(obs2$name == \"Eurasian Blackbird\")] &lt;- blackbird_img\nimgs[which(obs2$name == \"Eurasian Blue Tit\")] &lt;- bluetit_img\nimgs[which(obs2$name == \"White Wagtail\")] &lt;- wagtail_img\n\nmapview(obs2, popup = popupImage(imgs, src = \"remote\"))\n\n\n\n\n\n\n\n8.1.4 Compare maps\nIf you want to compare maps or data sets, you can use two interesting packages in combination with mapview (and leaflet) objects.\nThe first possibility consists of adding a slider to switch between two maps. In the following example, we first create two new data sets using the municipalities. The first one has an attribute showing the number of bird sightings in April, and the second one has an attribute for the number of sightings in July.\n\nobs$date &lt;- as.Date(obs$date)\nobs$month &lt;- as.numeric(format(obs$date, \"%m\"))\n\ncounts_muni_april &lt;- st_as_sf(data.frame(counts = lengths(st_intersects(muni, obs[obs$month == 4,])), geometry = muni))\ncounts_muni_july &lt;- st_as_sf(data.frame(counts = lengths(st_intersects(muni, obs[obs$month == 7,])), geometry = muni))\n\nWe can now create the two mapview objects and compare them thanks to the | operator of the leaflet.extras2 package. Note that we need to manually specify the breakpoints to be sure that both data sets have the same ones.\n\nlibrary(leaflet.extras2)\n\nLoading required package: leaflet\n\nmaxcounts &lt;- max(max(counts_muni_april$counts), max(counts_muni_july$counts))\n\nm1 &lt;- mapview(counts_muni_april, zcol = \"counts\", at = pretty(0:maxcounts))\nm2 &lt;- mapview(counts_muni_july, zcol = \"counts\", at = pretty(0:maxcounts))\n\nm1|m2\n\n\n\n\n\nThe other possibility consists of displaying the maps side by side and synchronizing them using the sync() function of the leafsync package. Instead of using two data sets, we will now use the same data set but with two different background maps.\n\nlibrary(leafsync)\n\nm1 &lt;- mapview(counts_muni_april, zcol = \"counts\", map.types = \"CartoDB.Positron\")\nm2 &lt;- mapview(counts_muni_april, zcol = \"counts\", map.types = \"Esri.WorldImagery\")\nsync(m1, m2)\n\n\n\n\n\n\n\n\n\n\n\n\nIf the extent of the two data sets you want to compare is different, be sure to use the hide=TRUE argument (see example above) to force an initial common extent for the two maps.\n\nm1 &lt;- mapview(muni[muni$name == \"Sursee\",], layer.name = \"Sursee\") + mapview(muni, hide = TRUE)\nm2 &lt;- mapview(muni[muni$name == \"Eich\",], layer.name = \"Eich\") + mapview(muni, hide = TRUE)\nsync(m1, m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.5 Large data sets\nIf you have large data sets with thousands of points or hundreds of complex polygons, the default leaflet library will probably not be able to render your data. In this case you can change the rendering engine to use the leafgl package (based on the Leaflet.glify library extending leaflet). In this case it is recommended to deactivate the map viewer in RStudio since it can cause some crashes. The map will be displayed in a new browser window.\n\nmapviewOptions(platform = \"leafgl\", viewer.suppress = TRUE)\npts &lt;- st_sample(muni, 100000)\npts &lt;- st_intersection(muni, pts)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nmapview(pts)\n\nLoading required namespace: leafgl\n\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n# Restore defaults\nmapviewOptions(default = TRUE)\n\n\n\n8.1.6 Customize zoom/extent\nUnfortunately it is not possible to limit the zoom factors and/or the map extent directly in the mapview() function. However, since mapview also produces a leaflet object, there are indirect solutions. It is possible to either first create a custom-made leaflet object and assign it to our mapview object. Or we can first create our mapview object and then manually hack the leaflet object inside.\nHere we limit only the zooming.\n\nmap &lt;- leaflet::leaflet(options = leafletOptions(minZoom = 10, maxZoom = 12)) |&gt; addTiles()\nmapview(muni, map = map)\n\n\n\n\n\nWe can also produce a semi-static map. No dragging or zooming is allowed but the map is still interactive.\n\nmap &lt;- leaflet::leaflet(options = leafletOptions(zoomControl = FALSE, minZoom = 12, maxZoom = 12, dragging = FALSE)) |&gt; addTiles()\nmapview(muni, map = map)\n\n\n\n\n\nIf we need to limit the zoom factors and the extent, we do the following.\n\nmap &lt;- leaflet::leaflet(options = leafletOptions(minZoom = 10, maxZoom = 12)) |&gt; addTiles() |&gt; setMaxBounds(lng1 = 8, lat1 = 47, lng2 = 8.5, lat2 = 47.3)\nmapview(muni, map = map)\n\n\n\n\n\nHere’s how we can hack the leaflet object after creating the mapview object.\n\nm &lt;- mapview(muni)\nm@map &lt;- leaflet::setMaxBounds(m@map, lng1 = 8, lat1 = 47, lng2 = 8.5, lat2 = 47.3)\nm@map$x$options$minZoom &lt;- 10\nm@map$x$options$maxZoom &lt;- 12\nm\n\n\n\n\n\nThe following hack produces the same result for the extent (instead of using the setMaxBound() function).\n\nm &lt;- mapview(muni)\nm@map$x$options$maxBounds &lt;- list(list(c(47, 8)), list(c(47.3, 8.5)))\n\n\n\n8.1.7 Saving maps\nOnce you’re happy with your map, you can export an HTML file using the mapshot() function. It should also be possible to export your map as a static image using the file argument instead of url. Unfortunately it doesn’t seem to work with the current version of mapview. If you manage to make it work, you can decide which controls should be removed (or not, typically the scale bar) using the remove_controls argument.\n\nmap &lt;- mapview(muni, col.regions = \"purple\", label = \"name\")\nmapshot(map, url = \"export/testmap.html\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic maps</span>"
    ]
  },
  {
    "objectID": "dynamic_maps.html#tmap",
    "href": "dynamic_maps.html#tmap",
    "title": "8  Dynamic maps",
    "section": "8.2 tmap",
    "text": "8.2 tmap\n\n\n\n\n\n\nIf you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nmuni &lt;- st_read(\"data/geodata.gpkg\", \"municipalities\", quiet = TRUE)\nmuni$area &lt;- st_area(muni)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\nsoi &lt;- st_as_sfc(\"POINT(2657271 1219754)\", crs = \"EPSG:2056\")\nelev &lt;- rast(\"data/dem.tif\")\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis tutorial was written for tmap version 4.x. It will not work with older versions of tmap.\n\n\nThe tmap package is not only amazing for static maps, you can also produce dynamic maps using the same code. You need to switch the tmapmode using tmap_mode(\"view\"). If you want to produce static maps, you can switch back to the standard mode using tmap_mode(\"plot\"). To quickly switch between the two modes, you can also use the ttm() function (without argument). Most of the functions and parameters are also available for dynamic maps. Similarly to mapview, the data is automatically projected to the Pseudo-Mercator CRS.\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(muni) + tm_polygons(fill = \"popsize\",\n                             fill.scale = tm_scale_intervals(style = \"quantile\", values = \"viridis\"),\n                             fill.legend = tm_legend(\"Population size\"),\n                             col = \"white\", lwd = 0.5)\n\n\n\n\n\n\nIt if of course possible to combine different data sets and use rasters. Once again, this is the same code we would have used for a static map.\n\ntm_shape(elev) +\n    tm_raster(col.scale = tm_scale_continuous()) +\ntm_shape(muni) +\n    tm_borders() +\ntm_shape(obs) +\n    tm_dots(size = 0.4, fill = \"violet\")\n\n\n\n\n\n\nIf you plot two maps side by side, they will be synchronized.\n\ntm_shape(muni) + tm_polygons(fill = c(\"popsize\", \"area\"),\n                             fill.scale = tm_scale(values = \"brewer.oranges\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you need other background maps, you can use the tm_basemap() function. Like with mapview, it is of course also possible to use the Swiss topographic maps or the orthophotos provided by Swisstopo. All the available basemaps with their respective names are shown on the following website: https://leaflet-extras.github.io/leaflet-providers/preview. You can also get the full list by calling the function names(leaflet::providers). If your favorite map is not listed there but you know the URL of a tile server (such as WMTS), you can also use it inside the tm_basemap() function (e.g., for the Swiss grey map, you would use the following: tm_basemap(\"https://wmts.geo.admin.ch/1.0.0/ch.swisstopo.pixelkarte-grau/default/current/3857/{z}/{x}/{y}.jpeg\")).\n\nm1 &lt;- tm_shape(soi) + tm_dots(fill = \"red\", size = 1.5) + tm_basemap(\"SwissFederalGeoportal.NationalMapColor\")\nm1\n\n\n\n\n\nWhen using dynamic maps, you can specify additional options that are not available for static maps using the tm_view() function. For example you can limit the zoom factors and the available extent.\n\nm1 + tm_view(set_zoom_limits = c(10, 12), set_bounds = c(8, 47, 8.5, 47.3))\n\n\n\n\n\nSimilarly to mapview, you can also change the default basemaps using the basemaps argument of the tmap_options() function.\n\nopts &lt;- tmap_options(basemap.server = c(NationalMapColor = \"SwissFederalGeoportal.NationalMapColor\",\n                                        NationalMapGrey = \"SwissFederalGeoportal.NationalMapGrey\",\n                                        SWISSIMAGE = \"SwissFederalGeoportal.SWISSIMAGE\"))\n\ntm_shape(soi) + tm_dots(fill = \"red\", size = 1.5)\n\n\n\n\n# Restore defaults\ntmap_options(opts)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic maps</span>"
    ]
  },
  {
    "objectID": "dynamic_maps.html#leaflet",
    "href": "dynamic_maps.html#leaflet",
    "title": "8  Dynamic maps",
    "section": "8.3 leaflet",
    "text": "8.3 leaflet\nOn the to-do list…",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic maps</span>"
    ]
  },
  {
    "objectID": "dynamic_maps.html#mapgl",
    "href": "dynamic_maps.html#mapgl",
    "title": "8  Dynamic maps",
    "section": "8.4 mapgl",
    "text": "8.4 mapgl\nOn the to-do list…\n\n\n\n\n1. Cheng J, Schloerke B, Karambelkar B, Xie Y. Leaflet: Create Interactive Web Maps with the JavaScript Leaflet Library. Published online 2023. https://rstudio.github.io/leaflet/",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic maps</span>"
    ]
  },
  {
    "objectID": "r_and_qgis.html",
    "href": "r_and_qgis.html",
    "title": "9  Using QGIS via R",
    "section": "",
    "text": "If you start from here…\n\n\n\n\n\nRun the following code to load and create everything you’ll need to run the examples in this section.\n\nlibrary(sf)\nobs &lt;- read.csv(\"data/observations.csv\")\nobs &lt;- st_as_sf(obs, coords = c(\"x\", \"y\"), crs = \"EPSG:2056\")\n\n\n\n\nSometimes you know how to do things in QGIS and you don’t have the time to search how to do it in R, or maybe QGIS is faster for a specific task. A recent R package called qgisprocess1 was developed for this reason. It gives access to the whole Processing toolbox available in QGIS. Of course you need to have QGIS installed as well. I will not go into the details of all the possibilities here but just show a quick example.\nIf you have multiple versions of QGIS installed, qgisprocess should find all of them and automatically use the most recent one.\n\nlibrary(qgisprocess)\n\nAttempting to load the package cache ... Success!\nQGIS version: 3.40.2-Bratislava\nHaving access to 685 algorithms from 7 QGIS processing providers.\nRun `qgis_configure(use_cached_data = TRUE)` to reload cache and get more details.\n&gt;&gt;&gt; Run `qgis_enable_plugins()` to enable 2 disabled plugins and access\n    their algorithms: cartography_tools, grassprovider\n\n\nYou can easily list all the available algorithms using the qgis_algorithms() function or search for a specific one using the qgis_search_algorithms() function. For example let’s compute a simple buffer around our sightings. The package will convert the sf object to a format understood by QGIS, then QGIS will compute the buffer and produce a GeoPackage with the output. We can then import it back to an sf object thanks to the st_as_sf() function. Recent versions of the package also support terra objects. If the output of the processing algorithm is a raster, you should thus use the qgis_as_terra() function instead of st_as_sf().\n\nobs_buff_qgis_res &lt;- qgis_run_algorithm(\"native:buffer\", \n                                        INPUT = obs,\n                                        DISTANCE = 1000,\n                                        DISSOLVE = TRUE,\n                                        .quiet = TRUE)\n\nArgument `SEGMENTS` is unspecified (using QGIS default value).\n\n\nUsing `END_CAP_STYLE = \"Round\"`\n\n\nUsing `JOIN_STYLE = \"Round\"`\n\n\nArgument `MITER_LIMIT` is unspecified (using QGIS default value).\n\n\nArgument `SEPARATE_DISJOINT` is unspecified (using QGIS default value).\n\n\nUsing `OUTPUT = qgis_tmp_vector()`\n\nobs_buff_qgis_res\n\n&lt;Result of `qgis_run_algorithm(\"native:buffer\", ...)`&gt;\nList of 1\n $ OUTPUT: 'qgis_outputVector' chr \"C:\\\\Users\\\\jgu\\\\AppData\\\\Local\\\\Temp\\\\Rtmpmg3IT2\\\\filea1c7fcb755\\\\filea1ccaa1b30.gpkg\"\n\nobs_buff_qgis &lt;- st_as_sf(obs_buff_qgis_res, as_tibble = FALSE)\nplot(st_geometry(obs_buff_qgis))\n\n\n\n\n\n\n\n\nTo check the name of the function parameters, you can access the QGIS help files via R using the qgis_show_help() function, or just display the parameters using the qgis_get_argument_specs() function.\n\nqgis_get_argument_specs(\"native:buffer\")\n\n# A tibble: 9 × 6\n  name    description qgis_type default_value available_values acceptable_values\n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;     &lt;list&gt;        &lt;list&gt;           &lt;list&gt;           \n1 INPUT   Input layer source    &lt;NULL&gt;        &lt;NULL&gt;           &lt;chr [1]&gt;        \n2 DISTAN… Distance    distance  &lt;int [1]&gt;     &lt;NULL&gt;           &lt;chr [3]&gt;        \n3 SEGMEN… Segments    number    &lt;int [1]&gt;     &lt;NULL&gt;           &lt;chr [3]&gt;        \n4 END_CA… End cap st… enum      &lt;int [1]&gt;     &lt;chr [3]&gt;        &lt;chr [2]&gt;        \n5 JOIN_S… Join style  enum      &lt;int [1]&gt;     &lt;chr [3]&gt;        &lt;chr [2]&gt;        \n6 MITER_… Miter limit number    &lt;int [1]&gt;     &lt;NULL&gt;           &lt;chr [3]&gt;        \n7 DISSOL… Dissolve r… boolean   &lt;lgl [1]&gt;     &lt;NULL&gt;           &lt;chr [4]&gt;        \n8 SEPARA… Keep disjo… boolean   &lt;lgl [1]&gt;     &lt;NULL&gt;           &lt;chr [4]&gt;        \n9 OUTPUT  Buffered    sink      &lt;NULL&gt;        &lt;NULL&gt;           &lt;chr [1]&gt;        \n\n\nIf you want to clean the temporary data that is created by qgisprocess, you can use the qgis_clean_result() function.\n\nfile.exists(qgis_extract_output(obs_buff_qgis_res))\n\n[1] TRUE\n\nqgis_clean_result(obs_buff_qgis_res)\nfile.exists(qgis_extract_output(obs_buff_qgis_res))\n\n[1] FALSE\n\n\n\n\n\n\n1. Dunnington D, Vanderhaeghe F, Caha J, Muenchow J. R package qgisprocess: Use QGIS processing algorithms. Published online 2024. https://r-spatial.github.io/qgisprocess/",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using QGIS via R</span>"
    ]
  },
  {
    "objectID": "getting_data.html",
    "href": "getting_data.html",
    "title": "10  Getting data",
    "section": "",
    "text": "Here’s a non-exhaustive list of packages you can use to download GIS data.\n\n\n\nPackage name\nDescription\n\n\n\n\nmaptiles\nDownload and import background maps from several providers\n\n\nosmdata\nDownload and import small OpenStreetMap data sets\n\n\nosmextract\nDownload and import large OpenStreetMap data sets\n\n\ngeodata\nDownload and import imports administrative, elevation, WorldClim data, etc.\n\n\nrnaturalearth\nAccess to Natural Earth vector and raster data\n\n\nelevatr\nImport digital elevation models\n\n\ngiscoR\nTools to download data from the GISCO (Geographic Information System of the Commission) Eurostat database\n\n\nrsi\nDownload Landsat and Sentinel data and compute spectral indices\n\n\nmodisfast\nDownload MODIS satellite data\n\n\nclimateR\nDownload climate, land cover, and soil data (https://mikejohnson51.github.io/climateR)\n\n\nswissgd\nDownload data from the Swiss geodata infrastructure (https://github.com/zumbov2/swissgd)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Getting data</span>"
    ]
  },
  {
    "objectID": "more_help.html",
    "href": "more_help.html",
    "title": "11  More help",
    "section": "",
    "text": "If you need more help, documentation, code examples, you should have a look at the sf vignettes and the terra documentation (https://rspatial.org).\nThe book Geocomputation with R1 is absolutely amazing and available for free (legally) on the web: https://r.geocompx.org. The book Spatial Data Science2 also provides some advanced information on spatial data: https://r-spatial.org/book/.\nIf you really like tmap, you should also check its website (https://r-tmap.github.io/tmap). There’s also a book draft which looks really promising: https://tmap.geocompx.org3.\n\n\n\n\n1. Lovelace R, Nowosad J, Muenchow J. Geocomputation with R. Second edition. CRC Press 2025.\n\n\n2. Pebesma E, Bivand R. Spatial Data Science: With Applications in R. Chapman; Hall/CRC 2023. https://r-spatial.org/book/\n\n\n3. Tennekes M, Nowosad J. Elegant and Informative Maps with Tmap. 2025. https://tmap.geocompx.org",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>More help</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Cheng J, Schloerke B, Karambelkar B, Xie Y.\nLeaflet: Create Interactive Web\nMaps with the JavaScript Leaflet\nLibrary. Published online 2023. https://rstudio.github.io/leaflet/\n\n\n2. Pebesma E. Lwgeom: Bindings to\nSelected liblwgeom Functions for\nSimple Features. Published online 2023. https://github.com/r-spatial/lwgeom/\n\n\n3. Appelhans T, Detsch F, Reudenbach C, Woellauer\nS. Mapview: Interactive Viewing of\nSpatial Data in R. Published\nonline 2023. https://github.com/r-spatial/mapview/\n\n\n4. Dunnington D, Vanderhaeghe F, Caha J, Muenchow\nJ. R package qgisprocess: Use QGIS processing algorithms.\nPublished online 2024. https://r-spatial.github.io/qgisprocess/\n\n\n5. Hijmans RJ. Terra: Spatial\nData Analysis. Published online 2023. https://rspatial.org\n\n\n6. Tennekes M. Tmap: Thematic\nMaps. Published online 2023. https://github.com/r-tmap/tmap/\n\n\n7. Pebesma E, Bivand R. Spatial\nData Science: With Applications\nin R. Chapman; Hall/CRC 2023. https://r-spatial.org/book/\n\n\n8. Pebesma E. Simple Features for\nR: Standardized Support for\nSpatial Vector Data. The R\nJournal. 2018. 10(1):439-446.\n\n\n9. Lovelace R, Nowosad J, Muenchow J.\nGeocomputation with R. Second edition. CRC Press\n2025.\n\n\n10. Tennekes M, Nowosad J. Elegant and\nInformative Maps with Tmap. 2025. https://tmap.geocompx.org\n\n\n11. Tennekes M, Puts MJH. cols4all: A color palette\nanalysis tool. In: EuroVis (Short\nPapers). Wiley 2023.",
    "crumbs": [
      "References"
    ]
  }
]